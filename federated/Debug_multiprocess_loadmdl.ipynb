{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "import argparse\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference, ASRLocalUpdate\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar, Data2VecAudioForCTC, DataCollatorCTCWithPadding\n",
    "from utils import get_dataset, average_weights, exp_details\n",
    "\n",
    "from transformers import Data2VecAudioConfig, Wav2Vec2Processor\n",
    "from multiprocessing import Pool\n",
    "from collections import OrderedDict\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# federated arguments (Notation for the arguments followed from paper)\n",
    "parser.add_argument('--epochs', type=int, default=2,\n",
    "                    help=\"number of rounds of training\")\n",
    "parser.add_argument('--num_users', type=int, default=2,\n",
    "                    help=\"number of users: K\")\n",
    "parser.add_argument('--frac', type=float, default=1.0,\n",
    "                    help='the fraction of clients: C')\n",
    "parser.add_argument('--local_ep', type=int, default=1,\n",
    "                    help=\"the number of local epochs: E\")\n",
    "\n",
    "parser.add_argument('--model', type=str, default='data2vec', help='model name')\n",
    "\n",
    "\n",
    "# other arguments\n",
    "parser.add_argument('--dataset', type=str, default='adress', help=\"name \\\n",
    "                    of dataset\") #cifar\n",
    "#parser.add_argument('--num_classes', type=int, default=10, help=\"number \\\n",
    "#                    of classes\")\n",
    "parser.add_argument('--gpu', default=1, help=\"To use cuda, set \\\n",
    "                    to a specific GPU ID. Default set to use CPU.\")\n",
    "\n",
    "# additional arguments\n",
    "parser.add_argument('--pretrain_name', type=str, default='facebook/data2vec-audio-large-960h', help=\"str used to load pretrain model\")\n",
    "parser.add_argument('-lam', '--LAMBDA', type=float, default=0.5, help=\"Lambda for GRL\")\n",
    "parser.add_argument('-st', '--STAGE', type=int, default=2, help=\"Current training stage\")\n",
    "parser.add_argument('-GRL', '--GRL', action='store_true', default=False, help=\"True: GRL\")\n",
    "parser.add_argument('-model_in', '--model_in_path', type=str, default=\"/mnt/Internal/FedASR/weitung/HuggingFace/Pretrain/saves/data2vec-audio-large-960h_new1_recall/final/\", help=\"Where the model is saved\")\n",
    "parser.add_argument('-model_out', '--model_out_path', type=str, default=\"./save/data2vec-audio-large-960h_new2_recall_FL\", help=\"Where to save the model\")\n",
    "parser.add_argument('-log', '--log_path', type=str, default=\"data2vec-audio-large-960h_new2_recall_FL.txt\", help=\"name for the txt file\")\n",
    "# 2023/01/08: loss type\n",
    "parser.add_argument('-ad_loss', '--AD_loss', type=str, default=\"recall\", help=\"loss to use for AD classifier\")\n",
    "# 2023/01/18: ckpt\n",
    "parser.add_argument('-ckpt', '--checkpoint', type=str, default=None, help=\"path to checkpoint\")\n",
    "# 2023/02/13: TOGGLE_RATIO\n",
    "parser.add_argument('-toggle_rt', '--TOGGLE_RATIO', type=float, default=0, help=\"To toggle more or less\")\n",
    "# 2023/02/15: GS_TAU, loss weight\n",
    "parser.add_argument('-gs_tau', '--GS_TAU', type=float, default=1, help=\"Tau for gumbel_softmax\")\n",
    "parser.add_argument('-w_loss', '--W_LOSS', type=float, default=None, nargs='+', help=\"weight for HC and AD\")\n",
    "\n",
    "args = parser.parse_args(args=[]) # for jupyter notebook\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\" # ÊàñËÄÖÂÖ∂‰ªñ‰Ω†ÊÉ≥Ë¶Å‰ΩøÁî®ÁöÑ GPU Á∑®Ëôü\n",
    "lock = mp.Lock()\n",
    "logger = SummaryWriter('../logs')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Á¨¨‰∏ÄÊ≠•ÊàëÊääloggerÊãøÊéâ‰∫Ü\n",
    "# final_result = pool.starmap_async(\n",
    "#     client_train, [(args, train_dataset, logger,\n",
    "#                     test_dataset, idx, epoch, global_weights)\n",
    "#                 for idx in idxs_users])\n",
    "#ÂÇ≥loggerÊúÉÂá∫ÁèæRuntimeError: Queue objects should only be shared between processes through inheritance\n",
    "final_result = pool.starmap_async(\n",
    "    client_train, [(args, train_dataset, None,\n",
    "                    test_dataset, idx, epoch, global_weights)\n",
    "                for idx in idxs_users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/FedASR/dacs/federated/dataset/train/cache-b6f4c0d2143105d5_*_of_00010.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/dataset/test/cache-f07dc6d726972452_*_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from local...\n",
      "Load data from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 11:17:46.800391: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 11:17:46.801945: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/FedASR/dacs/federated/update.py:26: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n",
      "/home/FedASR/dacs/federated/update.py:26: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from  /mnt/Internal/FedASR/weitung/HuggingFace/Pretrain/saves/data2vec-audio-large-960h_new1_recall/final/\n",
      "load from  /mnt/Internal/FedASR/weitung/HuggingFace/Pretrain/saves/data2vec-audio-large-960h_new1_recall/final/\n",
      "model loaded\n",
      "model loaded\n",
      "initialize ASRLocalUpdate\n",
      "Generating client training set for client  1 ...\n",
      "load model\n",
      "initialize ASRLocalUpdate\n",
      "Generating client training set for client  0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/FedASR/dacs/federated/dataset/train/cache-fb985c2cd39883d3.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/dataset/train/cache-5c6af673ecec18e7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `Data2VecAudioForCTC.forward` and have been ignored: text, path, array. If text, path, array are not expected by `Data2VecAudioForCTC.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  ready to train!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `Data2VecAudioForCTC.forward` and have been ignored: text, path, array. If text, path, array are not expected by `Data2VecAudioForCTC.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ready to train!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 419\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 105\n",
      "  0%|          | 0/105 [00:00<?, ?it/s]/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 543\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 136\n",
      "  0%|          | 0/136 [00:00<?, ?it/s]/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "  3%|‚ñé         | 3/105 [00:24<09:58,  5.86s/it]/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [01:51<00:00,  1.43it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [01:51<00:00,  1.06s/it]\n",
      "Saving model checkpoint to ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round0/final\n",
      "Configuration saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round0/final/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 111.2454, 'train_samples_per_second': 3.766, 'train_steps_per_second': 0.944, 'train_loss': 20.432831101190477, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 107/136 [01:50<00:21,  1.33it/s]Model weights saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round0/final/pytorch_model.bin\n",
      "Feature extractor saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round0/final/preprocessor_config.json\n",
      "loading configuration file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/config.json from cache at /home/FedASR/.cache/huggingface/transformers/a5e291023d6dd7ec0034390cee6d97f07e340fb24c68c7b5f3ec8d017a6fd29d.ed9b9e83fb80348aa91a073138fc7a0f44e669fc412c9c4bc98857f45bfd4330\n",
      "Model config Data2VecAudioConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Data2VecAudioForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_pos_kernel_size\": 19,\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0,\n",
      "  \"model_type\": \"data2vec-audio\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 5,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round0/final/pytorch_model.bin\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 112/136 [01:52<00:13,  1.84it/s]All model checkpoint weights were used when initializing Data2VecAudioForCTC.\n",
      "\n",
      "All the weights of Data2VecAudioForCTC were initialized from the model checkpoint at ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round0/final.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Data2VecAudioForCTC for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID 2797102 Getting  Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:05<00:00,  1.55it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:05<00:00,  1.09it/s]\n",
      "Saving model checkpoint to ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round0/final\n",
      "Configuration saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round0/final/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 125.2603, 'train_samples_per_second': 4.335, 'train_steps_per_second': 1.086, 'train_loss': 51.4159366383272, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round0/final/pytorch_model.bin\n",
      "Feature extractor saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round0/final/preprocessor_config.json\n",
      "loading configuration file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/config.json from cache at /home/FedASR/.cache/huggingface/transformers/a5e291023d6dd7ec0034390cee6d97f07e340fb24c68c7b5f3ec8d017a6fd29d.ed9b9e83fb80348aa91a073138fc7a0f44e669fc412c9c4bc98857f45bfd4330\n",
      "Model config Data2VecAudioConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Data2VecAudioForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_pos_kernel_size\": 19,\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0,\n",
      "  \"model_type\": \"data2vec-audio\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 5,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round0/final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID 2797101 Getting  Done\n",
      "local weights:  [OrderedDict([('weight', tensor([[-1.0251e-02,  2.9721e-02,  2.0969e-02,  ..., -3.0151e-02,\n",
      "          2.3156e-02,  2.9899e-03],\n",
      "        [ 1.7419e-02,  1.5695e-02,  3.6384e-02,  ...,  4.0979e-02,\n",
      "         -1.9435e-03,  5.6307e-02],\n",
      "        [-1.0266e-02, -4.6561e-02, -9.3507e-03,  ...,  4.6536e-03,\n",
      "         -5.3216e-03, -8.7337e-04],\n",
      "        ...,\n",
      "        [ 3.0671e-03, -5.8831e-03, -1.4685e-02,  ...,  2.6226e-02,\n",
      "         -1.4059e-02,  4.6610e-05],\n",
      "        [-2.6931e-02, -2.4982e-02,  2.8528e-02,  ...,  6.8621e-03,\n",
      "          2.6579e-02,  1.4392e-02],\n",
      "        [-2.8212e-02,  1.9698e-02,  3.6846e-02,  ...,  2.2174e-02,\n",
      "         -5.9081e-03,  1.0087e-02]])), ('bias', tensor([ 0.0009, -0.0022,  0.0030,  ...,  0.0086, -0.0057,  0.0071]))]), OrderedDict([('weight', tensor([[-0.0110,  0.0302,  0.0225,  ..., -0.0308,  0.0234,  0.0027],\n",
      "        [ 0.0181,  0.0147,  0.0369,  ...,  0.0426, -0.0014,  0.0568],\n",
      "        [-0.0089, -0.0472, -0.0091,  ...,  0.0045, -0.0058, -0.0014],\n",
      "        ...,\n",
      "        [ 0.0067, -0.0052, -0.0109,  ...,  0.0230, -0.0116, -0.0039],\n",
      "        [-0.0251, -0.0272,  0.0262,  ...,  0.0072,  0.0243,  0.0154],\n",
      "        [-0.0289,  0.0210,  0.0427,  ...,  0.0151, -0.0010,  0.0044]])), ('bias', tensor([ 0.0002, -0.0012,  0.0030,  ...,  0.0052, -0.0036,  0.0004]))])]\n",
      "global wegiths:  OrderedDict([('weight', tensor([[-0.0106,  0.0300,  0.0218,  ..., -0.0305,  0.0233,  0.0029],\n",
      "        [ 0.0178,  0.0152,  0.0366,  ...,  0.0418, -0.0017,  0.0565],\n",
      "        [-0.0096, -0.0469, -0.0092,  ...,  0.0046, -0.0056, -0.0011],\n",
      "        ...,\n",
      "        [ 0.0049, -0.0056, -0.0128,  ...,  0.0246, -0.0128, -0.0019],\n",
      "        [-0.0260, -0.0261,  0.0274,  ...,  0.0070,  0.0254,  0.0149],\n",
      "        [-0.0286,  0.0203,  0.0398,  ...,  0.0186, -0.0035,  0.0073]])), ('bias', tensor([ 0.0005, -0.0017,  0.0030,  ...,  0.0069, -0.0046,  0.0037]))])\n",
      "All results done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Data2VecAudioForCTC.\n",
      "\n",
      "All the weights of Data2VecAudioForCTC were initialized from the model checkpoint at ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round0/final.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Data2VecAudioForCTC for predictions without further training.\n",
      "2023-04-15 11:20:35.564423: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 11:20:35.592924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/FedASR/dacs/federated/update.py:26: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n",
      "/home/FedASR/dacs/federated/update.py:26: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from  /mnt/Internal/FedASR/weitung/HuggingFace/Pretrain/saves/data2vec-audio-large-960h_new1_recall/final/\n",
      "load from  /mnt/Internal/FedASR/weitung/HuggingFace/Pretrain/saves/data2vec-audio-large-960h_new1_recall/final/\n",
      "model loaded\n",
      "model loaded\n",
      "initialize ASRLocalUpdate\n",
      "Generating client training set for client  0 ...\n",
      "load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/FedASR/dacs/federated/dataset/train/cache-5c6af673ecec18e7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize ASRLocalUpdate\n",
      "Generating client training set for client  1 ...\n",
      "load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/FedASR/dacs/federated/dataset/train/cache-fb985c2cd39883d3.arrow\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `Data2VecAudioForCTC.forward` and have been ignored: array, text, path. If array, text, path are not expected by `Data2VecAudioForCTC.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  ready to train!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `Data2VecAudioForCTC.forward` and have been ignored: array, path, text. If array, path, text are not expected by `Data2VecAudioForCTC.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  ready to train!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 543\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 136\n",
      "  0%|          | 0/136 [00:00<?, ?it/s]/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 419\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 105\n",
      "  0%|          | 0/105 [00:00<?, ?it/s]/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/FedASR/.conda/envs/Flower-speechbrain/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [01:58<00:00,  1.20it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [01:58<00:00,  1.13s/it]\n",
      "Saving model checkpoint to ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round1/final\n",
      "Configuration saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round1/final/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 118.398, 'train_samples_per_second': 3.539, 'train_steps_per_second': 0.887, 'train_loss': 440.5591517857143, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 113/136 [02:00<00:15,  1.46it/s]Model weights saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round1/final/pytorch_model.bin\n",
      "Feature extractor saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round1/final/preprocessor_config.json\n",
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 114/136 [02:01<00:17,  1.29it/s]loading configuration file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/config.json from cache at /home/FedASR/.cache/huggingface/transformers/a5e291023d6dd7ec0034390cee6d97f07e340fb24c68c7b5f3ec8d017a6fd29d.ed9b9e83fb80348aa91a073138fc7a0f44e669fc412c9c4bc98857f45bfd4330\n",
      "Model config Data2VecAudioConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Data2VecAudioForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_pos_kernel_size\": 19,\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0,\n",
      "  \"model_type\": \"data2vec-audio\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 5,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round1/final/pytorch_model.bin\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 119/136 [02:03<00:09,  1.87it/s]All model checkpoint weights were used when initializing Data2VecAudioForCTC.\n",
      "\n",
      "All the weights of Data2VecAudioForCTC were initialized from the model checkpoint at ./save/data2vec-audio-large-960h_new2_recall_FL_client1_round1/final.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Data2VecAudioForCTC for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID 2799910 Getting  Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:12<00:00,  1.51it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [02:12<00:00,  1.03it/s]\n",
      "Saving model checkpoint to ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round1/final\n",
      "Configuration saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round1/final/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 132.7201, 'train_samples_per_second': 4.091, 'train_steps_per_second': 1.025, 'train_loss': 378.5946403952206, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round1/final/pytorch_model.bin\n",
      "Feature extractor saved in ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round1/final/preprocessor_config.json\n",
      "loading configuration file https://huggingface.co/facebook/data2vec-audio-large-960h/resolve/main/config.json from cache at /home/FedASR/.cache/huggingface/transformers/a5e291023d6dd7ec0034390cee6d97f07e340fb24c68c7b5f3ec8d017a6fd29d.ed9b9e83fb80348aa91a073138fc7a0f44e669fc412c9c4bc98857f45bfd4330\n",
      "Model config Data2VecAudioConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Data2VecAudioForCTC\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_pos_kernel_size\": 19,\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0,\n",
      "  \"model_type\": \"data2vec-audio\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 5,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round1/final/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID 2799909 Getting  Done\n",
      "local weights:  [OrderedDict([('weight', tensor([[-0.0079,  0.0306,  0.0205,  ..., -0.0311,  0.0234,  0.0026],\n",
      "        [ 0.0187,  0.0223,  0.0359,  ...,  0.0393, -0.0015,  0.0530],\n",
      "        [-0.0105, -0.0478, -0.0117,  ...,  0.0051, -0.0041, -0.0011],\n",
      "        ...,\n",
      "        [ 0.0021, -0.0081, -0.0190,  ...,  0.0318, -0.0178,  0.0043],\n",
      "        [-0.0302, -0.0224,  0.0354,  ..., -0.0007,  0.0335,  0.0069],\n",
      "        [-0.0291,  0.0202,  0.0329,  ...,  0.0254, -0.0094,  0.0146]])), ('bias', tensor([ 0.0023, -0.0048,  0.0055,  ...,  0.0138, -0.0126,  0.0107]))]), OrderedDict([('weight', tensor([[-0.0101,  0.0304,  0.0224,  ..., -0.0317,  0.0236,  0.0016],\n",
      "        [ 0.0186,  0.0189,  0.0364,  ...,  0.0410, -0.0014,  0.0538],\n",
      "        [-0.0100, -0.0479, -0.0108,  ...,  0.0059, -0.0054, -0.0010],\n",
      "        ...,\n",
      "        [ 0.0041, -0.0082, -0.0170,  ...,  0.0289, -0.0169,  0.0021],\n",
      "        [-0.0291, -0.0238,  0.0330,  ...,  0.0013,  0.0308,  0.0093],\n",
      "        [-0.0287,  0.0206,  0.0351,  ...,  0.0232, -0.0080,  0.0120]])), ('bias', tensor([ 0.0005, -0.0032,  0.0046,  ...,  0.0112, -0.0102,  0.0084]))])]\n",
      "global wegiths:  OrderedDict([('weight', tensor([[-0.0090,  0.0305,  0.0215,  ..., -0.0314,  0.0235,  0.0021],\n",
      "        [ 0.0186,  0.0206,  0.0362,  ...,  0.0401, -0.0015,  0.0534],\n",
      "        [-0.0102, -0.0478, -0.0113,  ...,  0.0055, -0.0048, -0.0011],\n",
      "        ...,\n",
      "        [ 0.0031, -0.0082, -0.0180,  ...,  0.0304, -0.0174,  0.0032],\n",
      "        [-0.0296, -0.0231,  0.0342,  ...,  0.0003,  0.0322,  0.0081],\n",
      "        [-0.0289,  0.0204,  0.0340,  ...,  0.0243, -0.0087,  0.0133]])), ('bias', tensor([ 0.0014, -0.0040,  0.0051,  ...,  0.0125, -0.0114,  0.0096]))])\n",
      "All results done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing Data2VecAudioForCTC.\n",
      "\n",
      "All the weights of Data2VecAudioForCTC were initialized from the model checkpoint at ./save/data2vec-audio-large-960h_new2_recall_FL_client0_round1/final.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Data2VecAudioForCTC for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "from client_train import client_train\n",
    "#!!! [NOTE] 3.ÈúÄË¶ÅÊääclient trainÈúÄË¶ÅÁç®Á´ãÂà∞Âà•ÁöÑÊ®°Â°äÁÑ∂ÂæåÁî®importÁöÑÊñπÂºèÂè´ÈÄ≤‰æÜ\n",
    "train_loss, test_wer = [], []\n",
    "val_acc_list, net_list = [], []\n",
    "cv_loss, cv_acc = [], []\n",
    "print_every = 2\n",
    "val_loss_pre, counter = 0, 0\n",
    "global_weights = None                                                           # initial global_weights\n",
    "train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing.set_start_method('spawn')\n",
    "    multiprocessing.set_start_method('spawn', force=True) #!!! [NOTE] 2.ÈúÄË¶ÅÊäämultiprocessingÁöÑmethodÂæûforkÊîπÊàêspawnÔºå‰∏¶‰∏îclient trainÈúÄË¶ÅÁç®Á´ãÂà∞Âà•ÁöÑÊ®°Â°äÁÑ∂ÂæåÁî®importÁöÑÊñπÂºèÂè´ÈÄ≤‰æÜ\n",
    "    for epoch in range(2):\n",
    "        m = max(int(args.frac * args.num_users), 1)                                 # num of clients to train, min:1\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)      # select by client_id\n",
    "        pool = multiprocessing.Pool(processes=m)\n",
    "        try:\n",
    "            # final_result = pool.starmap_async(\n",
    "            #     client_train, [(args, train_dataset, logger,\n",
    "            #                     test_dataset, idx, epoch, global_weights)\n",
    "            #                 for idx in idxs_users])\n",
    "            #!!! [NOTE] 1.ÂÇ≥loggerÊúÉÂá∫ÁèæRuntimeError: Queue objects should only be shared between processes through inheritance\n",
    "            final_result = pool.starmap_async(\n",
    "                client_train, [(args, train_dataset, None,\n",
    "                                test_dataset, idx, epoch, global_weights)\n",
    "                            for idx in idxs_users])\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while running local_model.update_weights(): {str(e)}\")\n",
    "        finally:\n",
    "            final_result.wait()\n",
    "            results = final_result.get()\n",
    "        \n",
    "        local_weights = []\n",
    "        local_losses = []\n",
    "        for idx in range(len(results)):\n",
    "            w, loss = results[idx]\n",
    "            local_weights.append(w)\n",
    "            local_losses.append(loss)\n",
    "        print(\"local weights: \", local_weights)\n",
    "        # get global weights by averaging local weights\n",
    "        global_weights = average_weights(local_weights)\n",
    "        print(\"global wegiths: \", global_weights)\n",
    "        # update global weights\n",
    "        #global_model.load_state_dict(global_weights)\n",
    "        loss_avg = sum(local_losses) / len(local_losses)                # average losses from participated client\n",
    "        train_loss.append(loss_avg)                                     # save loss for this round\n",
    "        print(\"All results done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flower-speechbrain",
   "language": "python",
   "name": "flower-speechbrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
