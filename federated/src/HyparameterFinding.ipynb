{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimental details:\n",
      "    Model     : data2vec\n",
      "    Global Rounds   : 10\n",
      "\n",
      "    Current Stage   : 0\n",
      "\n",
      "    Loss Type       : None\n",
      "\n",
      "    Federated parameters:\n",
      "    Number of users    : 100\n",
      "    Fraction of users  : 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/train/cache-b6f4c0d2143105d5_*_of_00010.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/test/cache-f07dc6d726972452_*_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from local...\n",
      "Load data from local...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ccc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2425338/3373826069.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m \u001b[0maaa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mccc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;31m# train_data = csv2dataset(PATH = '{}/clips/'.format(args.root_dir),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;31m#                          path = \"{}/mid_csv/train.csv\".format(args.root_dir)) #!!! librosa在load的時候非常慢，大約7分47秒讀完1869個file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ccc' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# 更新2023/04/10\n",
    "# 1. csv2dataset函數裡面使用csv_path和root_path\n",
    "# 2. 在讀音檔的時候增加一個選項：scipy.io，讀起來會快很多但是不知道會不會影響到原來的效果\n",
    "\n",
    "# 大約10138MiB\n",
    "\n",
    "# 訓練一次在CPU很滿的情況下7hr\n",
    "# =============================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "import librosa\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\n",
    "from transformers import Data2VecAudioConfig, HubertConfig, SEWDConfig, UniSpeechSatConfig\n",
    "from transformers import Data2VecAudioForCTC, HubertForCTC, SEWDForCTC, UniSpeechSatForCTC\n",
    "from jiwer import wer\n",
    "import scipy.io\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from typing import Dict\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import Data2VecAudioConfig\n",
    "from models import Data2VecAudioForCTC, Data2VecAudioForCTC_eval, DataCollatorCTCWithPadding\n",
    "from datasets import Dataset, load_from_disk\n",
    "import librosa\n",
    "from jiwer import wer\n",
    "import copy\n",
    "from transformers import Data2VecAudioConfig, Wav2Vec2Processor\n",
    "import copy\n",
    "from tensorboardX import SummaryWriter\n",
    "import argparse\n",
    "# from update import compute_metrics\n",
    "\n",
    "\n",
    "# set up trainer\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('-model', '--model_path', type=str, default=\"./saves/wav2vec2-base-960h_GRL_0.5\", help=\"Where the model is saved\")\n",
    "parser.add_argument('-opt', '--optimizer', type=str, default=\"adamw_hf\", help=\"The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor\")\n",
    "parser.add_argument('-MGN', '--max_grad_norm', type=float, default=1.0, help=\"Maximum gradient norm (for gradient clipping)\")\n",
    "parser.add_argument('-model_type', '--model_type', type=str, default=\"data2vec\", help=\"Type of the model\")\n",
    "parser.add_argument('-sr', '--sampl_rate', type=float, default=16000, help=\"librosa read smping rate\")\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=1e-5, help=\"Learning rate\")\n",
    "parser.add_argument('-RD', '--root_dir', default='/mnt/Internal/FedASR/Data/ADReSS-IS2020-data', help=\"Learning rate\")\n",
    "parser.add_argument('--epochs', type=int, default=10,\n",
    "                        help=\"number of rounds of training\")\n",
    "parser.add_argument('--num_users', type=int, default=100,\n",
    "                    help=\"number of users: K\")\n",
    "parser.add_argument('--frac', type=float, default=0.1,\n",
    "                    help='the fraction of clients: C')\n",
    "parser.add_argument('--local_ep', type=int, default=10,\n",
    "                    help=\"the number of local epochs: E\")\n",
    "# model arguments\n",
    "parser.add_argument('--model', type=str, default='data2vec', help='model name')\n",
    "# other arguments\n",
    "parser.add_argument('--dataset', type=str, default='adress', help=\"name \\\n",
    "                    of dataset\")\n",
    "parser.add_argument('--gpu', default=None, help=\"To use cuda, set \\\n",
    "                    to a specific GPU ID. Default set to use CPU.\")\n",
    "# additional arguments\n",
    "parser.add_argument('--pretrain_name', type=str, default='facebook/data2vec-audio-large-960h', help=\"str used to load pretrain model\")\n",
    "\n",
    "parser.add_argument('-lam', '--LAMBDA', type=float, default=0.5, help=\"Lambda for GRL\")\n",
    "parser.add_argument('-st', '--STAGE', type=int, default=0, help=\"Current training stage\")\n",
    "parser.add_argument('-fl_st', '--FL_STAGE', type=int, default=1, help=\"Current FL training stage\")\n",
    "parser.add_argument('-GRL', '--GRL', action='store_true', default=False, help=\"True: GRL\")\n",
    "parser.add_argument('-model_in', '--model_in_path', type=str, default=\"/home/FedASR/dacs/federated/save/data2vec-audio-large-960h_new1_recall_FLASR\", help=\"Where the global model is saved\")\n",
    "parser.add_argument('-model_out', '--model_out_path', type=str, default=\"/home/FedASR/dacs/federated/save/\", help=\"Where to save the model\")\n",
    "# parser.add_argument('-log', '--log_path', type=str, default=\"wav2vec2-base-960h_linear_GRL.txt\", help=\"name for the txt file\")\n",
    "parser.add_argument('-csv', '--csv_path', type=str, default=\"wav2vec2-base-960h_GRL_0.5\", help=\"name for the csv file\")\n",
    "# 2023/01/08: loss type\n",
    "parser.add_argument('-ad_loss', '--AD_loss', type=str, default=None, help=\"loss to use for AD classifier\")\n",
    "# 2023/01/18: ckpt\n",
    "parser.add_argument('-ckpt', '--checkpoint', type=str, default=None, help=\"path to checkpoint\")\n",
    "# 2023/02/13: TOGGLE_RATIO\n",
    "parser.add_argument('-toggle_rt', '--TOGGLE_RATIO', type=float, default=0, help=\"To toggle more or less\")\n",
    "# 2023/02/15: GS_TAU, loss weight\n",
    "parser.add_argument('-gs_tau', '--GS_TAU', type=float, default=1, help=\"Tau for gumbel_softmax\")\n",
    "parser.add_argument('-w_loss', '--W_LOSS', type=float, default=None, nargs='+', help=\"weight for HC and AD\")\n",
    "# 2023/04/20\n",
    "parser.add_argument('-EXTRACT', '--EXTRACT', action='store_true', default=False, help=\"True: extract embs\")\n",
    "parser.add_argument('-client_id', '--client_id', type=str, default=\"public\", help=\"client_id: public, 0, or 1\")\n",
    "# 2023/04/24\n",
    "parser.add_argument('--global_ep', type=int, default=30, help=\"number for global model\")\n",
    "    \n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "os.environ['DACS_codeRoot'] = '/home/FedASR/dacs/'\n",
    "os.environ['DACS_dataRoot'] = '/mnt/Internal/FedASR/Data/ADReSS-IS2020-data/'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2,3\"\n",
    "from utils import csv2dataset, get_dataset, average_weights, exp_details\n",
    "from update import update_network_weight\n",
    "#model_out_dir = args.model_path # where to save model\n",
    "model_type = args.model_type                # what type of the model\n",
    "lr = args.learning_rate                     # learning rate\n",
    "optim = args.optimizer                      # opt\n",
    "max_grad_norm = args.max_grad_norm          # max_grad_norm\n",
    "args.log_path=os.path.basename(args.model_in_path)\n",
    "args.log_path+=\"epoch-{}\".format(args.local_ep)\n",
    "\n",
    "def Write_log(content,LOG_DIR=\"/home/FedASR/dacs/federated/logs/\"):\n",
    "    # write to txt file\n",
    "    file_object = open(LOG_DIR + args.log_path, 'a')\n",
    "    # Append at the end of file\n",
    "    file_object.write(json.dumps(content) + '\\n')\n",
    "    # Close the file\n",
    "    file_object.close()\n",
    "\n",
    "def map_to_result(batch, model,processor):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"]).unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "    return batch\n",
    "\n",
    "class CustomTrainer(Trainer):    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            \"\"\"\n",
    "            How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "            Subclass and override for custom behavior.\n",
    "            \"\"\"\n",
    "            #dementia_labels = inputs.pop(\"dementia_labels\") # pop 出來就會不見?\n",
    "            \n",
    "            if self.label_smoother is not None and \"labels\" in inputs:\n",
    "                labels = inputs.pop(\"labels\")\n",
    "            else:\n",
    "                labels = None\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            # Save past state if it exists\n",
    "            # TODO: this needs to be fixed and made cleaner later.\n",
    "            if self.args.past_index >= 0:\n",
    "                self._past = outputs[self.args.past_index]\n",
    "\n",
    "            if labels is not None:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "            else:\n",
    "                # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "                loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "    def log(self, logs: Dict[str, float]) -> None:\n",
    "        \"\"\"\n",
    "        Log `logs` on the various objects watching training.\n",
    "        Subclass and override this method to inject custom behavior.\n",
    "        Args:\n",
    "            logs (`Dict[str, float]`):\n",
    "                The values to log.\n",
    "        \"\"\"\n",
    "        if self.state.epoch is not None:\n",
    "            logs[\"epoch\"] = round(self.state.epoch, 2)\n",
    "\n",
    "        output = {**logs, **{\"step\": self.state.global_step}}\n",
    "        self.state.log_history.append(output)\n",
    "        \n",
    "        Write_log(LOG_DIR=\"/home/FedASR/dacs/federated/logs/\",content=output)\n",
    "\n",
    "        self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n",
    "\n",
    "\n",
    "def get_model_weight(args, source_path, network):                                   # get \"network\" weights from model in source_path\n",
    "    mask_time_prob = 0                                                              # change config to avoid training stopping\n",
    "    config = Data2VecAudioConfig.from_pretrained(args.pretrain_name, mask_time_prob=mask_time_prob)\n",
    "                                                                                    # use pre-trained config\n",
    "    model = Data2VecAudioForCTC.from_pretrained(source_path, config=config, args=args)\n",
    "                                                                                    # load from source\n",
    "    model.config.ctc_zero_infinity = True                                           # to avoid inf values\n",
    "\n",
    "    if network == \"ASR\":                                                            # get ASR weights\n",
    "        return_weights = [copy.deepcopy(model.data2vec_audio.state_dict()), copy.deepcopy(model.lm_head.state_dict())]\n",
    "    elif network == \"AD\":                                                           # get AD classifier weights\n",
    "        return_weights = copy.deepcopy(model.dementia_head.state_dict())\n",
    "    elif network == \"toggling_network\":                                             # get toggling network weights\n",
    "        return_weights = copy.deepcopy(model.arbitrator.state_dict())  \n",
    "    \n",
    "    return return_weights\n",
    "\n",
    "class ASRLocalUpdate(object):\n",
    "    def __init__(self, args, dataset, global_test_dataset, client_id, \n",
    "                 model_in_path, model_out_path):\n",
    "        self.args = args                                                            # given configuration\n",
    "        self.client_train_dataset = self.train_split(dataset, client_id)            # get subset of training set (dataset of THIS client)\n",
    "        \n",
    "        self.device = 'cuda' if args.gpu else 'cpu'                                 # use gpu or cpu\n",
    "        \n",
    "        self.global_test_dataset = global_test_dataset\n",
    "        self.client_test_dataset = self.test_split(global_test_dataset, client_id)  # get subset of testing set (dataset of THIS client)\n",
    "        self.processor = Wav2Vec2Processor.from_pretrained(args.pretrain_name)\n",
    "        self.data_collator = DataCollatorCTCWithPadding(processor=self.processor, padding=True)\n",
    "        self.client_id = client_id\n",
    "\n",
    "        self.model_in_path = model_in_path                                          # no info for client_id & global_round\n",
    "        self.model_out_path = model_out_path                                        # no info for client_id & global_round\n",
    "\n",
    "    def train_split(self, dataset, client_id):\n",
    "        # generate sub- training set for given user-ID\n",
    "        if client_id == \"public\":                                                   # get spk_id for public dataset, 54 PAR (50% of all training set)\n",
    "            client_spks = ['S086', 'S021', 'S018', 'S156', 'S016', 'S077', 'S027', 'S116', 'S143', 'S082', 'S039', 'S150', 'S004', 'S126', 'S137', \n",
    "            'S097', 'S128', 'S059', 'S096', 'S081', 'S135', 'S094', 'S070', 'S049', 'S080', 'S040', 'S076', 'S093', 'S141', 'S034', 'S056', 'S090', \n",
    "            'S130', 'S092', 'S055', 'S019', 'S154', 'S017', 'S114', 'S100', 'S036', 'S029', 'S127', 'S073', 'S089', 'S051', 'S005', 'S151', 'S003', \n",
    "            'S033', 'S007', 'S084', 'S043', 'S009']                                 # 27 AD + 27 HC\n",
    "\n",
    "        elif client_id == 0:                                                        # get spk_id for client 1, 27 PAR (25% of all training set)\n",
    "            client_spks = ['S058', 'S030', 'S064', 'S104', 'S048', 'S118', 'S122', 'S001', 'S087', 'S013', 'S025', 'S083', 'S067', 'S068', 'S111', \n",
    "            'S028', 'S015', 'S108', 'S095', 'S002', 'S072', 'S020', 'S148', 'S144', 'S110', 'S124', 'S129']\n",
    "                                                                                    # 13 AD + 14 HC\n",
    "        elif client_id == 1:                                                        # get spk_id for client 2, 27 PAR (25% of all training set)  \n",
    "            client_spks = ['S071', 'S136', 'S140', 'S145', 'S032', 'S101', 'S103', 'S139', 'S038', 'S153', 'S035', 'S011', 'S132', 'S006', 'S149', \n",
    "            'S041', 'S079', 'S107', 'S063', 'S061', 'S125', 'S062', 'S012', 'S138', 'S024', 'S052', 'S142']\n",
    "                                                                                    # 14 AD + 13 HC\n",
    "        else:\n",
    "            print(\"Train with whole dataset!!\")\n",
    "            return dataset\n",
    "        \n",
    "        print(\"Generating client training set for client \", str(client_id), \"...\")\n",
    "        client_train_dataset = dataset.filter(lambda example: example[\"path\"].startswith(tuple(client_spks)))\n",
    "        \n",
    "        return client_train_dataset\n",
    "    \n",
    "    def test_split(self, dataset, client_id):\n",
    "        # generate sub- testing set for given user-ID\n",
    "        if client_id == \"public\":                                                   # get spk_id for public dataset, 24 PAR (50% of all testing set)\n",
    "            client_spks = ['S197', 'S163', 'S193', 'S169', 'S196', 'S184', 'S168', 'S205', 'S185', 'S171', 'S204', 'S173', 'S190', 'S191', 'S203', \n",
    "                           'S180', 'S165', 'S199', 'S160', 'S175', 'S200', 'S166', 'S177', 'S167']                                # 12 AD + 12 HC\n",
    "\n",
    "        elif client_id == 0:                                                        # get spk_id for client 1, 12 PAR (25% of all testing set)\n",
    "            client_spks = ['S198', 'S182', 'S194', 'S161', 'S195', 'S170', 'S187', 'S192', 'S178', 'S201', 'S181', 'S174']\n",
    "                                                                                    # 6 AD + 6 HC\n",
    "        elif client_id == 1:                                                        # get spk_id for client 2, 12 PAR (25% of all testing set)  \n",
    "            client_spks = ['S179', 'S188', 'S202', 'S162', 'S172', 'S183', 'S186', 'S207', 'S189', 'S164', 'S176', 'S206']\n",
    "                                                                                    # 6 AD + 6 HC\n",
    "        else:\n",
    "            print(\"Test with whole dataset!!\")\n",
    "            return dataset\n",
    "        \n",
    "        print(\"Generating client testing set for client \", str(client_id), \"...\")\n",
    "        client_test_dataset = dataset.filter(lambda example: example[\"path\"].startswith(tuple(client_spks)))\n",
    "        \n",
    "        return client_test_dataset\n",
    "    \n",
    "    def record_result(self, trainer, result_folder):                                # save training loss, testing loss, and testing wer\n",
    "        logger = SummaryWriter('../logs/' + result_folder.split(\"/\")[-1])           # use name of this model as folder's name\n",
    "\n",
    "        for idx in range(len(trainer.state.log_history)):\n",
    "            if \"loss\" in trainer.state.log_history[idx].keys():                     # add in training loss, epoch*100 to obtain int\n",
    "                logger.add_scalar('Loss/train', trainer.state.log_history[idx][\"loss\"], trainer.state.log_history[idx][\"epoch\"]*100)\n",
    "\n",
    "            elif \"eval_loss\" in trainer.state.log_history[idx].keys():              # add in testing loss & WER, epoch*100 to obtain int\n",
    "                logger.add_scalar('Loss/test', trainer.state.log_history[idx][\"eval_loss\"], trainer.state.log_history[idx][\"epoch\"]*100)\n",
    "                logger.add_scalar('wer/test', trainer.state.log_history[idx][\"eval_wer\"], trainer.state.log_history[idx][\"epoch\"]*100)\n",
    "\n",
    "            else:                                                                   # add in final training loss, epoch*100 to obtain int\n",
    "                logger.add_scalar('Loss/train', trainer.state.log_history[idx][\"train_loss\"], trainer.state.log_history[idx][\"epoch\"]*100)\n",
    "        logger.close()\n",
    "\n",
    "    def update_weights(self, global_weights, global_round):\n",
    "        if global_weights == None:                                                  # train from model from model_in_path\n",
    "            mask_time_prob = 0                                                      # change config to avoid training stopping\n",
    "            config = Data2VecAudioConfig.from_pretrained(self.args.pretrain_name, mask_time_prob=mask_time_prob)\n",
    "                                                                                    # use pre-trained config\n",
    "            model = Data2VecAudioForCTC.from_pretrained(self.model_in_path, config=config, args=self.args)\n",
    "            model.config.ctc_zero_infinity = True                                   # to avoid inf values\n",
    "        else:                                                                       # update train model using given weight\n",
    "            if self.args.STAGE == 0:                                                # train ASR\n",
    "                model = update_network_weight(args=self.args, source_path=self.model_in_path, target_weight=global_weights, network=\"ASR\")                    \n",
    "                                                                                    # from model from model_in_path, update ASR's weight\n",
    "            elif self.args.STAGE == 1:                                              # train AD classifier\n",
    "                model = update_network_weight(args=self.args, source_path=self.model_in_path, target_weight=global_weights, network=\"AD\")           \n",
    "                                                                                    # from model from model_in_path, update AD classifier's weight\n",
    "            elif self.args.STAGE == 2:                                              # train toggling network\n",
    "                model = update_network_weight(args=self.args, source_path=self.model_in_path, target_weight=global_weights, network=\"toggling_network\")             \n",
    "                                                                                    # from model from model_in_path, update arbitrator's weight\n",
    "        global log_path\n",
    "        log_path = self.args.log_path\n",
    "\n",
    "        model.train()\n",
    "        if self.args.STAGE == 0:                                                    # fine-tune ASR\n",
    "            lr = 1e-5\n",
    "        elif self.args.STAGE == 1:                                                  # train AD classifier\n",
    "            lr = 1e-4\n",
    "        elif self.args.STAGE == 2:                                                  # train toggling network\n",
    "            lr = 1e-3\n",
    "\n",
    "        if self.client_id == \"public\":                                              # model train with public dataset, name end with \"_global\"\n",
    "            save_path = self.model_out_path + \"_global\"\n",
    "        else:\n",
    "            save_path = self.model_out_path + \"_client\" + str(self.client_id) + \"_round\" + str(global_round)\n",
    "                                                                                    # for local models, record info for id & num_round\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=save_path,\n",
    "            group_by_length=True,\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            num_train_epochs=self.args.local_ep, #self.args.local_ep\n",
    "            fp16=True,\n",
    "            gradient_checkpointing=True, \n",
    "            save_steps=500, # 500\n",
    "            eval_steps=500, # 500\n",
    "            logging_steps=10, # 500\n",
    "            learning_rate=lr, # self.args.lr\n",
    "            weight_decay=0.005,\n",
    "            warmup_steps=1000,\n",
    "            save_total_limit=2,\n",
    "            log_level='debug',\n",
    "            logging_strategy=\"steps\",\n",
    "            #adafactor=True,            # default:false. Whether or not to use transformers.Adafactor optimizer instead of transformers.AdamW\n",
    "            #fp16_full_eval=True,      # to save memory\n",
    "            #max_grad_norm=0.5\n",
    "        )\n",
    "        global processor\n",
    "        processor = self.processor\n",
    "        # self.debug_mdl=model\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            data_collator=self.data_collator,\n",
    "            args=training_args,\n",
    "            compute_metrics=compute_metrics,\n",
    "            train_dataset=self.client_train_dataset,\n",
    "            eval_dataset=self.global_test_dataset,\n",
    "            tokenizer=self.processor.feature_extractor,\n",
    "        )\n",
    "\n",
    "        print(\" | Client \", str(self.client_id), \" ready to train! |\")\n",
    "        trainer.train()\n",
    "        return trainer.model\n",
    "\n",
    "        trainer.save_model(save_path + \"/final\")                                    # save final model\n",
    "        self.record_result(trainer, save_path)                           # save training loss, testing loss, and testing wer\n",
    "\n",
    "        # get \"network\" weights from model in source_path\n",
    "        if self.args.STAGE == 0:                                                    # train ASR\n",
    "            return_weights = get_model_weight(args=self.args, source_path=save_path + \"/final/\", network=\"ASR\")\n",
    "        elif self.args.STAGE == 1:                                                  # train AD classifier\n",
    "            return_weights = get_model_weight(args=self.args, source_path=save_path + \"/final/\", network=\"AD\")\n",
    "        elif self.args.STAGE == 2:                                                  # train toggling_network\n",
    "            return_weights = get_model_weight(args=self.args, source_path=save_path + \"/final/\", network=\"toggling_network\")  \n",
    "         \n",
    "        return return_weights, trainer.state.log_history[-1][\"train_loss\"]          # return weight, average losses for this round\n",
    "\n",
    "    def extract_embs(self):                                                         # extract emb. using model in args.model_in_path\n",
    "        # load model\n",
    "        mask_time_prob = 0                                                          # change config to avoid code from stopping\n",
    "        config = Data2VecAudioConfig.from_pretrained(self.args.pretrain_name, mask_time_prob=mask_time_prob)\n",
    "        model = Data2VecAudioForCTC_eval.from_pretrained(self.args.model_in_path, config=config, args=self.args)\n",
    "        processor = self.processor\n",
    "\n",
    "        # get emb.s, masks... 1 sample by 1 sample for client test\n",
    "        df = map_to_result(self.client_test_dataset[0], processor, model, 0)\n",
    "        for i in range(len(self.client_test_dataset) - 1):\n",
    "            df2 = map_to_result(self.client_test_dataset[i+1], processor, model, i+1)\n",
    "            df = pd.concat([df, df2], ignore_index=True)\n",
    "            print(\"\\r\"+ str(i), end=\"\")\n",
    "\n",
    "        csv_path = \"./results/\" + self.args.csv_path + \".csv\"\n",
    "        df.to_csv(csv_path)\n",
    "        print(\"Testing data Done\")\n",
    "\n",
    "        # get emb.s, masks... 1 sample by 1 sample for client train\n",
    "        df = map_to_result(self.client_train_dataset[0], processor, model, 0)\n",
    "        for i in range(len(self.client_train_dataset) - 1):\n",
    "            df2 = map_to_result(self.client_train_dataset[i+1], processor, model, i+1)\n",
    "            df = pd.concat([df, df2], ignore_index=True)\n",
    "            print(\"\\r\"+ str(i), end=\"\")\n",
    "\n",
    "        csv_path = \"./results/\" + self.args.csv_path + \"_train.csv\"\n",
    "        df.to_csv(csv_path)\n",
    "        print(\"Training data Done\")\n",
    "\n",
    "        print(self.args.csv_path + \" All Done\")\n",
    "\n",
    "from datasets import load_metric\n",
    "wer_metric = load_metric(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "exp_details(args)                                                               # print out details based on configuration\n",
    "# 吃資料\n",
    "train_dataset, test_dataset = get_dataset(args)                                 # get dataset\n",
    "\n",
    "\n",
    "aaa=ccc\n",
    "# train_data = csv2dataset(PATH = '{}/clips/'.format(args.root_dir),\n",
    "#                          path = \"{}/mid_csv/train.csv\".format(args.root_dir)) #!!! librosa在load的時候非常慢，大約7分47秒讀完1869個file\n",
    "# dev_data = csv2dataset(PATH = '{}/clips/'.format(args.root_dir),\n",
    "#                        path = \"{}/mid_csv/dev.csv\".format(args.root_dir))\n",
    "# test_data = csv2dataset(PATH = '{}/clips/'.format(args.root_dir),\n",
    "#                         path = \"{}/mid_csv/test.csv\".format(args.root_dir))\n",
    "# 吃global\n",
    "\n",
    "client_id=0\n",
    "model_in_path=args.pretrain_name\n",
    "global_mdl_path=args.model_in_path\n",
    "model_out_path=args.model_out_path\n",
    "global_weights = get_model_weight(args=args, source_path=global_mdl_path + \"_global/final/\", network=\"ASR\")\n",
    "                                                                    # local ASR and AD with global toggling network\n",
    "                                                                    # get toggling_network weights from model in model_out_path + \"_global/final/\"\n",
    "# 選資料\n",
    "local_model = ASRLocalUpdate(args=args, dataset=train_dataset, global_test_dataset=test_dataset, \n",
    "                                 client_id=client_id, model_in_path=model_in_path, model_out_path=model_out_path)\n",
    "                                                                                      # initial dataset of current client\n",
    "# 訓練模型\n",
    "trained_model = local_model.update_weights(global_weights=global_weights, global_round=0) \n",
    "\n",
    "\n",
    "args.model_out_path=args.model_out_path+\"_epoch-{}\".format(args.local_ep)\n",
    "trained_model.save_pretrained(args.model_out_path+\"/final\")\n",
    "# evaluate\n",
    "map_to_result_with_model = lambda batch: map_to_result(batch, trained_model, local_model.processor)\n",
    "result = test_dataset.map(map_to_result_with_model)\n",
    "Write_log({\"test_wer= \":wer(result[\"text\"], result[\"pred_str\"])},LOG_DIR=\"/home/FedASR/dacs/federated/logs/\")\n",
    "print(\"WER of \", args.pretrain_name, \" : \", wer(result[\"text\"], result[\"pred_str\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  tensor(0.5000)\n",
      "Current stage: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/train/cache-b6f4c0d2143105d5_*_of_00010.arrow\n",
      "Loading cached processed dataset at /home/FedASR/dacs/federated/src/dataset/test/cache-f07dc6d726972452_*_of_00010.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from local...\n",
      "Load data from local...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b343f7cf084d49af19b3e5304ce692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER of  facebook/data2vec-audio-large-960h  :  0.2802827965435978\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "def map_to_result(batch, model,processor):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"]).unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "# args.model_in_path=\"/home/FedASR/dacs/federated/save/data2vec-audio-large-960h_new1_recall_FLASR_global/final/\"\n",
    "\n",
    "args.model_in_path='/home/FedASR/dacs/federated/save/_epoch-10/final'\n",
    "mask_time_prob = 0                                                              # change config to avoid training stopping\n",
    "config = Data2VecAudioConfig.from_pretrained(args.model_in_path, mask_time_prob=mask_time_prob)\n",
    "                                                                                # use pre-trained config\n",
    "trained_model = Data2VecAudioForCTC.from_pretrained(args.model_in_path, config=config, args=args)\n",
    "                                                                                # load from source\n",
    "trained_model.config.ctc_zero_infinity = True                                           # to avoid inf values\n",
    "\n",
    "train_dataset, test_dataset = get_dataset(args) \n",
    "\n",
    "processor=Wav2Vec2Processor.from_pretrained(args.pretrain_name)\n",
    "# map_to_result_with_model = functools.partial(map_to_result, model=trained_model, processor=processor)\n",
    "map_to_result_with_model = lambda batch: map_to_result(batch, trained_model, processor)\n",
    "\n",
    "result = test_dataset.map(map_to_result_with_model)\n",
    "# processor=Wav2Vec2Processor.from_pretrained(args.pretrain_name)\n",
    "# model=trained_model\n",
    "# for i,batch in enumerate(test_dataset):\n",
    "#     with torch.no_grad():\n",
    "#         input_values = torch.tensor(batch[\"input_values\"]).unsqueeze(0)\n",
    "#         logits = model(input_values).logits\n",
    "\n",
    "#     pred_ids = torch.argmax(logits, dim=-1)\n",
    "#     batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "#     batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "\n",
    "print(\"WER of \", args.pretrain_name, \" : \", wer(result[\"text\"], result[\"pred_str\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2425338/3536326054.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'processor' is not defined"
     ]
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_result(batch, model):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"]).unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = model.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = model.decode(batch[\"labels\"], group_tokens=False)\n",
    "  \n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dataset:\n",
    "    # batch=test_dataset[0]\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"]).unsqueeze(0)\n",
    "        logits = trained_model(input_values).logits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flower-speechbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
