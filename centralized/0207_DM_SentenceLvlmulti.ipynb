{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu=1, epochs=5, lr=2e-05, random_seed=2023, t_embed='mbert', a_embed='en', SaveRoot='/mnt/External/Seagate/FedASR/LLaMa2/dacs', file_in='/home/FedASR/dacs/centralized/saves/results/data2vec-audio-large-960h_total.csv')\n",
      "Using PyTorch Ver 2.1.1+cu121\n",
      "Fix Seed: 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train:1868, val:206, test:800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_321402/3247309726.py:205: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  torch.tensor(df_train[a_col_name].tolist(), dtype=torch.float),\n",
      "Trainer will use only 1 of 5 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=5)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Start Training ::\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp_find_unused_parameters_true')` is not compatible with an interactive environment. Run your code as a script, or choose a notebook-compatible strategy: `Trainer(strategy='ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 460\u001b[0m\n\u001b[1;32m    458\u001b[0m     args \u001b[38;5;241m=\u001b[39m Arg()\n\u001b[1;32m    459\u001b[0m     args\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mepochs\n\u001b[0;32m--> 460\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m       \n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03mpython 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed en\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m \n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 416\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args, config)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:: Start Training ::\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m#     \u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_checkpointing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_dev_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_sanity_val_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# ensure full reproducibility from run to run you need to set seeds for pseudo-random generators,\u001b[39;49;00m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# For GPU Setup\u001b[39;49;00m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# gpus=[config['gpu']] if torch.cuda.is_available() else None,\u001b[39;49;00m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mddp_find_unused_parameters_true\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[1;32m    428\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model)\n\u001b[1;32m    430\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model,dataloaders\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtest_dataloader(),ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:401\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m _DataConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 401\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43m_AcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m _LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m _CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:165\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_init_precision()\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# 6. Instantiate Strategy - Part 2\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:605\u001b[0m, in \u001b[0;36m_AcceleratorConnector._lazy_init_strategy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_configure_launcher()\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_INTERACTIVE \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mis_interactive_compatible:\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer(strategy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy_flag\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m)` is not compatible with an interactive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m environment. Run your code as a script, or choose a notebook-compatible strategy:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    608\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `Trainer(strategy=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddp_notebook\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m In case you are spawning processes yourself, make sure to include the Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m creation inside the worker function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# TODO: should be moved to _check_strategy_and_fallback().\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# Current test check precision first, so keep this check here to meet error order\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, XLAAccelerator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, (SingleDeviceXLAStrategy, XLAStrategy)\n\u001b[1;32m    617\u001b[0m ):\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp_find_unused_parameters_true')` is not compatible with an interactive environment. Run your code as a script, or choose a notebook-compatible strategy: `Trainer(strategy='ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# torch:\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "from transformers import BertTokenizer, BertConfig, BertModel,XLMTokenizer, XLMModel\n",
    "import librosa\n",
    "\n",
    "class Arg:\n",
    "    version = 1\n",
    "    # data\n",
    "    epochs: int = 5  # Max Epochs, BERT paper setting [3,4,5]\n",
    "    max_length: int = 350  # Max Length input size\n",
    "    report_cycle: int = 30  # Report (Train Metrics) Cycle\n",
    "    cpu_workers: int = os.cpu_count()  # Multi cpu workers\n",
    "    test_mode: bool = False  # Test Mode enables `fast_dev_run`\n",
    "    optimizer: str = 'AdamW'  # AdamW vs AdamP\n",
    "    lr_scheduler: str = 'exp'  # ExponentialLR vs CosineAnnealingWarmRestarts\n",
    "    fp16: bool = False  # Enable train on FP16\n",
    "    a_hidden_size = 512 # BERT-base: 768, BERT-large: 1024, BERT paper setting\n",
    "    t_hidden_size = 768\n",
    "    t_x_hidden_size = a_hidden_size+t_hidden_size\n",
    "    batch_size: int = 8\n",
    "            \n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "    \n",
    "    \n",
    "class Model(LightningModule):\n",
    "    def __init__(self, args,config):\n",
    "        super().__init__()\n",
    "        # config:\n",
    "        \n",
    "        self.args = args\n",
    "        self.config = config\n",
    "        self.batch_size = self.args.batch_size\n",
    "        \n",
    "        # meta data:\n",
    "        self.epochs_index = 0\n",
    "        self.label_cols = 'dementia_labels'\n",
    "        self.label_names = ['Control','ProbableAD']\n",
    "        self.num_labels = 2\n",
    "        self.t_embed_type = self.config['t_embed']\n",
    "        self.a_embed_type = self.config['a_embed']\n",
    "        self.a_hidden = self.args.a_hidden_size\n",
    "        \n",
    "        # --> HERE STEP 1 <--\n",
    "        # ATTRIBUTES TO SAVE BATCH OUTPUTS\n",
    "        self.test_step_outputs = []   # save outputs in each batch to compute metric overall epoch\n",
    "        self.test_step_targets = []   # save targets in each batch to compute metric overall epoch\n",
    "        self.val_step_outputs = []        # save outputs in each batch to compute metric overall epoch\n",
    "        self.val_step_targets = []        # save targets in each batch to compute metric overall epoch\n",
    "\n",
    "\n",
    "        if self.t_embed_type == \"mbert\":\n",
    "            self.t_hidden = self.args.t_hidden_size\n",
    "            \n",
    "            t_pretrained = 'bert-base-multilingual-uncased'\n",
    "            self.t_tokenizer = BertTokenizer.from_pretrained(t_pretrained)\n",
    "            self.t_model = BertModel.from_pretrained(t_pretrained)\n",
    "            \n",
    "            \n",
    "        elif self.t_embed_type == \"xlm\":\n",
    "            self.t_hidden = self.args.t_x_hidden_size\n",
    "            \n",
    "            t_pretrained = 'xlm-mlm-100-1280'\n",
    "            self.t_tokenizer = XLMTokenizer.from_pretrained(t_pretrained)\n",
    "            self.t_model = XLMModel.from_pretrained(t_pretrained)\n",
    "            self.pooler = BertPooler(self.t_hidden)\n",
    "            \n",
    "        self.hidden = int(self.a_hidden + self.t_hidden)\n",
    "        \n",
    "        if self.a_embed_type == \"en\":\n",
    "            a_pretrained =  \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "            \n",
    "        elif self.a_embed_type == \"gr\":\n",
    "            a_pretrained =  \"lighteternal/wav2vec2-large-xlsr-53-greek\"\n",
    "\n",
    "        elif self.a_embed_type == \"multi\":\n",
    "            a_pretrained = \"voidful/wav2vec2-xlsr-multilingual-56\"\n",
    "            \n",
    "        elif self.a_embed_type == \"wv\":\n",
    "            a_pretrained ='facebook/wav2vec2-base'\n",
    "            \n",
    "        self.a_tokenizer = Wav2Vec2FeatureExtractor.from_pretrained(a_pretrained)\n",
    "        self.a_model = Wav2Vec2Model.from_pretrained(a_pretrained)\n",
    "        \n",
    "        \n",
    "        self.clf1 = nn.Linear(self.hidden, int(self.hidden/2))\n",
    "        self.clf2 = nn.Linear(int(self.hidden/2), self.num_labels)\n",
    "        \n",
    "            \n",
    "            \n",
    "    def forward(self, text, audio):\n",
    "    # def forward(self, text):\n",
    "        \n",
    "        if self.t_embed_type == \"mbert\":\n",
    "            t_out = self.t_model(text)[1] \n",
    "\n",
    "            \n",
    "        elif self.t_embed_type == \"xlm\":\n",
    "            t_out = self.t_model(text)[0]\n",
    "            t_out = self.pooler(t_out)\n",
    "            \n",
    "            \n",
    "        a_out = self.a_model(audio)['extract_features']#[2] #last_hidden_state , feature extraction\n",
    "        a_out = a_out[:, 0, :] \n",
    "        \n",
    "        # print(a_out)\n",
    "        # print(a_out['extract_features'].shape) # ([8, 437, 512])\n",
    "        # print(a_out['last_hidden_state'].shape) # ([8, 437, 1024]) => pooling 필요\n",
    "        \n",
    "        \n",
    "        output = torch.cat((t_out,a_out),axis=1)   \n",
    "        # output = t_out\n",
    "        # print(output.shape)\n",
    "        \n",
    "        logits = self.clf2(self.clf1(output))\n",
    "    \n",
    "        return logits\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config['lr'])\n",
    "        scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }\n",
    "\n",
    "    def preprocess_dataframe(self):\n",
    "        \n",
    "        tg_sr = 16000\n",
    "        t_col_name = \"text\" \n",
    "        a_col_name = \"path\"         \n",
    "        # df = pd.read_json('/mnt/Internal/FedASR/Data/230126_total_asr_data.json')\n",
    "        df = pd.read_csv(config.file_in)\n",
    "\n",
    "        df[t_col_name] = df[t_col_name].map(lambda x: self.t_tokenizer.encode(\n",
    "            str(x),\n",
    "            padding = 'max_length',\n",
    "            max_length=self.args.max_length,\n",
    "            truncation=True,\n",
    "            ))\n",
    "        \n",
    "        audio_root=\"/mnt/Internal/FedASR/Data/ADReSS-IS2020-data/clips\"\n",
    "        # 원래 길이: 562992, batch 16: 90000, batch 8: 140000\n",
    "        # max_length=16000, truncation=True 이건 일단 돌려보고 결정 => 뒤쪽, 앞에쪽 뭐보면 좋을 지 그런거 check하면 좋으니까! \n",
    "        df[a_col_name] = df[a_col_name].map(lambda x: self.a_tokenizer(\n",
    "            librosa.load(f\"{audio_root}/{x}\")[0],\n",
    "            padding='max_length',\n",
    "            sampling_rate = tg_sr,\n",
    "            max_length=100000, \n",
    "            truncation=True\n",
    "            )['input_values'][0])\n",
    "        \n",
    "        \n",
    "\n",
    "        df_train = df[df['ex'] == 'train']\n",
    "        df_val = df[df['ex'] == 'dev']\n",
    "        df_test = df[df['ex'] == 'test']\n",
    "\n",
    "\n",
    "        print(f'# of train:{len(df_train)}, val:{len(df_val)}, test:{len(df_test)}')\n",
    "\n",
    "        self.train_data = TensorDataset(\n",
    "            torch.tensor(df_train[t_col_name].tolist(), dtype=torch.long),\n",
    "            torch.tensor(df_train[a_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(df_train[self.label_cols].tolist(), dtype=torch.long),\n",
    "        )\n",
    "        \n",
    "        self.val_data = TensorDataset(\n",
    "             torch.tensor(df_val[t_col_name].tolist(), dtype=torch.long),\n",
    "             torch.tensor(df_val[a_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(df_val[self.label_cols].tolist(), dtype=torch.long),\n",
    "        )\n",
    "\n",
    "        self.test_data = TensorDataset(\n",
    "             torch.tensor(df_test[t_col_name].tolist(), dtype=torch.long),\n",
    "             torch.tensor(df_test[a_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(df_test[self.label_cols].tolist(), dtype=torch.long),\n",
    "             torch.tensor(df_test.index.tolist(), dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "\n",
    "        return DataLoader(\n",
    "            self.test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        token, audio, labels = batch  \n",
    "        # token,  labels = batch  \n",
    "        logits = self(token, audio) \n",
    "        # logits = self(token) \n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)   \n",
    "        \n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        token, audio, labels = batch  \n",
    "        # token, labels = batch  \n",
    "        logits = self(token, audio) \n",
    "        # logits = self(token) \n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)     \n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "        # --> HERE STEP 2 <--\n",
    "        self.val_step_outputs.append({\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        })\n",
    "        # self.val_step_targets.append(y_true)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "        \n",
    "            \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        token, audio, labels,id_ = batch \n",
    "        # token, labels,id_ = batch \n",
    "        print('id', id_)\n",
    "        logits = self(token, audio) \n",
    "        # logits = self(token) \n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "        # --> HERE STEP 2 <--\n",
    "        self.test_step_outputs.append({\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        })\n",
    "        # self.test_step_targets.append(y_true)\n",
    "        return {\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        # print(\"Value= \",self.val_step_outputs)\n",
    "        # print(\"type(self.val_step_outputs)=\",type(self.val_step_outputs))\n",
    "        # print(\"type(self.val_step_outputs[0])=\",type(self.val_step_outputs[0]))\n",
    "        # print(\"type(self.val_step_outputs[0] loss)=\",type(self.val_step_outputs[0]['loss']))\n",
    "        for i in self.val_step_outputs:\n",
    "            loss += i['loss'].cpu().detach()\n",
    "        _loss = loss / len(self.val_step_outputs)\n",
    "        loss = float(_loss)\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for i in self.val_step_outputs:\n",
    "            y_true += i['y_true']\n",
    "            y_pred += i['y_pred']\n",
    "            \n",
    "        y_pred = np.asanyarray(y_pred)#y_temp_pred y_pred\n",
    "        y_true = np.asanyarray(y_true)\n",
    "        \n",
    "        pred_dict = {}\n",
    "        pred_dict['y_pred']= y_pred\n",
    "        pred_dict['y_true']= y_true\n",
    "        \n",
    "        val_acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "        \n",
    "        self.log(\"val_acc\", val_acc)\n",
    "        # print(\"y_pred= \", y_pred)\n",
    "        # print('\\n\\n\\n')\n",
    "        # print(\"y_true= \", y_true)\n",
    "        # print('\\n\\n\\n')\n",
    "        print(\"-------val_report-------\")\n",
    "        metrics_dict = classification_report(y_true, y_pred,zero_division=1,\n",
    "                                             target_names = self.label_names, \n",
    "                                             output_dict=True)\n",
    "        df_result = pd.DataFrame(metrics_dict).transpose()\n",
    "        pprint(df_result)\n",
    "        \n",
    "\n",
    "        df_result.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_val.csv')\n",
    "\n",
    "        pred_df = pd.DataFrame(pred_dict)\n",
    "        pred_df.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_val_pred.csv')\n",
    "        self.val_step_outputs.clear()\n",
    "        # self.val_step_targets.clear()\n",
    "        return {'loss': _loss}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for i in self.test_step_outputs:\n",
    "            y_true += i['y_true']\n",
    "            y_pred += i['y_pred']\n",
    "            \n",
    "        y_pred = np.asanyarray(y_pred)#y_temp_pred y_pred\n",
    "        y_true = np.asanyarray(y_true)\n",
    "        \n",
    "        pred_dict = {}\n",
    "        pred_dict['y_pred']= y_pred\n",
    "        pred_dict['y_true']= y_true\n",
    "        \n",
    "        \n",
    "        print(\"-------test_report-------\")\n",
    "        metrics_dict = classification_report(y_true, y_pred,zero_division=1,\n",
    "                                             target_names = self.label_names, \n",
    "                                             output_dict=True)\n",
    "        df_result = pd.DataFrame(metrics_dict).transpose()\n",
    "        self.test_step_outputs.clear()\n",
    "        # self.test_step_targets.clear()\n",
    "        pprint(df_result)\n",
    "\n",
    "        df_result.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_test.csv')\n",
    "\n",
    "        pred_df = pd.DataFrame(pred_dict)\n",
    "        pred_df.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_test_pred.csv')\n",
    "\n",
    "    \n",
    "def main(args,config):\n",
    "    print(\"Using PyTorch Ver\", torch.__version__)\n",
    "    print(\"Fix Seed:\", config['random_seed'])\n",
    "    seed_everything( config['random_seed'])\n",
    "        \n",
    "    model = Model(args,config) \n",
    "    model.preprocess_dataframe()\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=f\"{SaveRoot}/Model/checkpoints\",\n",
    "        monitor='val_acc',\n",
    "        auto_insert_metric_name=True,\n",
    "        verbose=True,\n",
    "        mode='max', \n",
    "        save_top_k=1,\n",
    "      )    \n",
    "\n",
    "    print(\":: Start Training ::\")\n",
    "    #     \n",
    "    trainer = Trainer(\n",
    "        logger=False,\n",
    "        callbacks=[early_stop_callback,checkpoint_callback],\n",
    "        enable_checkpointing = True,\n",
    "        max_epochs=args.epochs,\n",
    "        fast_dev_run=args.test_mode,\n",
    "        num_sanity_val_steps=None if args.test_mode else 0,\n",
    "        deterministic=False, # ensure full reproducibility from run to run you need to set seeds for pseudo-random generators,\n",
    "        # For GPU Setup\n",
    "        # gpus=[config['gpu']] if torch.cuda.is_available() else None,\n",
    "        strategy='ddp_find_unused_parameters_true',\n",
    "        precision=16 if args.fp16 else 32\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model,dataloaders=model.test_dataloader(),ckpt_path=\"best\")\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "\n",
    "    parser = argparse.ArgumentParser(\"main.py\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--gpu\", type=int, default=1)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=5)\n",
    "    parser.add_argument(\"--lr\", type=float, default=2e-5, help=\"learning rate\")\n",
    "    parser.add_argument(\"--random_seed\", type=int, default=2023) \n",
    "    parser.add_argument(\"--t_embed\", type=str, default=\"mbert\") \n",
    "    parser.add_argument(\"--a_embed\", type=str, default=\"en\") \n",
    "    parser.add_argument(\"--SaveRoot\", type=str, default='/mnt/External/Seagate/FedASR/LLaMa2/dacs') \n",
    "    parser.add_argument(\"--file_in\", type=str, default='/home/FedASR/dacs/centralized/saves/results/data2vec-audio-large-960h_total.csv') \n",
    "    \n",
    "    \n",
    "    config = parser.parse_args(args=[])\n",
    "    SaveRoot=config.SaveRoot\n",
    "    \n",
    "    __file__ = os.path.abspath(\"__file__\")\n",
    "    script_path, file_extension = os.path.splitext(__file__)\n",
    "\n",
    "    # 使用os.path模組取得檔案名稱\n",
    "    script_name = os.path.basename(script_path)\n",
    "\n",
    "    Output_dir=f\"{SaveRoot}/result/{script_name}/\"\n",
    "    os.makedirs(Output_dir, exist_ok=True)\n",
    "\n",
    "    print(config)\n",
    "    args = Arg()\n",
    "    args.epochs=config.epochs\n",
    "    main(args,config.__dict__)       \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed en\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed xlm --a_embed en\n",
    "\n",
    "# don\n",
    "python 0207_DM_multi.py --gpu 0 --t_embed xlm --a_embed gr\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed gr\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train:1868, val:206, test:800\n"
     ]
    }
   ],
   "source": [
    "args = Arg()\n",
    "args.epochs=config.epochs\n",
    "model=Model(args, config.__dict__)\n",
    "model.preprocess_dataframe()\n",
    "self=model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=self.test_data\n",
    "# 创建一个 DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 获取下一个项目\n",
    "iterator = iter(dataloader)\n",
    "next_item = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101, 13907,   112,   161, 27948, 10483, 16856, 27756,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), tensor([[-1.5219, -2.1620, -2.2315,  ...,  0.3804,  0.3926,  0.2570]]), tensor([0]), tensor([2074])]\n"
     ]
    }
   ],
   "source": [
    "print(next_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self=model\n",
    "for batch in model.train_data:\n",
    "    token, audio, labels = batch \n",
    "    # token, labels,id_ = batch \n",
    "    print('id', id_)\n",
    "    logits = self(token, audio) \n",
    "    # logits = self(token) \n",
    "    \n",
    "    preds = logits.argmax(dim=-1)\n",
    "\n",
    "    y_true = list(labels.cpu().numpy())\n",
    "    y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "    # --> HERE STEP 2 <--\n",
    "    # self.test_step_outputs.append({\n",
    "    #     'y_true': y_true,\n",
    "    #     'y_pred': y_pred,\n",
    "    # })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, audio= token, audio\n",
    "print(audio)\n",
    "if self.t_embed_type == \"mbert\":\n",
    "    t_out = self.t_model(text)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_col_name = \"path\"\n",
    "tg_sr=16000\n",
    "t_col_name = \"text\" \n",
    "df = pd.read_csv(config.file_in)\n",
    "audio_root=\"/mnt/Internal/FedASR/Data/ADReSS-IS2020-data/clips\"\n",
    "# df[a_col_name] = df[a_col_name].map(lambda x: self.a_tokenizer(\n",
    "#     librosa.load(f\"{audio_root}/{x}\")[0],\n",
    "#     sampling_rate = tg_sr,\n",
    "#     max_length=100000, \n",
    "#     truncation=True\n",
    "#     )['input_values'][0])\n",
    "df[a_col_name] = df[a_col_name].map(lambda x: self.a_tokenizer(\n",
    "    librosa.load(f\"{audio_root}/{x}\")[0],\n",
    "    sampling_rate = tg_sr,\n",
    "    padding = 'max_length',\n",
    "    max_length=100000, \n",
    "    truncation=True\n",
    "    )['input_values'][0])\n",
    "df[t_col_name] = df[t_col_name].map(lambda x: self.t_tokenizer.encode(\n",
    "    str(x),\n",
    "    padding = 'max_length',\n",
    "    max_length=self.args.max_length,\n",
    "    truncation=True,\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
