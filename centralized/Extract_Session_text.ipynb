{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from addict import Dict\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "import argparse \n",
    "import os\n",
    "from bleu import list_bleu\n",
    "from fastchat.model import load_model, get_conversation_template\n",
    "from langchain.prompts import PromptTemplate\n",
    "import torch\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "from addict import Dict\n",
    "\n",
    "import openai\n",
    "#importing the necessary dependencies\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, FewShotPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain import hub\n",
    "from rouge_score import rouge_scorer\n",
    "import subprocess\n",
    "\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--using_text_mode', type=str, default='text', help=\"[text | pred_str]\")\n",
    "parser.add_argument('--input_file', type=str, default='./saves/results/data2vec-audio-large-960h_test.csv', help=\"[./saves/results/data2vec-audio-large-960h.csv | data2vec-audio-large-960h.train]\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "using_text_mode=args.using_text_mode\n",
    "input_file=args.input_file\n",
    "\n",
    "ASR_name, ds_tag=os.path.basename(input_file).replace(\".csv\",\"\").split(\"_\")\n",
    "\n",
    "df_test=pd.read_csv(input_file)\n",
    "\n",
    "\n",
    "def Augment_info_df(df_test):\n",
    "    # 將 'path' 欄位進行字串操作\n",
    "    df_test['path'] = df_test['path'].str.rstrip('.wav')\n",
    "\n",
    "    # 使用 str.split 拆分 'path' 欄位\n",
    "    df_test[['session', 'role', 'number', 'start_time', 'end_time']] = df_test['path'].str.split('_', expand=True)\n",
    "\n",
    "    # 如果 'number' 欄位的末尾包含 '.wav'，進行一次額外的拆分\n",
    "    df_test['number'] = df_test['number'].str.rstrip('.wav')\n",
    "\n",
    "    # 將 'start_time' 和 'end_time' 欄位轉換為數值型別\n",
    "    df_test[['start_time', 'end_time']] = df_test[['start_time', 'end_time']].astype(int)\n",
    "\n",
    "    return df_test\n",
    "df_test = Augment_info_df(df_test)\n",
    "\n",
    "\n",
    "\n",
    "# Packer\n",
    "def Packer(df_test) -> dict:\n",
    "    People_dict = dict(tuple(df_test.groupby('session')))\n",
    "    return People_dict\n",
    "People_dict=Packer(df_test)\n",
    "# Filterer\n",
    "\n",
    "\n",
    "\n",
    "def filter_people_dict(People_dict, mode=\"INV+PAR\", verbose=False) -> dict:# 'INV' , 'PAR', 'INV+PAR'\n",
    "    filtered_people_dict = {}\n",
    "\n",
    "    for session, data_frame in People_dict.items():\n",
    "        # 使用 query 過濾 'role' 為 'INV' 或 'PAR'\n",
    "        if mode == \"PAR\":\n",
    "            filtered_data_frame = data_frame.query(\"role == 'PAR'\")\n",
    "            filtered_people_dict[session] = filtered_data_frame\n",
    "        elif mode == \"INV\":\n",
    "            filtered_data_frame = data_frame.query(\"role == 'INV'\")\n",
    "            filtered_people_dict[session] = filtered_data_frame\n",
    "        elif mode == \"INV+PAR\":\n",
    "            filtered_people_dict[session] = data_frame\n",
    "        else:\n",
    "            raise OSError\n",
    "    \n",
    "    if verbose:\n",
    "        # 印出過濾後的 People_dict 中每個 session 的 DataFrame\n",
    "        for session, data_frame in filtered_people_dict.items():\n",
    "            print(f\"Session: {session}\")\n",
    "            print(data_frame)\n",
    "            print(\"\\n\")\n",
    "\n",
    "    return filtered_people_dict\n",
    "role_mode=\"INV+PAR\"\n",
    "People_dict = filter_people_dict(People_dict, mode=role_mode, verbose=False)\n",
    "\n",
    "def Dialogueturn2corpus(data_frame, mode='text'): #mode can be 'pred_str' or 'text'\n",
    "    # 按 'start_time' 列進行排序\n",
    "    sorted_data_frame = data_frame.sort_values(by='start_time')\n",
    "\n",
    "    # 添加前綴並使用 '\\n' 進行拼接\n",
    "    processed_text = sorted_data_frame.apply(lambda row: f\"{row['role']}: {row[mode]}\", axis=1).str.cat(sep='\\n')\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "# Dialogue Formatter\n",
    "def Dialogue_Formatter(People_dict, sep=\"\\n\",role_mode='PAR')->dict:\n",
    "    session_df=pd.DataFrame()\n",
    "    for session, data_frame in People_dict.items():\n",
    "        total_info=data_frame.iloc[0].copy()\n",
    "        sessional_text = Dialogueturn2corpus(data_frame,mode='text')\n",
    "        sessional_predStr = Dialogueturn2corpus(data_frame,mode='pred_str')\n",
    "        \n",
    "        # total_info,'text']=sessional_text\n",
    "        # session_df.loc[session,'pred_str']=sessional_predStr\n",
    "        # session_df.loc[session,'role']=role_mode\n",
    "        # session_df.loc[session,'start_time']=data_frame['start_time'].min()\n",
    "        # session_df.loc[session,'end_time']=data_frame['end_time'].max()\n",
    "\n",
    "        total_info['text']=sessional_text\n",
    "        total_info['pred_str']=sessional_predStr\n",
    "        total_info['role']=role_mode\n",
    "        total_info['start_time']=data_frame['start_time'].min()\n",
    "        total_info['end_time']=data_frame['end_time'].max()\n",
    "        session_df = pd.concat([session_df, pd.DataFrame([total_info], index=[session])])\n",
    "    return session_df\n",
    "\n",
    "session_df=Dialogue_Formatter(People_dict,role_mode=role_mode)\n",
    "\n",
    "#########====================end Retreiver area==============================\n",
    "class RAG_chatbot:\n",
    "    def __init__(self):\n",
    "        self.retreiver = None\n",
    "        self.stepback_model = None\n",
    "        self.answer_model = None\n",
    "        self.few_shot_prompt=None\n",
    "\n",
    "    def Initialize_openai(self,env_script_path = './env.sh'):\n",
    "        #activate environment\n",
    "        # 读取 env.sh 文件内容\n",
    "        with open(env_script_path, 'r') as file:\n",
    "            env_content = file.read()\n",
    "\n",
    "        # 将 env.sh 文件内容以换行符分割，并逐行执行\n",
    "        for line in env_content.split('\\n'):\n",
    "            # 跳过注释和空行\n",
    "            if line.strip() and not line.startswith('#'):\n",
    "                # 使用 split 等号来分割键值对\n",
    "                key, value = line.split('=', 1)\n",
    "                # 设置环境变量\n",
    "                os.environ[key.replace(\"export \",\"\")] = value.strip()\n",
    "        #Initialize openai\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_version = \"2023-05-15\" \n",
    "        openai.api_base = os.getenv('OPENAI_API_BASE')  # Your Azure OpenAI resource's endpoint value.\n",
    "        openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "        chatopenai=ChatOpenAI(api_key=openai.api_key,model_kwargs={\"engine\": \"gpt-35-turbo\"})\n",
    "        return chatopenai\n",
    "    def Initialize_Embedder(self):\n",
    "        os.environ[\"AZURE_OPENAI_API_KEY\"] = openai.api_key\n",
    "        os.environ[\"AZURE_OPENAI_ENDPOINT\"] = openai.api_base\n",
    "\n",
    "\n",
    "        embedder = AzureOpenAIEmbeddings(\n",
    "            azure_deployment=\"text-embedding-ada-002\",\n",
    "            openai_api_version=\"2023-05-15\",\n",
    "        )\n",
    "        return embedder\n",
    "\n",
    "    def Initialize_fewshot_prompt(self, user_input):\n",
    "        # 在知識庫中搜尋與使用者輸入相關的資訊\n",
    "        # 這裡假設 knowledge_base 是一個包含資訊的字典或其他數據結構\n",
    "        if user_input in self.knowledge_base:\n",
    "            return self.knowledge_base[user_input]\n",
    "        else:\n",
    "            return None\n",
    "RAG_bot=RAG_chatbot()\n",
    "chatopenai=RAG_bot.Initialize_openai()\n",
    "Embedder=RAG_bot.Initialize_Embedder()\n",
    "# Transform to Emb\n",
    "# def Transform_to_Emb(dict: data, prompt, chatopenai, )\n",
    "    \n",
    "#     return Emb\n",
    "\n",
    "\n",
    "from prompts import assesmentPrompt_template, Instruction_templates, Psychology_template,\\\n",
    "    Sensitive_replace_dict, generate_psychology_prompt, mmse_analyze_selected_people\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_sessions(session_df, prompt_template, chatopenai, Sensitive_replace_dict, Embedder, use_text='text',\\\n",
    "    selected_people=[]):\n",
    "    Embedding_dict, Summary_dict, Prompt_dict = {}, {}, {}\n",
    "    for session, row in session_df.iterrows():\n",
    "        if not selected_people:\n",
    "            if session not in selected_people:\n",
    "                continue\n",
    "        \n",
    "        if session in Sensitive_replace_dict.keys():\n",
    "            dialogue_content = row[use_text]\n",
    "            for values in Sensitive_replace_dict[session]:\n",
    "                dialogue_content = dialogue_content.replace(values[0], values[1])\n",
    "            \n",
    "        else:\n",
    "            dialogue_content = row[use_text]\n",
    "\n",
    "        prompt=prompt_template.format(dialogue_content=dialogue_content)\n",
    "        ans_middle = chatopenai.invoke(prompt)\n",
    "\n",
    "        output_parser = StrOutputParser()\n",
    "        summary = output_parser.parse(ans_middle).content\n",
    "        embeddings = Embedder.embed_query(summary)\n",
    "        Embedding_dict[session] = embeddings\n",
    "        Summary_dict[session] = summary\n",
    "        Prompt_dict[session] = prompt\n",
    "\n",
    "    session_df['Embedding'] = session_df.index.to_series().apply(lambda x: Embedding_dict.get(x, []))\n",
    "    session_df['Psych_Summary'] = session_df.index.to_series().apply(lambda x: Summary_dict.get(x, []))\n",
    "    session_df['Psych_Prompt'] = session_df.index.to_series().apply(lambda x: Prompt_dict.get(x, []))\n",
    "    return session_df\n",
    "\n",
    "prompts_dict = generate_psychology_prompt(assessment_prompt_template=assesmentPrompt_template,\n",
    "                                            instruction_templates=Instruction_templates,\n",
    "                                            psychology_template=Psychology_template,\n",
    "                                            )\n",
    "\n",
    "args.selected_psych=['anomia']\n",
    "\n",
    "# result_prompts={k:v for k,v in prompts_dict.items() if k in selected_psych}\n",
    "result_prompts={k:v for k,v in  prompts_dict.items()}\n",
    "for key, prompt_template in tqdm(result_prompts.items()):\n",
    "    # session_df = process_sessions(session_df, prompt_template, chatopenai, Sensitive_replace_dict, Embedder, use_text=using_text_mode,\n",
    "    #                               selected_people=mmse_analyze_selected_people)\n",
    "    session_df = process_sessions(session_df, prompt_template, chatopenai, Sensitive_replace_dict, Embedder, use_text=using_text_mode)\n",
    "    prompt_name=f\"Phych-{key}\"\n",
    "\n",
    "    \n",
    "\n",
    "    # OutFile_path=f\"dacs/centralized/EmbFeats/Lexical/TextSummarize_Emb.pkl\"\n",
    "    # Output_Root=\"EmbFeats/Lexical\"\n",
    "    Output_Root=f\"/mnt/External/Seagate/FedASR/LLaMa2/dacs/EmbFeats/Lexical/Embeddings/{using_text_mode}_{ASR_name}_{prompt_name}\"\n",
    "    if not os.path.exists(Output_Root):\n",
    "        os.makedirs(Output_Root)\n",
    "\n",
    "\n",
    "    OutFile_path=f\"{Output_Root}/{ds_tag}.pkl\"\n",
    "    # Save the DataFrame as a pickle file\n",
    "    print(f\"File saved at {OutFile_path}\")\n",
    "    session_df.to_pickle(OutFile_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check_nan_inf(df, column_name):\n",
    "    nan_inf_check = df[column_name].apply(lambda x: any(pd.isna(x)) or any(np.isinf(x)))\n",
    "    rows_with_nan_inf = df[nan_inf_check]\n",
    "    return rows_with_nan_inf\n",
    "\n",
    "# Assuming session_df is the DataFrame you want to check\n",
    "# Replace this with the actual logic to create or modify session_df\n",
    "# ...\n",
    "\n",
    "# Specify the column name to check\n",
    "column_to_check = 'Embedding'\n",
    "\n",
    "# Call the function\n",
    "rows_with_nan_inf = check_nan_inf(session_df, column_to_check)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Rows with NaN or Inf in '{column_to_check}' column:\")\n",
    "print(rows_with_nan_inf)\n",
    "assert len(rows_with_nan_inf)==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 這邊手動check一下讓passage能夠至少送出去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sensitive=\"\"\"INV: WHAT DO YOU SEE GOING ON IN THAT PICTURE\n",
    "# PAR: OH YEAH\n",
    "# PAR: KID'S CLIMBING UP ON THE STOOL AND REACHING UP IN THE CUPBOARD\n",
    "# PAR: THEY AREN'T GONNA KNOCK THINGS OFF\n",
    "# PAR: AND UH THE MOTHER OH BOY THE WATER'S ALL SPILLING OUT OF THE SINK\n",
    "# PAR: SHE'S JUST LOOKING AT IT LIKE\n",
    "# PAR: OH FOR GOODNESS SAKES\n",
    "# PAR: HUH\n",
    "# INV: ANYTHING ELSE\n",
    "# PAR: UH HM WELL AS I SAY THAT UH YOU KNOW THE BOY ON THE S STAND OR ON THAT STOOL AND IT'S TILTING\n",
    "# PAR: THAT WILL HURT HIMSELF\n",
    "# PAR: LIKE HURT HIS BACK I SHOULDA SAID\n",
    "# PAR: WOMAN LEFT HER FAUCET RUNNING AND IT ROLLED OVER THE FLOOR\n",
    "# PAR: THAT'S BAD ENOUGH FOR ME TO DO\n",
    "# INV: IS THAT IT\n",
    "# PAR: UM LET ME SCAN IT A LITTLE BIT MORE\n",
    "# PAR: UH OH IT'S THERE MORE IN TIME\n",
    "# INV: NO NO THAT'S ALL\n",
    "# PAR: OH OH\n",
    "# INV: NO IT'S THAT ONE\n",
    "# INV: IS THAT IT\n",
    "# PAR: YEAH\n",
    "# INV: OKAY\n",
    "# \"\"\"\n",
    "\n",
    "sensitive=\"\"\"INV: TAKE A LOOK AT THAT PICTURE AND TELL ME EVERYTHING THAT'S GOING ON IN THAT PICTURE EVERYTHING THAT YOU SEE HAPPENING THERE\n",
    "PAR: WELL THE LITTLE KID'S FALLING OFF HIS CHAIR\n",
    "INV: MHM\n",
    "PAR: AND THE MOTHER IS HAVING WATER RUN OVER THE UH SINK\n",
    "INV: ANYTHING ELSE GOING ON IN THAT PICTURE\n",
    "PAR: WELL THE WATER'S RUNNING ON THE FLOOR\n",
    "INV: MHM\n",
    "PAR: UNDER HER FEET\n",
    "INV: MHM\n",
    "PAR: I'M LOOKING OUTSIDE BUT THAT YARD IS OKAY\n",
    "PAR: THE WINDOWS ARE OPEN\n",
    "PAR: THE LITTLE GIRL FINDS AMUSEMENT IN THE BOY ACCIDENTALLY FALLING OFF THE CHAIR\n",
    "PAR: THAT THAT'S BAD\n",
    "INV: OKAY THANKS\n",
    "\"\"\"\n",
    "rephrase_prefix=\"Can you detect all sensitive terms that involves: Sexual, Violence, and Self-Harm, rephrase them for me \"\n",
    "Rephrase_template=\"\"\"{rephrase_prefix}\n",
    "\n",
    "{dialogue_content}\n",
    "\"\"\"\n",
    "\n",
    "rephrased_prompt=Rephrase_template.format(rephrase_prefix=prompt_prefix, dialogue_content=sensitive)\n",
    "\n",
    "ans_middle=chatopenai.invoke(rephrased_prompt)\n",
    "print(rephrased_prompt)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session_df.loc[\"S205\",'text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
