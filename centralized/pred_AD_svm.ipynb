{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train w/ PAR only...\n",
      "Remaining training samples:  1327\n",
      "Remaining testing data:  584\n",
      "The result:                                                 model       ACC      BACC  \\\n",
      "0  data2vec-audio-large-960h_ori_INV_False_min sp...  0.645833  0.645833   \n",
      "\n",
      "         F1   Sens      Spec       UAR  \n",
      "0  0.638298  0.625  0.666667  0.645833  \n"
     ]
    }
   ],
   "source": [
    "# predict AD using (masked) embeddings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import argparse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from utils import ID2Label\n",
    "import os\n",
    "import pickle\n",
    "def trainSVM(x_train, y_train, x_test, y_test, df_test, title, outdir=\"./saves/results/SVM\"):\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(x_train)\n",
    "    x_train_std = sc.transform(x_train)\n",
    "    x_test_std = sc.transform(x_test)\n",
    "    \n",
    "    svm = SVC() #class_weight='balanced')\n",
    "    svm.fit(x_train_std, y_train[\"dementia_labels\"].values)\n",
    "    \n",
    "    pred = svm.predict(x_test_std)\n",
    "    true = y_test[\"dementia_labels\"].values\n",
    "    \"\"\"\n",
    "    # utt-wise results\n",
    "    cm = confusion_matrix(true, pred)\n",
    "    \n",
    "    # save results\n",
    "    df = pd.read_csv(\"./saves/results/SVM/results.csv\")                           # read in previous results\n",
    "    new_row = {'model': title + \" utt-wise\",\n",
    "               'ACC':accuracy_score(true, pred), 'BACC':balanced_accuracy_score(true, pred), 'F1':f1_score(true, pred),\n",
    "               'Sens':recall_score(true, pred), 'Spec':cm[0,0]/(cm[0,0]+cm[0,1]), 'UAR': recall_score(true, pred, average='macro')}\n",
    "                                                                                  # result to save\n",
    "    df2 = pd.DataFrame([new_row])\n",
    "    df3 = pd.concat((df, df2), axis = 0)                                          # append row\n",
    "    df3.to_csv(\"./saves/results/SVM/results.csv\", index=False)\n",
    "    \"\"\"\n",
    "    sorted_dict = {}                                                              # sort based on spk id\n",
    "    for idx, i in enumerate(df_test.index.tolist()): \n",
    "        id_part = df_test['path'][i].split('_')                                   # split file name   \n",
    "        if id_part[1] == 'PAR':                                                   # predict only on participant\n",
    "            if id_part[0] not in sorted_dict.keys():                              # new spk\n",
    "                sorted_dict[id_part[0]] = [pred[idx]]                             # add values to this spk\n",
    "            else:\n",
    "                sorted_dict[id_part[0]].append(pred[idx])                         # append to existing list\n",
    "\n",
    "    true = []                                                                     # ground truth\n",
    "    pred = []                                                                     # prediction\n",
    "    for spkid in sorted_dict.keys():                                              # for each spk\n",
    "        true_label = ID2Label(spkid + '_PAR')                                     # get his/her label\n",
    "        true.append(true_label)                                                   # add to list\n",
    "\n",
    "        vote = sum(sorted_dict[spkid]) / len(sorted_dict[spkid])                  # average result of predictions\n",
    "        if vote > 0.5:                                                            # over half of the pred is AD\n",
    "            pred.append(1)                                                        # view as AD\n",
    "        else:\n",
    "            pred.append(0)                                                        # view as HC\n",
    "    \n",
    "    cm = confusion_matrix(true, pred)\n",
    "    \n",
    "    # save results\n",
    "    df = pd.read_csv(f\"{outdir}/results.csv\")                           # read in previous results\n",
    "    new_row = {'model': title + \" spkid-wise\",\n",
    "               'ACC':accuracy_score(true, pred), 'BACC':balanced_accuracy_score(true, pred), 'F1':f1_score(true, pred),\n",
    "               'Sens':recall_score(true, pred), 'Spec':cm[0,0]/(cm[0,0]+cm[0,1]), 'UAR': recall_score(true, pred, average='macro')}\n",
    "                                                                                  # result to save\n",
    "    df2 = pd.DataFrame([new_row])\n",
    "    df3 = pd.concat((df, df2), axis = 0)                                          # append row\n",
    "    print(\"The result: \", df2)\n",
    "    df3.to_csv(f\"{outdir}/results.csv\", index=False)\n",
    "\n",
    "\n",
    "def df2xy_masked(df_data, feat_col=\"masked_hidden_states\"):\n",
    "    # re = df_train.hidden_states * df_train.lm_mask\n",
    "    re = df_data.hidden_states.copy() * df_data.lm_mask.copy()\n",
    "    # re = df_data.hidden_states.copy()\n",
    "    for idx, i in enumerate(df_data.index.tolist()):\n",
    "        re[i] = pooling_func(re[i], axis=0)# 平均成(1, hidden_size)\n",
    "        #  = data[0]                                                      # 轉成(hidden_size)\n",
    "        #print(re[i].shape)0\n",
    "        print(\"\\r\"+ str(idx+1), end=\"\")\n",
    "    print(\" \")\n",
    "    x_train = pd.DataFrame(re, columns=[feat_col])[feat_col].tolist() # masked_hidden_states to list\n",
    "    y_train = pd.DataFrame(df_data[\"dementia_labels\"])\n",
    "    return x_train,y_train\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-model', '--model_name', type=str, default=\"data2vec-audio-large-960h\", help=\"name of the desired model, ex: FSM_indiv_AMLoss_5_lmFSM\")\n",
    "parser.add_argument('-INV', '--INV', action='store_true', default=False, help=\"True: train w/ INV\")\n",
    "parser.add_argument('-sq', '--squeeze', type=str, default=\"min\", help=\"way to squeeze hidden_states, 'mean', 'min', and 'max'\")\n",
    "parser.add_argument('-Audio_dataIn', '--Audio_dataIn_dir', type=str, default=\"/home/FedASR/dacs/centralized/saves/results/\", help=\"\")\n",
    "parser.add_argument('-Lexical_dataIn', '--Lexical_dataIn_dir', type=str, default=\"/mnt/External/Seagate/FedASR/LLaMa2/dacs/EmbFeats/Lexical/Embeddings/text_data2vec-audio-large-960h_prompt_No1\", help=\"\")\n",
    "parser.add_argument('-rsltOut', '--rsltOut_dir', type=str, default=\"/mnt/External/Seagate/FedASR/LLaMa2/dacs/EmbFeats/Lexical/results/SVM/\", help=\"\")\n",
    "parser.add_argument('--mode', default='text', help=\"True: train w/ INV\")\n",
    "args = parser.parse_args(args=[])\n",
    "sqz = args.squeeze\n",
    "mode=args.mode\n",
    "\n",
    "Lexical_dataIn_dir=args.Lexical_dataIn_dir\n",
    "model_name=args.model_name\n",
    "\n",
    "\n",
    "# 注意df_text系列data是session level的\n",
    "df_text_train = pd.read_pickle(f\"{Lexical_dataIn_dir}/train.pkl\")\n",
    "df_text_test = pd.read_pickle(f\"{Lexical_dataIn_dir}/test.pkl\")\n",
    "\n",
    "Str2Func={\n",
    "    \"mean\":np.mean,\n",
    "    \"min\":np.min,\n",
    "    \"max\":np.max,\n",
    "    \"median\":np.median,\n",
    "}\n",
    "pooling_func=Str2Func[sqz]\n",
    "\n",
    "suffix='.pkl'\n",
    "if not os.path.exists(args.rsltOut_dir):\n",
    "    os.makedirs(args.rsltOut_dir)\n",
    "# load in train / test data for certain model\n",
    "# df_train = pd.read_csv(f\"{args.Audio_dataIn_dir}\" + args.model_name + \"_train.csv\")\n",
    "# df_train = pd.read_csv(f\"{args.Audio_dataIn_dir}\" + args.model_name + \".csv\") \n",
    "# df_test = pd.read_csv(f\"{args.Audio_dataIn_dir}\" + args.model_name + \".csv\")\n",
    "# training_pkl=f\"{args.Audio_dataIn_dir}\" + args.model_name + '_train' + suffix\n",
    "# testing_pkl=f\"{args.Audio_dataIn_dir}\" + args.model_name + '' + suffix\n",
    "\n",
    "# Audio的data寫死在這邊\n",
    "training_pkl=\"/home/FedASR/dacs/centralized/saves/results/data2vec-audio-large-960h_train.pkl\"\n",
    "testing_pkl=\"/home/FedASR/dacs/centralized/saves/results/data2vec-audio-large-960h.pkl\"\n",
    "with open(training_pkl, \"rb\") as f:\n",
    "    df_train = pickle.load(f)\n",
    "with open(testing_pkl, \"rb\") as f:\n",
    "    df_test = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not args.INV:\n",
    "    print(\"Train w/ PAR only...\")\n",
    "    df_train = df_train[df_train.path.str.contains(\"PAR\")]                        # train w/ PAR only\n",
    "    print(\"Remaining training samples: \", len(df_train))                          # show # of utt left\n",
    "    df_test = df_test[df_test.path.str.contains(\"PAR\")]                           # train w/ PAR only\n",
    "    print(\"Remaining testing data: \", len(df_test))                               # show # of utt left\n",
    "\n",
    "def df_fusion_2xy(df_data, df_text_data, main_feat_col=\"hidden_states\",assist_feat_col='Embedding'):\n",
    "    # 在 df_data 中新增 'session' 欄位\n",
    "    df_data['session'] = df_data['path'].str.split('_').str[0]\n",
    "    # 迭代 df_train，尋找相應的 'session' 並將 'embedding' 添加到 'embedding' 欄位中\n",
    "    for index, row in df_data.iterrows():\n",
    "        session = row['session']\n",
    "        matching_row = df_text_data[df_text_data['session'] == session]\n",
    "        \n",
    "        audio_embedding=pooling_func(row['hidden_states'], axis=0)\n",
    "        if not matching_row.empty: \n",
    "            text_embedding = np.array(matching_row[assist_feat_col].values[0])\n",
    "            fusion_embedding=np.concatenate([audio_embedding,text_embedding],axis=0)\n",
    "\n",
    "            df_data.at[index, 'hidden_states'] = fusion_embedding\n",
    "    re = df_data[main_feat_col].copy()\n",
    "    x_train = pd.DataFrame(re, columns=[main_feat_col])[main_feat_col].tolist()\n",
    "    y_train = pd.DataFrame(df_data[\"dementia_labels\"])\n",
    "    return x_train,y_train\n",
    "########################\n",
    "def df2xy(df_data, feat_col=\"hidden_states\"):\n",
    "    re = df_data[feat_col].copy()\n",
    "    for idx, i in enumerate(df_data.index.tolist()):\n",
    "        re[i] = pooling_func(re[i], axis=0)# 平均成(1, hidden_size)\n",
    "        #  = data[0]                                                      # 轉成(hidden_size)\n",
    "        #print(re[i].shape)0\n",
    "        print(\"\\r\"+ str(idx+1), end=\"\")\n",
    "    print(\" \")\n",
    "    x_train = pd.DataFrame(re, columns=[feat_col])[feat_col].tolist() # masked_hidden_states to list\n",
    "    y_train = pd.DataFrame(df_data[\"dementia_labels\"])\n",
    "    return x_train,y_train\n",
    "def df_text2xy(df_data, df_text_data,  main_feat_col=\"hidden_states\",assist_feat_col='Embedding'):\n",
    "    # 在 df_data 中新增 'session' 欄位\n",
    "    df_data['session'] = df_data['path'].str.split('_').str[0]\n",
    "    # 迭代 df_train，尋找相應的 'session' 並將 'embedding' 添加到 'embedding' 欄位中\n",
    "    for index, row in df_data.iterrows():\n",
    "        session = row['session']\n",
    "        matching_row = df_text_data[df_text_data['session'] == session]\n",
    "        \n",
    "        # audio_embedding=pooling_func(row['hidden_states'], axis=0)\n",
    "        if not matching_row.empty: \n",
    "            text_embedding = np.array(matching_row[assist_feat_col].values[0])\n",
    "            # fusion_embedding=np.concatenate([audio_embedding,text_embedding],axis=0)\n",
    "            # text_embedding.shape\n",
    "            df_data.at[index, main_feat_col] = text_embedding\n",
    "    re = df_data[main_feat_col].copy()\n",
    "    x_train = pd.DataFrame(re, columns=[main_feat_col])[main_feat_col].tolist()\n",
    "    y_train = pd.DataFrame(df_data[\"dementia_labels\"])\n",
    "    return x_train,y_train\n",
    "\n",
    "Only_text=True\n",
    "if mode=='fusion':\n",
    "    x_train,y_train=df_fusion_2xy(df_train, df_text_train, main_feat_col=\"hidden_states\",assist_feat_col='Embedding')\n",
    "    x_test,y_test=df_fusion_2xy(df_test, df_text_test, main_feat_col=\"hidden_states\",assist_feat_col='Embedding')\n",
    "elif mode=='text':\n",
    "    x_train,y_train=df_text2xy(df_train,df_text_train, main_feat_col=\"hidden_states\",assist_feat_col='Embedding')\n",
    "    x_test,y_test=df_text2xy(df_test,df_text_test, main_feat_col=\"hidden_states\",assist_feat_col='Embedding')\n",
    "else:\n",
    "    x_train,y_train=df2xy(df_train, feat_col=\"hidden_states\")\n",
    "    x_test,y_test=df2xy(df_test, feat_col=\"hidden_states\")\n",
    "\n",
    "# Train SVM start here\n",
    "title=args.model_name + \"_ori_INV_\" + str(args.INV) + \"_\" + sqz\n",
    "outdir=args.rsltOut_dir\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(x_train)\n",
    "x_train_std = sc.transform(x_train)\n",
    "x_test_std = sc.transform(x_test)\n",
    "\n",
    "svm = SVC() #class_weight='balanced')\n",
    "svm.fit(x_train_std, y_train[\"dementia_labels\"].values)\n",
    "\n",
    "pred = svm.predict(x_test_std)\n",
    "true = y_test[\"dementia_labels\"].values\n",
    "\"\"\"\n",
    "# utt-wise results\n",
    "cm = confusion_matrix(true, pred)\n",
    "\n",
    "# save results\n",
    "df = pd.read_csv(\"./saves/results/SVM/results.csv\")                           # read in previous results\n",
    "new_row = {'model': title + \" utt-wise\",\n",
    "            'ACC':accuracy_score(true, pred), 'BACC':balanced_accuracy_score(true, pred), 'F1':f1_score(true, pred),\n",
    "            'Sens':recall_score(true, pred), 'Spec':cm[0,0]/(cm[0,0]+cm[0,1]), 'UAR': recall_score(true, pred, average='macro')}\n",
    "                                                                                # result to save\n",
    "df2 = pd.DataFrame([new_row])\n",
    "df3 = pd.concat((df, df2), axis = 0)                                          # append row\n",
    "df3.to_csv(\"./saves/results/SVM/results.csv\", index=False)\n",
    "\"\"\"\n",
    "sorted_dict = {}                                                              # sort based on spk id\n",
    "for idx, i in enumerate(df_test.index.tolist()): \n",
    "    id_part = df_test['path'][i].split('_')                                   # split file name   \n",
    "    if id_part[1] == 'PAR':                                                   # predict only on participant\n",
    "        if id_part[0] not in sorted_dict.keys():                              # new spk\n",
    "            sorted_dict[id_part[0]] = [pred[idx]]                             # add values to this spk\n",
    "        else:\n",
    "            sorted_dict[id_part[0]].append(pred[idx])                         # append to existing list\n",
    "\n",
    "true = []                                                                     # ground truth\n",
    "pred = []                                                                     # prediction\n",
    "for spkid in sorted_dict.keys():                                              # for each spk\n",
    "    true_label = ID2Label(spkid + '_PAR')                                     # get his/her label\n",
    "    true.append(true_label)                                                   # add to list\n",
    "\n",
    "    vote = sum(sorted_dict[spkid]) / len(sorted_dict[spkid])                  # average result of predictions\n",
    "    if vote > 0.5:                                                            # over half of the pred is AD\n",
    "        pred.append(1)                                                        # view as AD\n",
    "    else:\n",
    "        pred.append(0)                                                        # view as HC\n",
    "\n",
    "cm = confusion_matrix(true, pred)\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "# save results\n",
    "prev_savedFile=f\"{outdir}/results.csv\"\n",
    "if not os.path.exists(prev_savedFile):\n",
    "    df=pd.DataFrame()\n",
    "else:\n",
    "    df=pd.read_csv(prev_savedFile)  # read in previous results\n",
    "new_row = {'model': title + \" spkid-wise\",\n",
    "            'ACC':accuracy_score(true, pred), 'BACC':balanced_accuracy_score(true, pred), 'F1':f1_score(true, pred),\n",
    "            'Sens':recall_score(true, pred), 'Spec':cm[0,0]/(cm[0,0]+cm[0,1]), 'UAR': recall_score(true, pred, average='macro')}\n",
    "                                                                                # result to save\n",
    "df2 = pd.DataFrame([new_row])\n",
    "print(\"The result: \", df2)\n",
    "df3 = pd.concat((df, df2), axis = 0)                                          # append row\n",
    "df3.to_csv(f\"{outdir}/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/External/Seagate/FedASR/LLaMa2/dacs/EmbFeats/Lexical/results/SVM/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Syntactic and Grammatical Impairments\n",
    "# Example 1: Sentence Construction Issues\n",
    "[\"Sentence Construction Issues: He, um, went to the store and, uh, bought some, you know, groceries for dinner.\",\n",
    "\"Impaired Syntax: Yesterday, I was, you know, meeting my friend for lunch, and we, uh, talked about the, um, upcoming event.\",\n",
    "\"Simplified Sentence Structure: The movie was, uh, good. It had, you know, action and, um, interesting characters.\" \n",
    " ]\n",
    "\n",
    "\n",
    "# 4. Pragmatic Language Deficits\n",
    "# Example 1: Lack of Narrative Coherence\n",
    "[\"Lack of Narrative Coherence: So, um, there was this, you know, place, and, uh, people were doing things, but I can't quite, you know, remember how it all fits together.\",\n",
    " \"Simplified Sentence Structure: The movie was, uh, good. It had, you know, action and, um, interesting characters.\",\n",
    " \"Difficulty Organizing Description: I saw, um, a thing, and it was, you know, interesting because of, uh, some reasons that I can't quite, you know, put in order.\"\n",
    " ]\n",
    "\n",
    "\n",
    "# 5. Comprehension Deficits\n",
    "# Example 1: Misinterpretation of Details\n",
    "[\"Misinterpretation of Details: The, you know, person in the picture was, um, doing something strange, like, um, dancing, but I'm not sure exactly what.\",\n",
    " \"Inability to Answer Questions: I, um, don't really know what's happening in the picture. You see, there are, you know, things, but I can't say much about them.\",\n",
    "\"Difficulty Providing Relevant Answers: When you asked about, um, the scene, I'm not sure, you know, what to say. It's a bit confusing for me.\" \n",
    " ]\n",
    "\n",
    "# 6. Memory Impairments\n",
    "# Example 1: Limited Recall of Details\n",
    "[\"Limited Recall of Details: I remember, you know, seeing something, but I can't recall the, uh, specific details, like colors or, um, what people were doing.\",\n",
    " \"Repetition Errors: Oh, this picture looks familiar, but, um, I can't quite remember what I said the last time. Sorry, my, you know, memory isn't working well.\",\n",
    " \"Short-Term Memory Challenges: I think I, you know, talked about this picture before, but I can't remember what I said. It's a bit, uh, frustrating.\"\n",
    " ]\n",
    "\n",
    "\n",
    "# 7. Prosodic and Articulatory Features\n",
    "# Example 1: Altered Prosody\n",
    "[\"Altered Prosody: So, I was, um, at the park, and there was this, you know, bird singing, and it was like, uh, tweeting, but not in a regular way.\",\n",
    " \"Dysarthria: I, um, wanted to tell you about my, you know, day, but my words are, uh, not coming out very clearly. It's like, um, they're getting stuck.\",\n",
    " \"Changes in Speech Clarity: The, uh, way I'm talking might sound a bit, you know, different. It's like, um, my mouth isn't forming the words quite right.\"\n",
    " ]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
