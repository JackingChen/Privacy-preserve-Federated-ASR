{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current model:  facebook/data2vec-audio-large-960h\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n",
      "Ignored unknown kwarg option normalize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 4 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:46<08:55, 23.27s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 4 on device 4.\nOriginal Traceback (most recent call last):\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_3029304/2239454136.py\", line 100, in forward\n    outputs = self.data2vec_audio(\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py\", line 910, in forward\n    extract_features = self.feature_extractor(input_values)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py\", line 301, in forward\n    hidden_states = conv_layer(hidden_states)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py\", line 208, in forward\n    hidden_states = self.layer_norm(hidden_states)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/normalization.py\", line 196, in forward\n    return F.layer_norm(\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/functional.py\", line 2543, in layer_norm\n    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 424.00 MiB. GPU 4 has a total capacty of 3.81 GiB of which 413.12 MiB is free. Including non-PyTorch memory, this process has 3.39 GiB memory in use. Of the allocated memory 2.00 GiB is allocated by PyTorch, and 431.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 356\u001b[0m\n\u001b[1;32m    352\u001b[0m test_data \u001b[38;5;241m=\u001b[39m csv2dataset(audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/clips/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mroot_dir),\n\u001b[1;32m    353\u001b[0m                         csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/mid_csv/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mroot_dir))\n\u001b[1;32m    354\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mmap(prepare_dataset, num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m--> 356\u001b[0m df_test\u001b[38;5;241m=\u001b[39m\u001b[43mExtract_Emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mGPU_batchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPU_batchsize\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 189\u001b[0m, in \u001b[0;36mExtract_Emb\u001b[0;34m(dataset, GPU_batchsize)\u001b[0m\n\u001b[1;32m    187\u001b[0m         idxs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(i,\u001b[38;5;28mmin\u001b[39m(i\u001b[38;5;241m+\u001b[39mbs,\u001b[38;5;28mlen\u001b[39m(dataset))))\n\u001b[1;32m    188\u001b[0m         subset_dataset \u001b[38;5;241m=\u001b[39m Subset(dataset, idxs)\n\u001b[0;32m--> 189\u001b[0m         df_data\u001b[38;5;241m=\u001b[39m\u001b[43mget_Embs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, df_data], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# get emb.s, masks... 1 sample by 1 sample\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 210\u001b[0m, in \u001b[0;36mget_Embs\u001b[0;34m(subset_dataset)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# # 打印填充後的序列張量的形狀\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# print(padded_input_sequences.shape)\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# input_values=padded_input_sequences.cuda()\u001b[39;00m\n\u001b[1;32m    209\u001b[0m input_values\u001b[38;5;241m=\u001b[39mpadded_input_sequences\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 210\u001b[0m logits\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits  \n\u001b[1;32m    211\u001b[0m asr_lg \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mASR logits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# 轉換length的度量從sample到output的timestep\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:110\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    108\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 110\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 4 on device 4.\nOriginal Traceback (most recent call last):\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_3029304/2239454136.py\", line 100, in forward\n    outputs = self.data2vec_audio(\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py\", line 910, in forward\n    extract_features = self.feature_extractor(input_values)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py\", line 301, in forward\n    hidden_states = conv_layer(hidden_states)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/transformers/models/data2vec/modeling_data2vec_audio.py\", line 208, in forward\n    hidden_states = self.layer_norm(hidden_states)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/normalization.py\", line 196, in forward\n    return F.layer_norm(\n  File \"/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/functional.py\", line 2543, in layer_norm\n    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 424.00 MiB. GPU 4 has a total capacty of 3.81 GiB of which 413.12 MiB is free. Including non-PyTorch memory, this process has 3.39 GiB memory in use. Of the allocated memory 2.00 GiB is allocated by PyTorch, and 431.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# only data2vec model is used!!!!!!!!!!\n",
    "# from transformers.models.wav2vec2.configuration_wav2vec2 import Wav2Vec2Config\n",
    "# from dataclasses import dataclass\n",
    "# from typing import Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, Wav2Vec2Model,\n",
    "    Data2VecAudioModel, Data2VecAudioPreTrainedModel,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutput\n",
    "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from Models import (\n",
    "    Data2VecAudioPreTrainedModel,\n",
    "    DataCollatorCTCWithPadding,\n",
    "    Data2VecAudioForCTC,\n",
    "    # ReverseLayerF,\n",
    "    # RecallLoss,\n",
    "    # FSMatt_loss,\n",
    "    # gumbel_softmax\n",
    ")\n",
    "\n",
    "from datasets import (\n",
    "    load_metric,\n",
    ")\n",
    "from transformers.models.data2vec.configuration_data2vec_audio import Data2VecAudioConfig\n",
    "from utils import csv2dataset\n",
    "import pandas as pd\n",
    "# from argparse import ArgumentParser\n",
    "import argparse\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel import DataParallel\n",
    "#2023/04/23 For using GPU\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "from addict import Dict\n",
    "import pickle\n",
    "class Data2VecAudioForCTC(Data2VecAudioPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.data2vec_audio = Data2VecAudioModel(config)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "\n",
    "        if config.vocab_size is None:\n",
    "            raise ValueError(\n",
    "                f\"You are trying to instantiate {self.__class__} with a configuration that \"\n",
    "                \"does not define the vocabulary size of the language model head. Please \"\n",
    "                \"instantiate the model as follows: `Data2VecAudioForCTC.from_pretrained(..., vocab_size=vocab_size)`. \"\n",
    "                \"or define `vocab_size` of your model's configuration.\"\n",
    "            )\n",
    "\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)          # output字母的\"機率\"\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def freeze_feature_encoder(self):\n",
    "        \"\"\"\n",
    "        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n",
    "        not be updated during training.\n",
    "        \"\"\"\n",
    "        self.data2vec_audio.feature_extractor._freeze_parameters()\n",
    "\n",
    "    # @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n",
    "    # @add_code_sample_docstrings(\n",
    "    #     processor_class=_PROCESSOR_FOR_DOC,\n",
    "    #     checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "    #     output_type=CausalLMOutput,\n",
    "    #     config_class=_CONFIG_FOR_DOC,\n",
    "    #     expected_output=_CTC_EXPECTED_OUTPUT,\n",
    "    #     expected_loss=_CTC_EXPECTED_LOSS,\n",
    "    # )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_values,\n",
    "        attention_mask=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        labels=None,\n",
    "        dementia_labels=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n",
    "            Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\n",
    "            the sequence length of the output logits. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`.\n",
    "            All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ...,\n",
    "            config.vocab_size - 1]`.\n",
    "        \"\"\"\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.data2vec_audio(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        \n",
    "        final_loss = None\n",
    "        if labels is not None:\n",
    "\n",
    "            if labels.max() >= self.config.vocab_size:\n",
    "                raise ValueError(f\"Label values must be <= vocab_size: {self.config.vocab_size}\")\n",
    "\n",
    "            # retrieve loss input_lengths from attention_mask\n",
    "            attention_mask = (\n",
    "                attention_mask if attention_mask is not None else torch.ones_like(input_values, dtype=torch.long)\n",
    "            )\n",
    "            input_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n",
    "\n",
    "            # assuming that padded tokens are filled with -100\n",
    "            # when not being attended to\n",
    "            labels_mask = labels >= 0\n",
    "            target_lengths = labels_mask.sum(-1)\n",
    "            flattened_targets = labels.masked_select(labels_mask)\n",
    "\n",
    "            # ctc_loss doesn't support fp16\n",
    "            log_probs = nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)\n",
    "            \n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                loss = nn.functional.ctc_loss(\n",
    "                    log_probs,\n",
    "                    flattened_targets,\n",
    "                    input_lengths,\n",
    "                    target_lengths,\n",
    "                    blank=self.config.pad_token_id,\n",
    "                    reduction=self.config.ctc_loss_reduction,\n",
    "                    zero_infinity=self.config.ctc_zero_infinity,\n",
    "                )\n",
    "\n",
    "                final_loss = loss\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]\n",
    "        # return info that we might need\n",
    "        logits_all = {'ASR logits': logits,'hidden_states': hidden_states}\n",
    "\n",
    "        return CausalLMOutput(\n",
    "            loss=final_loss, logits=logits_all, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n",
    "        )\n",
    "\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"array\"]\n",
    "\n",
    "    # batched output is \"un-batched\" to ensure mapping is correct\n",
    "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n",
    "    \n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
    "        \n",
    "    return batch\n",
    "\n",
    "wer_metric = load_metric(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "def Extract_Emb(dataset, GPU_batchsize=16):\n",
    "    if GPU_batchsize!=None:\n",
    "        bs=int(GPU_batchsize)\n",
    "        df=pd.DataFrame()\n",
    "        for i in tqdm(range(0,len(dataset),bs)):\n",
    "            idxs=list(range(i,min(i+bs,len(dataset))))\n",
    "            subset_dataset = Subset(dataset, idxs)\n",
    "            df_data=get_Embs(subset_dataset)\n",
    "            df = pd.concat([df, df_data], ignore_index=True)\n",
    "    else:\n",
    "        # get emb.s, masks... 1 sample by 1 sample\n",
    "        df = map_to_result(dataset[0], 0)\n",
    "        for i in tqdm(range(len(dataset) - 1)):\n",
    "            df2 = map_to_result(dataset[i+1], i+1)\n",
    "            df = pd.concat([df, df2], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def get_Embs(subset_dataset):\n",
    "    with torch.no_grad():\n",
    "        # 將每個元素的 \"input_values\" 提取出來並組成一個列表\n",
    "        input_sequences = [torch.tensor(sample['input_values']) for sample in subset_dataset]\n",
    "        lengths = [len(sample['input_values']) for sample in subset_dataset]\n",
    "        # 將列表中的序列進行填充\n",
    "        padded_input_sequences = pad_sequence(input_sequences, batch_first=True)\n",
    "        # # 打印填充後的序列張量的形狀\n",
    "        # print(padded_input_sequences.shape)\n",
    "        # input_values=padded_input_sequences.cuda()\n",
    "        input_values=padded_input_sequences.to(device)\n",
    "        logits=model(input_values).logits  \n",
    "        asr_lg = logits['ASR logits']\n",
    "        # 轉換length的度量從sample到output的timestep\n",
    "        ratio=max(lengths)/asr_lg.shape[1]  # (batchsize, seqlength, logitsize)\n",
    "        oupLens=[int(l/ratio) for l in lengths]\n",
    "        # for l in lengths:\n",
    "        #     oupLens.append(int(l/ratio))\n",
    "        pred_ids = torch.argmax(asr_lg, dim=-1)\n",
    "        # batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "        pred_str=processor.batch_decode(pred_ids)\n",
    "        # batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "        # texts=[processor.decode(batch[\"labels\"], group_tokens=False) for batch in subset_dataset]\n",
    "        # text=processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "    df = pd.DataFrame()\n",
    "    dummy=None\n",
    "    for i in range(len(subset_dataset)):\n",
    "        RealLength=oupLens[i]  #只要有從logits取出來的都要還原\n",
    "        df2 = pd.DataFrame({'path': subset_dataset[i][\"path\"],                                    # to know which sample\n",
    "                # 'array': str(subset_dataset[i][\"array\"]),\n",
    "                'text': subset_dataset[i][\"text\"],\n",
    "                'dementia_labels': subset_dataset[i][\"dementia_labels\"],\n",
    "                # 'input_values': str(subset_dataset[i][\"input_values\"]),               # input of the model\n",
    "                # 'labels': str(subset_dataset[i][\"labels\"]),\n",
    "                # 'ASR logits': str(logits[\"ASR logits\"][i].tolist()),\n",
    "                # 'hidden_states': str(logits[\"hidden_states\"][i].tolist()), #原本的hidden state架構\n",
    "                'hidden_states': [logits[\"hidden_states\"][i][:RealLength,:].cpu().numpy()],  #(time-step,node_dimension)\n",
    "                'pred_str': pred_str[i]},\n",
    "                index=[i])\n",
    "        df = pd.concat([df, df2], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def map_to_result(batch, idx):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"]).unsqueeze(0)            \n",
    "        logits = model(input_values).logits                                     # includes ASR logits, dementia logits, hidden_states\n",
    "        asr_lg = logits['ASR logits']\n",
    "        #AD_lg = logits['dementia logits'][0]                                    # (1, time-step, 2) --> (time-step, 2)\n",
    "    \n",
    "    \"\"\"\n",
    "    pred_ad_tstep = torch.argmax(AD_lg, dim=-1)                                 # pred of each time-step\n",
    "    pred_ad = pred_ad_tstep.sum() / pred_ad_tstep.size()[0]                     # average result\n",
    "    if pred_ad > 0.5:                                                           # over half of the time pred AD\n",
    "        batch[\"pred_AD\"] = 1\n",
    "    else:\n",
    "        batch[\"pred_AD\"] = 0\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(asr_lg, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "    \n",
    "    # for toggle\n",
    "    # for fine-tune model\n",
    "    df = pd.DataFrame({'path': batch[\"path\"],                                    # to know which sample\n",
    "                # 'array': str(subset_dataset[i][\"array\"]),\n",
    "                'text': batch[\"text\"],\n",
    "                'dementia_labels': batch[\"dementia_labels\"],\n",
    "                # 'input_values': str(subset_dataset[i][\"input_values\"]),               # input of the model\n",
    "                # 'labels': str(subset_dataset[i][\"labels\"]),\n",
    "                # 'ASR logits': str(logits[\"ASR logits\"][i].tolist()),\n",
    "                'hidden_states': logits[\"hidden_states\"].tolist(),\n",
    "                'pred_str': batch[\"pred_str\"]},\n",
    "                index=[idx])\n",
    "    return df\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-lam', '--LAMBDA', type=float, default=0.5, help=\"Lambda for GRL\")\n",
    "parser.add_argument('-st', '--STAGE', type=int, default=1, help=\"Current stage\")\n",
    "parser.add_argument('-model', '--model_path', type=str, default=\"/mnt/Internal/FedASR/weitung/HuggingFace/Pretrain/saves/data2vec-audio-large-960h_finetuned/final/\", help=\"Where the model is saved\")\n",
    "parser.add_argument('-thres', '--threshold', type=float, default=0.5, help=\"Threshold for AD & ASR\")\n",
    "parser.add_argument('-model_type', '--model_type', type=str, default=\"data2vec\", help=\"Type of the model\")\n",
    "parser.add_argument('-RD', '--root_dir', default='/mnt/Internal/FedASR/Data/ADReSS-IS2020-data', help=\"Learning rate\")\n",
    "parser.add_argument('--AudioLoadFunc', default='librosa', help=\"用scipy function好像可以比較快\")\n",
    "parser.add_argument('--savepath', default='./saves/results/', help=\"用scipy function好像可以比較快\")\n",
    "parser.add_argument('--GPU_batchsize', type=str, default=32, help=\"如果cpu滿了就用GPU\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "LAMBDA = args.LAMBDA                    # lambda for GRL\n",
    "STAGE = args.STAGE                      # stage 1: train AD classifier; stage 2: train toggling network\n",
    "model_dir = args.model_path             # path to load the model\n",
    "model_type = args.model_type            # select different type of model (here only data2vec is ready to use)\n",
    "savePath = args.savepath\n",
    "\n",
    "# threshold for maskes\n",
    "AD_THRES = args.threshold\n",
    "LM_THRES = args.threshold\n",
    "\n",
    "# load according to model type\n",
    "# note that only data2vec is done for this version\n",
    "if model_type == \"wav2vec\":\n",
    "    name = \"facebook/wav2vec2-base-960h\" # + model_dir.split(\"/\")[-3]\n",
    "    print(\"Current model: \", name)\n",
    "    csv_name=name.split('/')[-1]\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(name)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "elif model_type == \"data2vec\":\n",
    "    name = \"facebook/data2vec-audio-large-960h\" # + model_in_dir.split(\"/\")[-3]\n",
    "    print(\"Current model: \", name)\n",
    "    csv_name=name.split('/')[-1]\n",
    "    mask_time_prob = 0                                                                     # change config to avoid code from stopping\n",
    "    config = Data2VecAudioConfig.from_pretrained(name, mask_time_prob=mask_time_prob)\n",
    "    model = Data2VecAudioForCTC.from_pretrained(model_dir, config=config)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "\n",
    "elif model_type == \"hubert\":\n",
    "    name = \"facebook/hubert-xlarge-ls960-ft\" # + model_in_dir.split(\"/\")[-3]\n",
    "    print(\"Current model: \", name)\n",
    "    mask_time_prob = 0                                                                     # change config\n",
    "    config = HubertConfig.from_pretrained(name, mask_time_prob=mask_time_prob)\n",
    "    model = HubertForCTC.from_pretrained(model_dir, config=config)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "elif model_type == \"sewd\":\n",
    "    name = \"asapp/sew-d-mid-400k-ft-ls100h\" #+ model_in_dir.split(\"/\")[-3]\n",
    "    print(\"Current model: \", name)\n",
    "    mask_time_prob = 0                                                                     # change config\n",
    "    config = SEWDConfig.from_pretrained(name, mask_time_prob=mask_time_prob)\n",
    "    model = SEWDForCTC.from_pretrained(model_dir, config=config)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(name)\n",
    "elif model_type == \"unispeech\":\n",
    "    name = \"microsoft/unispeech-sat-base-100h-libri-ft\" # + model_in_dir.split(\"/\")[-3]\n",
    "    print(\"Current model: \", name)\n",
    "    mask_time_prob = 0                                                                     # change config\n",
    "    config = UniSpeechSatConfig.from_pretrained(name, mask_time_prob=mask_time_prob)\n",
    "    model = UniSpeechSatForCTC.from_pretrained(model_dir, config=config)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(name)   \n",
    "\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "if args.GPU_batchsize != None:\n",
    "    # ======================\n",
    "    # model = model.cuda()\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "    # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = DataParallel(model)\n",
    "\n",
    "    # 將模型移動到GPU上\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # ======================\n",
    "\n",
    "# store result of test data\n",
    "test_data = csv2dataset(audio_path = '{}/clips/'.format(args.root_dir),\n",
    "                        csv_path = \"{}/mid_csv/test.csv\".format(args.root_dir))\n",
    "test_data = test_data.map(prepare_dataset, num_proc=10)\n",
    "\n",
    "df_test=Extract_Emb(test_data,GPU_batchsize=args.GPU_batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "\n",
    "with open(f\"{savePath}/{csv_name}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_test, f)\n",
    "print(\"Testing data Done\")\n",
    "\n",
    "# store result of train data\n",
    "train_data = csv2dataset(audio_path = '{}/clips/'.format(args.root_dir),\n",
    "                         csv_path = \"{}/mid_csv/train.csv\".format(args.root_dir)) #!!! librosa在load的時候非常慢，大約7分47秒讀完1869個file\n",
    "train_data = train_data.map(prepare_dataset, num_proc=10)\n",
    "\n",
    "# get emb.s, masks... 1 sample by 1 sample\n",
    "# df = map_to_result(train_data[0], 0)\n",
    "# for i in range(len(train_data) - 1):\n",
    "#     df2 = map_to_result(train_data[i+1], i+1)\n",
    "#     df = pd.concat([df, df2], ignore_index=True)\n",
    "#     print(\"\\r\"+ str(i), end=\"\")\n",
    "df_train=Extract_Emb(train_data,GPU_batchsize=args.GPU_batchsize)\n",
    "with open(f\"{savePath}/{csv_name}_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_train, f)\n",
    "print(\"Training data Done\")\n",
    "\n",
    "# store result of dev data\n",
    "\"\"\"\n",
    "dev_data = csv2dataset(path = \"/mnt/Internal/FedASR/Data/ADReSS-IS2020-data/mid_csv/dev.csv\")\n",
    "dev_data = dev_data.map(prepare_dataset, num_proc=10)\n",
    "\n",
    "df = map_to_result(dev_data[0], 0)\n",
    "for i in range(len(dev_data) - 1):\n",
    "    df2 = map_to_result(dev_data[i+1], i+1)\n",
    "    df = pd.concat([df, df2], ignore_index=True)\n",
    "    print(\"\\r\"+ str(i), end=\"\")\n",
    "\n",
    "csv_path = \"./saves/results/\" + csv_name + \"_dev.csv\"\n",
    "df.to_csv(csv_path)\n",
    "\"\"\"\n",
    "print(csv_name + \" All Done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
