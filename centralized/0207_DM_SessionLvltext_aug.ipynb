{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Seed set to 2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu=1, epochs=5, lr=2e-05, random_seed=2023, t_embed='mbert', a_embed='en', SaveRoot='/mnt/External/Seagate/FedASR/LLaMa2/dacs', file_in='/home/FedASR/dacs/centralized/saves/results/data2vec-audio-large-960h_total.csv', summary_dir_in='/mnt/External/Seagate/FedASR/LLaMa2/dacs/EmbFeats/Lexical/Embeddings/text_data2vec-audio-large-960h_Phych-anomia', Augment_dir_in='/mnt/External/Seagate/FedASR/LLaMa2/dacs/EmbFeats/Lexical/Augment_data/text_data2vec-audio-large-960h_Phych-anomia')\n",
      "Using PyTorch Ver 2.1.1+cu121\n",
      "Fix Seed: 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session S009 has no data\n",
      "Session S011 has no data\n",
      "Session S017 has no data\n",
      "Session S027 has no data\n",
      "Session S029 has no data\n",
      "Session S052 has no data\n",
      "Session S080 has no data\n",
      "Session S084 has no data\n",
      "Session S086 has no data\n",
      "Session S118 has no data\n",
      "Session S124 has no data\n",
      "Session S138 has no data\n",
      "Session S145 has no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 5 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=5)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Start Training ::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /mnt/External/Seagate/FedASR/LLaMa2/dacs/Model/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]\n",
      "/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:375: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | t_model | BertModel     | 167 M \n",
      "1 | a_model | Wav2Vec2Model | 315 M \n",
      "2 | clf1    | Linear        | 295 K \n",
      "3 | clf2    | Linear        | 770   \n",
      "------------------------------------------\n",
      "483 M     Trainable params\n",
      "0         Non-trainable params\n",
      "483 M     Total params\n",
      "1,932.365 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bb4c6ba65f4806b9a56b2f87315ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d36bdfde104958a472297a588d4502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved. New best score: 0.534\n",
      "Epoch 0, global step 14: 'val_acc' reached 0.53425 (best 0.53425), saving model to '/mnt/External/Seagate/FedASR/LLaMa2/dacs/Model/checkpoints/epoch=0-step=14-v2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------val_report-------\n",
      "              precision    recall  f1-score    support\n",
      "Control        0.521739  0.972973  0.679245  37.000000\n",
      "ProbableAD     0.750000  0.083333  0.150000  36.000000\n",
      "accuracy       0.534247  0.534247  0.534247   0.534247\n",
      "macro avg      0.635870  0.528153  0.414623  73.000000\n",
      "weighted avg   0.634306  0.534247  0.418248  73.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72468d2dd1dd41cc9c03513a27183485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 28: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------val_report-------\n",
      "              precision    recall  f1-score    support\n",
      "Control        0.506849  1.000000  0.672727  37.000000\n",
      "ProbableAD     1.000000  0.000000  0.000000  36.000000\n",
      "accuracy       0.506849  0.506849  0.506849   0.506849\n",
      "macro avg      0.753425  0.500000  0.336364  73.000000\n",
      "weighted avg   0.750047  0.506849  0.340971  73.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39de7013bb5e4960ad324c26e96b8857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 42: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------val_report-------\n",
      "              precision    recall  f1-score    support\n",
      "Control        1.000000  0.000000  0.000000  37.000000\n",
      "ProbableAD     0.493151  1.000000  0.660550  36.000000\n",
      "accuracy       0.493151  0.493151  0.493151   0.493151\n",
      "macro avg      0.746575  0.500000  0.330275  73.000000\n",
      "weighted avg   0.750047  0.493151  0.325751  73.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4526f57af3b4ec5815eb3ac9d3b9be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------val_report-------\n",
      "              precision    recall  f1-score    support\n",
      "Control        1.000000  0.000000  0.000000  37.000000\n",
      "ProbableAD     0.493151  1.000000  0.660550  36.000000\n",
      "accuracy       0.493151  0.493151  0.493151   0.493151\n",
      "macro avg      0.746575  0.500000  0.330275  73.000000\n",
      "weighted avg   0.750047  0.493151  0.325751  73.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 56: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec33f766d04419fa2eddedf572d36be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 70: 'val_acc' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------val_report-------\n",
      "              precision    recall  f1-score    support\n",
      "Control        1.000000  0.000000  0.000000  37.000000\n",
      "ProbableAD     0.493151  1.000000  0.660550  36.000000\n",
      "accuracy       0.493151  0.493151  0.493151   0.493151\n",
      "macro avg      0.746575  0.500000  0.330275  73.000000\n",
      "weighted avg   0.750047  0.493151  0.325751  73.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /mnt/External/Seagate/FedASR/LLaMa2/dacs/Model/checkpoints/epoch=0-step=14-v2.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]\n",
      "Loaded model weights from the checkpoint at /mnt/External/Seagate/FedASR/LLaMa2/dacs/Model/checkpoints/epoch=0-step=14-v2.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef347311ecd34d01aad1649e3067594b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')\n",
      "id tensor([ 8,  9, 10, 11, 12, 13, 14, 15], device='cuda:0')\n",
      "id tensor([16, 17, 18, 19, 20, 21, 22, 23], device='cuda:0')\n",
      "id tensor([24, 25, 26, 27, 28, 29, 30, 31], device='cuda:0')\n",
      "id tensor([32, 33, 34, 35, 36, 37, 38, 39], device='cuda:0')\n",
      "id tensor([40, 41, 42, 43, 44, 45, 46, 47], device='cuda:0')\n",
      "-------test_report-------\n",
      "              precision    recall  f1-score  support\n",
      "Control             0.5  0.958333  0.657143     24.0\n",
      "ProbableAD          0.5  0.041667  0.076923     24.0\n",
      "accuracy            0.5  0.500000  0.500000      0.5\n",
      "macro avg           0.5  0.500000  0.367033     48.0\n",
      "weighted avg        0.5  0.500000  0.367033     48.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\npython 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed en\\npython 0207_DM_multi.py --gpu 1 --t_embed xlm --a_embed en\\n\\n# don\\npython 0207_DM_multi.py --gpu 0 --t_embed xlm --a_embed gr\\npython 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed gr\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# torch:\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "from transformers import BertTokenizer, BertConfig, BertModel,XLMTokenizer, XLMModel\n",
    "\n",
    "from Dementia_challenge_models import Embsize_map\n",
    "\n",
    "class Arg:\n",
    "    version = 1\n",
    "    # data\n",
    "    epochs: int = 5  # Max Epochs, BERT paper setting [3,4,5]\n",
    "    max_length: int = 350  # Max Length input size\n",
    "    report_cycle: int = 30  # Report (Train Metrics) Cycle\n",
    "    cpu_workers: int = os.cpu_count()  # Multi cpu workers\n",
    "    test_mode: bool = False  # Test Mode enables `fast_dev_run`\n",
    "    optimizer: str = 'AdamW'  # AdamW vs AdamP\n",
    "    lr_scheduler: str = 'exp'  # ExponentialLR vs CosineAnnealingWarmRestarts\n",
    "    fp16: bool = False  # Enable train on FP16\n",
    "    a_hidden_size = 0 # BERT-base: 768, BERT-large: 1024, BERT paper setting\n",
    "    t_hidden_size = 768\n",
    "    t_x_hidden_size = a_hidden_size+t_hidden_size\n",
    "    batch_size: int = 8\n",
    "            \n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "    \n",
    "    \n",
    "class Model(LightningModule):\n",
    "    def __init__(self, args,config):\n",
    "        super().__init__()\n",
    "        # config:\n",
    "        \n",
    "        self.args = args\n",
    "        self.config = config\n",
    "        self.batch_size = self.args.batch_size\n",
    "        \n",
    "        # meta data:\n",
    "        self.epochs_index = 0\n",
    "        self.label_cols = 'dementia_labels'\n",
    "        self.label_names = ['Control','ProbableAD']\n",
    "        self.num_labels = 2\n",
    "        self.t_embed_type = self.config['t_embed']\n",
    "        self.a_embed_type = self.config['a_embed']\n",
    "        self.a_hidden = self.args.a_hidden_size\n",
    "        \n",
    "        # --> HERE STEP 1 <--\n",
    "        # ATTRIBUTES TO SAVE BATCH OUTPUTS\n",
    "        self.test_step_outputs = []   # save outputs in each batch to compute metric overall epoch\n",
    "        self.test_step_targets = []   # save targets in each batch to compute metric overall epoch\n",
    "        self.val_step_outputs = []        # save outputs in each batch to compute metric overall epoch\n",
    "        self.val_step_targets = []        # save targets in each batch to compute metric overall epoch\n",
    "\n",
    "\n",
    "        if self.t_embed_type == \"mbert\":\n",
    "            self.t_hidden = self.args.t_hidden_size\n",
    "            \n",
    "            t_pretrained = 'bert-base-multilingual-uncased'\n",
    "            self.t_tokenizer = BertTokenizer.from_pretrained(t_pretrained)\n",
    "            self.t_model = BertModel.from_pretrained(t_pretrained)\n",
    "            \n",
    "            \n",
    "        elif self.t_embed_type == \"xlm\":\n",
    "            self.t_hidden = self.args.a_hidden_size+self.args.t_hidden_size\n",
    "            \n",
    "            t_pretrained = 'xlm-mlm-100-1280'\n",
    "            self.t_tokenizer = XLMTokenizer.from_pretrained(t_pretrained)\n",
    "            self.t_model = XLMModel.from_pretrained(t_pretrained)\n",
    "            self.pooler = BertPooler(self.t_hidden)\n",
    "            \n",
    "        self.hidden = int(self.a_hidden + self.t_hidden)\n",
    "        \n",
    "        if self.a_embed_type == \"en\":\n",
    "            a_pretrained =  \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "            \n",
    "        elif self.a_embed_type == \"gr\":\n",
    "            a_pretrained =  \"lighteternal/wav2vec2-large-xlsr-53-greek\"\n",
    "\n",
    "        elif self.a_embed_type == \"multi\":\n",
    "            a_pretrained = \"voidful/wav2vec2-xlsr-multilingual-56\"\n",
    "            \n",
    "        elif self.a_embed_type == \"wv\":\n",
    "            a_pretrained ='facebook/wav2vec2-base'\n",
    "            \n",
    "        self.a_tokenizer = Wav2Vec2FeatureExtractor.from_pretrained(a_pretrained)\n",
    "        self.a_model = Wav2Vec2Model.from_pretrained(a_pretrained)\n",
    "        \n",
    "        \n",
    "        self.clf1 = nn.Linear(self.hidden, int(self.hidden/2))\n",
    "        self.clf2 = nn.Linear(int(self.hidden/2), self.num_labels)\n",
    "        \n",
    "            \n",
    "            \n",
    "    # def forward(self, text, audio):\n",
    "    def forward(self, text):\n",
    "        \n",
    "        if self.t_embed_type == \"mbert\":\n",
    "            t_out = self.t_model(text)[1] \n",
    "\n",
    "            \n",
    "        elif self.t_embed_type == \"xlm\":\n",
    "            t_out = self.t_model(text)[0]\n",
    "            t_out = self.pooler(t_out)\n",
    "            \n",
    "            \n",
    "        # a_out = self.a_model(audio)['extract_features']#[2] #last_hidden_state , feature extraction\n",
    "        # a_out = a_out[:, 0, :] \n",
    "        \n",
    "        #print(a_out)\n",
    "        #print(a_out['extract_features'].shape) # ([8, 437, 512])\n",
    "        #print(a_out['last_hidden_state'].shape) # ([8, 437, 1024]) => pooling 필요\n",
    "        \n",
    "        \n",
    "        # output = torch.cat((t_out,a_out),axis=1)   \n",
    "        output = t_out\n",
    "        #print(output.shape)\n",
    "        \n",
    "        logits = self.clf2(self.clf1(output))\n",
    "    \n",
    "        return logits\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config['lr'])\n",
    "        scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }\n",
    "\n",
    "    def preprocess_dataframe(self):\n",
    "        \n",
    "        tg_sr = 16000\n",
    "        t_col_name = \"text\" \n",
    "        # a_col_name = \"path\"         \n",
    "        # df = pd.read_json('/mnt/Internal/FedASR/Data/230126_total_asr_data.json')\n",
    "        df = pd.read_csv(config.file_in)\n",
    "\n",
    "        def Augment_info_df(df_test):\n",
    "            # 將 'path' 欄位進行字串操作\n",
    "            df_test['path'] = df_test['path'].str.rstrip('.wav')\n",
    "\n",
    "            # 使用 str.split 拆分 'path' 欄位\n",
    "            df_test[['session', 'role', 'number', 'start_time', 'end_time']] = df_test['path'].str.split('_', expand=True)\n",
    "\n",
    "            # 如果 'number' 欄位的末尾包含 '.wav'，進行一次額外的拆分\n",
    "            df_test['number'] = df_test['number'].str.rstrip('.wav')\n",
    "\n",
    "            # 將 'start_time' 和 'end_time' 欄位轉換為數值型別\n",
    "            df_test[['start_time', 'end_time']] = df_test[['start_time', 'end_time']].astype(int)\n",
    "\n",
    "            return df_test\n",
    "        df = Augment_info_df(df)\n",
    "\n",
    "\n",
    "        # Packer\n",
    "        def Packer(df_test) -> dict:\n",
    "            People_dict = dict(tuple(df_test.groupby('session')))\n",
    "            return People_dict\n",
    "\n",
    "        def Dialogueturn2corpus(data_frame, mode='text'): #mode can be 'pred_str' or 'text'\n",
    "            # 按 'start_time' 列進行排序\n",
    "            sorted_data_frame = data_frame.sort_values(by='start_time')\n",
    "\n",
    "            # 添加前綴並使用 '\\n' 進行拼接\n",
    "            processed_text = sorted_data_frame.apply(lambda row: f\"{row['role']}: {row[mode]}\", axis=1).str.cat(sep='\\n')\n",
    "\n",
    "            return processed_text\n",
    "\n",
    "        def filter_people_dict(People_dict, mode=\"INV+PAR\", verbose=False) -> dict:# 'INV' , 'PAR', 'INV+PAR'\n",
    "            filtered_people_dict = {}\n",
    "\n",
    "            for session, data_frame in People_dict.items():\n",
    "                # 使用 query 過濾 'role' 為 'INV' 或 'PAR'\n",
    "                if mode == \"PAR\":\n",
    "                    filtered_data_frame = data_frame.query(\"role == 'PAR'\")\n",
    "                    filtered_people_dict[session] = filtered_data_frame\n",
    "                elif mode == \"INV\":\n",
    "                    filtered_data_frame = data_frame.query(\"role == 'INV'\")\n",
    "                    filtered_people_dict[session] = filtered_data_frame\n",
    "                elif mode == \"INV+PAR\":\n",
    "                    filtered_people_dict[session] = data_frame\n",
    "                else:\n",
    "                    raise OSError\n",
    "            \n",
    "            if verbose:\n",
    "                # 印出過濾後的 People_dict 中每個 session 的 DataFrame\n",
    "                for session, data_frame in filtered_people_dict.items():\n",
    "                    print(f\"Session: {session}\")\n",
    "                    print(data_frame)\n",
    "                    print(\"\\n\")\n",
    "\n",
    "            return filtered_people_dict\n",
    "\n",
    "        # Dialogue Formatter\n",
    "        def Dialogue_Formatter(People_dict, sep=\"\\n\",role_mode='PAR')->dict:\n",
    "            session_df=pd.DataFrame()\n",
    "            for session, data_frame in People_dict.items():\n",
    "                if len(data_frame)>0:\n",
    "                    total_info=data_frame.iloc[0].copy()\n",
    "                    sessional_text = Dialogueturn2corpus(data_frame,mode='text')\n",
    "                    sessional_predStr = Dialogueturn2corpus(data_frame,mode='pred_str')\n",
    "                    \n",
    "                    # total_info,'text']=sessional_text\n",
    "                    # session_df.loc[session,'pred_str']=sessional_predStr\n",
    "                    # session_df.loc[session,'role']=role_mode\n",
    "                    # session_df.loc[session,'start_time']=data_frame['start_time'].min()\n",
    "                    # session_df.loc[session,'end_time']=data_frame['end_time'].max()\n",
    "\n",
    "                    total_info['text']=sessional_text\n",
    "                    total_info['pred_str']=sessional_predStr\n",
    "                    total_info['role']=role_mode\n",
    "                    total_info['start_time']=data_frame['start_time'].min()\n",
    "                    total_info['end_time']=data_frame['end_time'].max()\n",
    "                    session_df = pd.concat([session_df, pd.DataFrame([total_info], index=[session])])\n",
    "                else:\n",
    "                    print(f\"Session {session} has no data\")\n",
    "            return session_df\n",
    "        df_train = df[df['ex'] == 'train']\n",
    "        df_val = df[df['ex'] == 'dev']\n",
    "        df_test = df[df['ex'] == 'test']\n",
    "\n",
    "        def SentenceLvldf2SessionLvldf(df, role_mode=\"PAR\"):\n",
    "            People_dict=Packer(df)\n",
    "            People_dict = filter_people_dict(People_dict, mode=role_mode, verbose=False)\n",
    "            df_dialogue=Dialogue_Formatter(People_dict,role_mode)\n",
    "            return df_dialogue\n",
    "\n",
    "        df_train=SentenceLvldf2SessionLvldf(df_train)\n",
    "        df_val=SentenceLvldf2SessionLvldf(df_val)\n",
    "        df_test=SentenceLvldf2SessionLvldf(df_test)\n",
    "\n",
    "        def Tokenize(df_data):\n",
    "            df_data[t_col_name] = df_data[t_col_name].map(lambda x: self.t_tokenizer.encode(\n",
    "                str(x),\n",
    "                padding = 'max_length',\n",
    "                max_length=self.args.max_length,\n",
    "                truncation=True,\n",
    "                ))\n",
    "            return df_data\n",
    "        df_train=Tokenize(df_train)\n",
    "        df_val=Tokenize(df_val)\n",
    "        df_test=Tokenize(df_test)\n",
    "        df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "        # audio_root=\"/mnt/Internal/FedASR/Data/ADReSS-IS2020-data/clips\"\n",
    "        # # 원래 길이: 562992, batch 16: 90000, batch 8: 140000\n",
    "        # # max_length=16000, truncation=True 이건 일단 돌려보고 결정 => 뒤쪽, 앞에쪽 뭐보면 좋을 지 그런거 check하면 좋으니까! \n",
    "        # df[a_col_name] = df[a_col_name].map(lambda x: self.a_tokenizer(\n",
    "        #     f\"{audio_root}/{x}\",\n",
    "        #     sampling_rate = tg_sr,\n",
    "        #     max_length=100000, \n",
    "        #     truncation=True\n",
    "        #     )['input_values'][0])\n",
    "\n",
    "        self.train_data = TensorDataset(\n",
    "            torch.tensor(df_train[t_col_name].tolist(), dtype=torch.long),\n",
    "            # torch.tensor(df_train[a_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(df_train[self.label_cols].tolist(), dtype=torch.long),\n",
    "        )\n",
    "        \n",
    "        self.val_data = TensorDataset(\n",
    "             torch.tensor(df_val[t_col_name].tolist(), dtype=torch.long),\n",
    "            #  torch.tensor(df_val[a_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(df_val[self.label_cols].tolist(), dtype=torch.long),\n",
    "        )\n",
    "\n",
    "        self.test_data = TensorDataset(\n",
    "             torch.tensor(df_test[t_col_name].tolist(), dtype=torch.long),\n",
    "            #  torch.tensor(df_test[a_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(df_test[self.label_cols].tolist(), dtype=torch.long),\n",
    "             torch.tensor(df_test.index.tolist(), dtype=torch.long),\n",
    "        )\n",
    "    def ADD_AugmentedTrainingData(self):\n",
    "        df_train = pd.read_pickle(f\"{config.Augment_dir_in}/train.pkl\")\n",
    "\n",
    "        self.df_train_aug=df_train\n",
    "\n",
    "        \n",
    "    def preprocess_loaded_summaries(self):\n",
    "        df_train = pd.read_pickle(f\"{config.summary_dir_in}/train.pkl\")\n",
    "        df_val = pd.read_pickle(f\"{config.summary_dir_in}/dev.pkl\")\n",
    "        df_test = pd.read_pickle(f\"{config.summary_dir_in}/test.pkl\")\n",
    "\n",
    "        Aug_col_name='Embedding'\n",
    "\n",
    "        def Tokenize(df_data):\n",
    "            df_data[Aug_col_name] = df_data[Aug_col_name].map(lambda x: self.t_tokenizer.encode(\n",
    "                str(x),\n",
    "                padding = 'max_length',\n",
    "                max_length=self.args.max_length,\n",
    "                truncation=True,\n",
    "                ))\n",
    "            return df_data\n",
    "        df_train=Tokenize(df_train)\n",
    "        df_val=Tokenize(df_val)\n",
    "        df_test=Tokenize(df_test)\n",
    "        df_test = df_test.reset_index(drop=True)\n",
    "        self.df_train=df_train\n",
    "        self.df_val=df_val\n",
    "        self.df_test=df_test\n",
    "    def df2Dataset(self):\n",
    "        self.train_data = TensorDataset(\n",
    "            torch.tensor(self.df_train[self.t_col_name].tolist(), dtype=torch.long),\n",
    "            torch.tensor(self.df_train[self.a_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(self.df_train[self.Aug_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(self.df_train[self.label_cols].tolist(), dtype=torch.long),\n",
    "        )\n",
    "        \n",
    "        self.val_data = TensorDataset(\n",
    "             torch.tensor(self.df_val[self.t_col_name].tolist(), dtype=torch.long),\n",
    "             torch.tensor(self.df_val[self.a_col_name].tolist(), dtype=torch.float),\n",
    "             torch.tensor(self.df_val[self.Aug_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(self.df_val[self.label_cols].tolist(), dtype=torch.long),\n",
    "        )\n",
    "\n",
    "        self.test_data = TensorDataset(\n",
    "             torch.tensor(self.df_test[self.t_col_name].tolist(), dtype=torch.long),\n",
    "             torch.tensor(self.df_test[self.a_col_name].tolist(), dtype=torch.float),\n",
    "             torch.tensor(self.df_test[self.Aug_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(self.df_test[self.label_cols].tolist(), dtype=torch.long),\n",
    "             torch.tensor(self.df_test.index.tolist(), dtype=torch.long),\n",
    "        )\n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "\n",
    "        return DataLoader(\n",
    "            self.test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # token, audio, labels = batch  \n",
    "        token,  labels = batch  \n",
    "        # logits = self(token, audio) \n",
    "        logits = self(token) \n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)   \n",
    "        \n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # token, audio, labels = batch  \n",
    "        token, labels = batch  \n",
    "        # logits = self(token, audio) \n",
    "        logits = self(token) \n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)     \n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "        # --> HERE STEP 2 <--\n",
    "        self.val_step_outputs.append({\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        })\n",
    "        # self.val_step_targets.append(y_true)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "        \n",
    "            \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # token, audio, labels,id_ = batch \n",
    "        token, labels,id_ = batch \n",
    "        print('id', id_)\n",
    "        # logits = self(token, audio) \n",
    "        logits = self(token) \n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "        # --> HERE STEP 2 <--\n",
    "        self.test_step_outputs.append({\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        })\n",
    "        # self.test_step_targets.append(y_true)\n",
    "        return {\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        # print(\"Value= \",self.val_step_outputs)\n",
    "        # print(\"type(self.val_step_outputs)=\",type(self.val_step_outputs))\n",
    "        # print(\"type(self.val_step_outputs[0])=\",type(self.val_step_outputs[0]))\n",
    "        # print(\"type(self.val_step_outputs[0] loss)=\",type(self.val_step_outputs[0]['loss']))\n",
    "        for i in self.val_step_outputs:\n",
    "            loss += i['loss'].cpu().detach()\n",
    "        _loss = loss / len(self.val_step_outputs)\n",
    "        loss = float(_loss)\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for i in self.val_step_outputs:\n",
    "            y_true += i['y_true']\n",
    "            y_pred += i['y_pred']\n",
    "            \n",
    "        y_pred = np.asanyarray(y_pred)#y_temp_pred y_pred\n",
    "        y_true = np.asanyarray(y_true)\n",
    "        \n",
    "        pred_dict = {}\n",
    "        pred_dict['y_pred']= y_pred\n",
    "        pred_dict['y_true']= y_true\n",
    "        \n",
    "        val_acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "        \n",
    "        self.log(\"val_acc\", val_acc)\n",
    "        # print(\"y_pred= \", y_pred)\n",
    "        # print('\\n\\n\\n')\n",
    "        # print(\"y_true= \", y_true)\n",
    "        # print('\\n\\n\\n')\n",
    "        print(\"-------val_report-------\")\n",
    "        metrics_dict = classification_report(y_true, y_pred,zero_division=1,\n",
    "                                             target_names = self.label_names, \n",
    "                                             output_dict=True)\n",
    "        df_result = pd.DataFrame(metrics_dict).transpose()\n",
    "        pprint(df_result)\n",
    "        \n",
    "\n",
    "        df_result.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_val.csv')\n",
    "\n",
    "        pred_df = pd.DataFrame(pred_dict)\n",
    "        pred_df.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_val_pred.csv')\n",
    "        self.val_step_outputs.clear()\n",
    "        # self.val_step_targets.clear()\n",
    "        return {'loss': _loss}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for i in self.test_step_outputs:\n",
    "            y_true += i['y_true']\n",
    "            y_pred += i['y_pred']\n",
    "            \n",
    "        y_pred = np.asanyarray(y_pred)#y_temp_pred y_pred\n",
    "        y_true = np.asanyarray(y_true)\n",
    "        \n",
    "        pred_dict = {}\n",
    "        pred_dict['y_pred']= y_pred\n",
    "        pred_dict['y_true']= y_true\n",
    "        \n",
    "        \n",
    "        print(\"-------test_report-------\")\n",
    "        metrics_dict = classification_report(y_true, y_pred,zero_division=1,\n",
    "                                             target_names = self.label_names, \n",
    "                                             output_dict=True)\n",
    "        df_result = pd.DataFrame(metrics_dict).transpose()\n",
    "        self.test_step_outputs.clear()\n",
    "        # self.test_step_targets.clear()\n",
    "        pprint(df_result)\n",
    "\n",
    "        df_result.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_test.csv')\n",
    "\n",
    "        pred_df = pd.DataFrame(pred_dict)\n",
    "        pred_df.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_test_pred.csv')\n",
    "\n",
    "    \n",
    "def main(args,config):\n",
    "    print(\"Using PyTorch Ver\", torch.__version__)\n",
    "    print(\"Fix Seed:\", config['random_seed'])\n",
    "    seed_everything( config['random_seed'])\n",
    "        \n",
    "    model = Model(args,config) \n",
    "    model.preprocess_dataframe()\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=f\"{SaveRoot}/Model/checkpoints\",\n",
    "        monitor='val_acc',\n",
    "        auto_insert_metric_name=True,\n",
    "        verbose=True,\n",
    "        mode='max', \n",
    "        save_top_k=1,\n",
    "      )    \n",
    "\n",
    "    print(\":: Start Training ::\")\n",
    "    #     \n",
    "    trainer = Trainer(\n",
    "        logger=False,\n",
    "        callbacks=[early_stop_callback,checkpoint_callback],\n",
    "        enable_checkpointing = True,\n",
    "        max_epochs=args.epochs,\n",
    "        fast_dev_run=args.test_mode,\n",
    "        num_sanity_val_steps=None if args.test_mode else 0,\n",
    "        deterministic=True, # ensure full reproducibility from run to run you need to set seeds for pseudo-random generators,\n",
    "        # For GPU Setup\n",
    "        # gpus=[config['gpu']] if torch.cuda.is_available() else None,\n",
    "        # strategy='ddp_find_unused_parameters_true',\n",
    "        precision=16 if args.fp16 else 32\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model,dataloaders=model.test_dataloader(),ckpt_path=\"best\")\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "\n",
    "    parser = argparse.ArgumentParser(\"main.py\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--gpu\", type=int, default=1)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=5)\n",
    "    parser.add_argument(\"--lr\", type=float, default=2e-5, help=\"learning rate\")\n",
    "    parser.add_argument(\"--random_seed\", type=int, default=2023) \n",
    "    parser.add_argument(\"--t_embed\", type=str, default=\"mbert\") \n",
    "    parser.add_argument(\"--a_embed\", type=str, default=\"en\") \n",
    "    parser.add_argument(\"--SaveRoot\", type=str, default='/mnt/External/Seagate/FedASR/LLaMa2/dacs') \n",
    "    parser.add_argument(\"--file_in\", type=str, default='/home/FedASR/dacs/centralized/saves/results/data2vec-audio-large-960h_total.csv') \n",
    "    parser.add_argument(\"--summary_dir_in\", type=str, default='/mnt/External/Seagate/FedASR/LLaMa2/dacs/EmbFeats/Lexical/Embeddings/text_data2vec-audio-large-960h_Phych-anomia')  \n",
    "    parser.add_argument(\"--Augment_dir_in\", type=str, default='/mnt/External/Seagate/FedASR/LLaMa2/dacs/EmbFeats/Lexical/Augment_data/text_data2vec-audio-large-960h_Phych-anomia')  \n",
    "    \n",
    "    config = parser.parse_args(args=[])\n",
    "    SaveRoot=config.SaveRoot\n",
    "    __file__ = os.path.abspath(\"__file__\")\n",
    "    script_path, file_extension = os.path.splitext(__file__)\n",
    "    \n",
    "\n",
    "    # 使用os.path模組取得檔案名稱\n",
    "    script_name = os.path.basename(script_path)\n",
    "\n",
    "    Output_dir=f\"{SaveRoot}/result/{script_name}/\"\n",
    "    os.makedirs(Output_dir, exist_ok=True)\n",
    "    print(config)\n",
    "    args = Arg()\n",
    "    args.t_hidden_size=Embsize_map['t_hidden_size'][config.t_embed]\n",
    "    args.epochs=config.epochs\n",
    "    main(args,config.__dict__)       \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed en\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed xlm --a_embed en\n",
    "\n",
    "# don\n",
    "python 0207_DM_multi.py --gpu 0 --t_embed xlm --a_embed gr\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed gr\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "args.epochs=config.epochs\n",
    "\n",
    "model = Model(args,config.__dict__)\n",
    "\n",
    "model.preprocess_loaded_summaries()\n",
    "# model.ADD_AugmentedTrainingData()\n",
    "# model.df2Dataset()\n",
    "self=model\n",
    "df_train = pd.read_pickle(f\"{config.Augment_dir_in}/train.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>dementia_labels</th>\n",
       "      <th>pred_str</th>\n",
       "      <th>ID</th>\n",
       "      <th>Aug_Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S001_aug0</th>\n",
       "      <td>S001_INV_1_2360_4266</td>\n",
       "      <td>INV: EVERYTHING THAT YOU SEE HAPPENING\\nPAR: W...</td>\n",
       "      <td>0</td>\n",
       "      <td>INV: EVERYTHING THAT YOU SEE HAPPENING\\nPAR: W...</td>\n",
       "      <td>S001</td>\n",
       "      <td>Based on the following dementia assessment res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S002_aug0</th>\n",
       "      <td>S002_PAR_0_6039_9568</td>\n",
       "      <td>INV: JUST LOOK AT THE PICTURE AND TELL ME EVER...</td>\n",
       "      <td>0</td>\n",
       "      <td>INV: SO JUST LOOK AT THE PICTURE AND TELL ME E...</td>\n",
       "      <td>S002</td>\n",
       "      <td>Based on the following dementia assessment res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S003_aug0</th>\n",
       "      <td>S003_INV_2_67484_68339</td>\n",
       "      <td>PAR: OKAY\\nPAR: THERE'S A LITTLE BOY AND HE'S ...</td>\n",
       "      <td>0</td>\n",
       "      <td>PAR: OKAY\\nPAR: THERE'S A LITTLE BOY AND HE'S ...</td>\n",
       "      <td>S003</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S004_aug0</th>\n",
       "      <td>S004_PAR_21_69694_71381</td>\n",
       "      <td>PAR: ARE YOU READY\\nINV: YES, I'M READY.\\nPAR:...</td>\n",
       "      <td>0</td>\n",
       "      <td>PAR: EARE IS YOUR MILLUS LOW\\nINV: MHM\\nPAR: W...</td>\n",
       "      <td>S004</td>\n",
       "      <td>Based on the following dementia assessment res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S005_aug0</th>\n",
       "      <td>S005_PAR_3_23473_26848</td>\n",
       "      <td>Certainly! Here's another description of the p...</td>\n",
       "      <td>0</td>\n",
       "      <td>INV: OKAY\\nINV: THERE'S THE PICTURE\\nINV: GOHE...</td>\n",
       "      <td>S005</td>\n",
       "      <td>Based on the following dementia assessment res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S150_aug0</th>\n",
       "      <td>S150_PAR_2_10330_12000</td>\n",
       "      <td>PAR: THE LADY IS UH PICKING UP A BOOK\\nPAR: PA...</td>\n",
       "      <td>1</td>\n",
       "      <td>PAR: THE LADY IS UH WIPING A DISH\\nPAR: WATER ...</td>\n",
       "      <td>S150</td>\n",
       "      <td>Based on the following dementia assessment res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S151_aug0</th>\n",
       "      <td>S151_PAR_2_20762_23322</td>\n",
       "      <td>Sure, here's a simulated result based on the g...</td>\n",
       "      <td>1</td>\n",
       "      <td>INV: EVERYTHING THAT YOU SEE HAPPENING IN THAT...</td>\n",
       "      <td>S151</td>\n",
       "      <td>Based on the following dementia assessment res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S153_aug0</th>\n",
       "      <td>S153_INV_0_0_3864</td>\n",
       "      <td>INV: PICTURE AND TELL ME EVERYTHING THAT YOU S...</td>\n",
       "      <td>0</td>\n",
       "      <td>INV: PTURE AND TELL ME EVERYTHING THAT YOU SEE...</td>\n",
       "      <td>S153</td>\n",
       "      <td>Based on the following dementia assessment res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S154_aug0</th>\n",
       "      <td>S154_PAR_5_22228_26102</td>\n",
       "      <td>INV: OKAY\\nINV: THE PICTURE\\nINV: TELL ME EVER...</td>\n",
       "      <td>1</td>\n",
       "      <td>INV: OKAY\\nINV: THE PICTURE\\nINV: TELL ME EVER...</td>\n",
       "      <td>S154</td>\n",
       "      <td>Based on the following dementia assessment res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S156_aug0</th>\n",
       "      <td>S156_PAR_7_37414_42600</td>\n",
       "      <td>Based on the following dementia assessment res...</td>\n",
       "      <td>1</td>\n",
       "      <td>INV: PICTURE OKAY\\nPAR: MHM\\nINV: CAN YOU TELL...</td>\n",
       "      <td>S156</td>\n",
       "      <td>Based on the following dementia assessment res...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              path  \\\n",
       "S001_aug0     S001_INV_1_2360_4266   \n",
       "S002_aug0     S002_PAR_0_6039_9568   \n",
       "S003_aug0   S003_INV_2_67484_68339   \n",
       "S004_aug0  S004_PAR_21_69694_71381   \n",
       "S005_aug0   S005_PAR_3_23473_26848   \n",
       "...                            ...   \n",
       "S150_aug0   S150_PAR_2_10330_12000   \n",
       "S151_aug0   S151_PAR_2_20762_23322   \n",
       "S153_aug0        S153_INV_0_0_3864   \n",
       "S154_aug0   S154_PAR_5_22228_26102   \n",
       "S156_aug0   S156_PAR_7_37414_42600   \n",
       "\n",
       "                                                        text dementia_labels  \\\n",
       "S001_aug0  INV: EVERYTHING THAT YOU SEE HAPPENING\\nPAR: W...               0   \n",
       "S002_aug0  INV: JUST LOOK AT THE PICTURE AND TELL ME EVER...               0   \n",
       "S003_aug0  PAR: OKAY\\nPAR: THERE'S A LITTLE BOY AND HE'S ...               0   \n",
       "S004_aug0  PAR: ARE YOU READY\\nINV: YES, I'M READY.\\nPAR:...               0   \n",
       "S005_aug0  Certainly! Here's another description of the p...               0   \n",
       "...                                                      ...             ...   \n",
       "S150_aug0  PAR: THE LADY IS UH PICKING UP A BOOK\\nPAR: PA...               1   \n",
       "S151_aug0  Sure, here's a simulated result based on the g...               1   \n",
       "S153_aug0  INV: PICTURE AND TELL ME EVERYTHING THAT YOU S...               0   \n",
       "S154_aug0  INV: OKAY\\nINV: THE PICTURE\\nINV: TELL ME EVER...               1   \n",
       "S156_aug0  Based on the following dementia assessment res...               1   \n",
       "\n",
       "                                                    pred_str ID     \\\n",
       "S001_aug0  INV: EVERYTHING THAT YOU SEE HAPPENING\\nPAR: W...  S001   \n",
       "S002_aug0  INV: SO JUST LOOK AT THE PICTURE AND TELL ME E...  S002   \n",
       "S003_aug0  PAR: OKAY\\nPAR: THERE'S A LITTLE BOY AND HE'S ...  S003   \n",
       "S004_aug0  PAR: EARE IS YOUR MILLUS LOW\\nINV: MHM\\nPAR: W...  S004   \n",
       "S005_aug0  INV: OKAY\\nINV: THERE'S THE PICTURE\\nINV: GOHE...  S005   \n",
       "...                                                      ...   ...   \n",
       "S150_aug0  PAR: THE LADY IS UH WIPING A DISH\\nPAR: WATER ...  S150   \n",
       "S151_aug0  INV: EVERYTHING THAT YOU SEE HAPPENING IN THAT...  S151   \n",
       "S153_aug0  INV: PTURE AND TELL ME EVERYTHING THAT YOU SEE...  S153   \n",
       "S154_aug0  INV: OKAY\\nINV: THE PICTURE\\nINV: TELL ME EVER...  S154   \n",
       "S156_aug0  INV: PICTURE OKAY\\nPAR: MHM\\nINV: CAN YOU TELL...  S156   \n",
       "\n",
       "                                                  Aug_Prompt  \n",
       "S001_aug0  Based on the following dementia assessment res...  \n",
       "S002_aug0  Based on the following dementia assessment res...  \n",
       "S003_aug0                                                NaN  \n",
       "S004_aug0  Based on the following dementia assessment res...  \n",
       "S005_aug0  Based on the following dementia assessment res...  \n",
       "...                                                      ...  \n",
       "S150_aug0  Based on the following dementia assessment res...  \n",
       "S151_aug0  Based on the following dementia assessment res...  \n",
       "S153_aug0  Based on the following dementia assessment res...  \n",
       "S154_aug0  Based on the following dementia assessment res...  \n",
       "S156_aug0  Based on the following dementia assessment res...  \n",
       "\n",
       "[108 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(\"/mnt/External/Seagate/FedASR/LLaMa2/dacs/EmbFeats/Lexical/Augment_data/text_data2vec-audio-large-960h_Phych-anomia/train.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
