{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu=1, epochs=5, lr=2e-05, random_seed=2023, inp1_embed='mbert_sentence', inp2_embed='en', SaveRoot='/mnt/External/Seagate/FedASR/LLaMa2/dacs')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\npython 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed en\\npython 0207_DM_multi.py --gpu 1 --t_embed xlm --a_embed en\\n\\n# don\\npython 0207_DM_multi.py --gpu 0 --t_embed xlm --a_embed gr\\npython 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed gr\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# torch:\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "from transformers import BertTokenizer, BertConfig, BertModel,XLMTokenizer, XLMModel\n",
    "\n",
    "from Dementia_challenge_models import SingleForwardModel, BertPooler, Audio_pretrain, ModelArg, Model_settings_dict, Text_pretrain\n",
    "import librosa\n",
    "\n",
    "class Model(SingleForwardModel):\n",
    "    def __init__(self, args, config):\n",
    "        super().__init__(args, config)\n",
    "        self.inp1Arg = args.inp1Arg\n",
    "        self.inp2Arg = args.inp2Arg\n",
    "        self.inp1_embed_type = self.config['inp1_embed']\n",
    "        self.inp2_embed_type = self.config['inp2_embed']\n",
    "        self.inp1_col_name = self.inp1Arg.inp_col_name\n",
    "        self.inp2_col_name = self.inp2Arg.inp_col_name\n",
    "        \n",
    "\n",
    "        self.inp1_hidden_size = self.inp1Arg.inp_hidden_size\n",
    "        self.inp2_hidden_size = self.inp2Arg.inp_hidden_size\n",
    "        self.hidden = int(self.inp1_hidden_size + self.inp2_hidden_size)\n",
    "        self.clf1 = nn.Linear(self.hidden, int(self.hidden/2))\n",
    "        self.clf2 = nn.Linear(int(self.hidden/2), self.num_labels)\n",
    "        self.inp1_tokenizer, self.inp1_model, self.pooler1=self._setup_embedding(self.inp1_embed_type, self.inp1_hidden_size)\n",
    "        self.inp2_tokenizer, self.inp2_model, self.pooler2=self._setup_embedding(self.inp2_embed_type, self.inp2_hidden_size)\n",
    "\n",
    "    def forward(self, inp1, inp2):\n",
    "        # Add or modify the forward method for NewModel2\n",
    "        # You can still use the functionality from the parent class by calling super().forward(inp)\n",
    "        # ...\n",
    "        out1 = self._get_embedding(inp1,self.inp1_embed_type, self.inp1_model, self.pooler1)\n",
    "        out2 = self._get_embedding(inp2,self.inp2_embed_type, self.inp2_model, self.pooler2)\n",
    "        output = torch.cat((out1,out2),axis=1)  \n",
    "        logits = self.clf2(self.clf1(output))\n",
    "    \n",
    "        return logits\n",
    "    def preprocess_dataframe(self):\n",
    "        \n",
    "        df_train = pd.read_csv(f\"{self.inp1Arg.file_in}/train.csv\")\n",
    "        df_dev = pd.read_csv(f\"{self.inp1Arg.file_in}/dev.csv\")\n",
    "        df_test = pd.read_csv(f\"{self.inp1Arg.file_in}/test.csv\")\n",
    "        self.df_train=self._Tokenize(df_train, self.inp1_embed_type, self.inp1Arg.inp_col_name, self.inp1_tokenizer)\n",
    "        self.df_dev=self._Tokenize(df_dev, self.inp1_embed_type, self.inp1Arg.inp_col_name, self.inp1_tokenizer)\n",
    "        self.df_test=self._Tokenize(df_test, self.inp1_embed_type, self.inp1Arg.inp_col_name, self.inp1_tokenizer)\n",
    "\n",
    "        self.df_train=self._Tokenize(self.df_train, self.inp2_embed_type,self.inp2Arg.inp_col_name, self.inp2_tokenizer)\n",
    "        self.df_dev=self._Tokenize(self.df_dev, self.inp2_embed_type,self.inp2Arg.inp_col_name, self.inp2_tokenizer)\n",
    "        self.df_test=self._Tokenize(self.df_test, self.inp2_embed_type,self.inp2Arg.inp_col_name, self.inp2_tokenizer)\n",
    "\n",
    "        print(f'# of train:{len(df_train)}, val:{len(df_dev)}, test:{len(df_test)}')\n",
    "        \n",
    "        \n",
    "        self._df2Dataset()\n",
    "    def _df2Dataset(self):\n",
    "        dtype1=self._DecideDtype(self.inp1_embed_type)\n",
    "        dtype2=self._DecideDtype(self.inp2_embed_type)\n",
    "\n",
    "        self.train_data = TensorDataset(\n",
    "            torch.tensor(self.df_train[self.inp1Arg.inp_col_name].tolist(), dtype=dtype1),\n",
    "            torch.tensor(self.df_train[self.inp2Arg.inp_col_name].tolist(), dtype=dtype2),\n",
    "            torch.tensor(self.df_train[self.label_cols].tolist(), dtype=torch.long),\n",
    "        )\n",
    "        \n",
    "        self.val_data = TensorDataset(\n",
    "             torch.tensor(self.df_dev[self.inp1Arg.inp_col_name].tolist(), dtype=dtype1),\n",
    "             torch.tensor(self.df_dev[self.inp2Arg.inp_col_name].tolist(), dtype=dtype2),\n",
    "            torch.tensor(self.df_dev[self.label_cols].tolist(), dtype=torch.long),\n",
    "        )\n",
    "\n",
    "        self.test_data = TensorDataset(\n",
    "             torch.tensor(self.df_test[self.inp1Arg.inp_col_name].tolist(), dtype=dtype1),\n",
    "             torch.tensor(self.df_test[self.inp2Arg.inp_col_name].tolist(), dtype=dtype2),\n",
    "            torch.tensor(self.df_test[self.label_cols].tolist(), dtype=torch.long),\n",
    "             torch.tensor(self.df_test.index.tolist(), dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inp1, inp2, labels = batch  \n",
    "        # token,  labels = batch  \n",
    "        logits = self(inp1, inp2) \n",
    "        # logits = self(token) \n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)   \n",
    "        \n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inp1, inp2, labels = batch  \n",
    "        # token, labels = batch  \n",
    "        logits = self(inp1, inp2) \n",
    "        # logits = self(token) \n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)     \n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "        # --> HERE STEP 2 <--\n",
    "        self.val_step_outputs.append({\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        })\n",
    "        # self.val_step_targets.append(y_true)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inp1, inp2, labels,id_ = batch \n",
    "        # token, labels,id_ = batch \n",
    "        print('id', id_)\n",
    "        logits = self(inp1, inp2) \n",
    "        # logits = self(token) \n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "        # --> HERE STEP 2 <--\n",
    "        self.test_step_outputs.append({\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        })\n",
    "        # self.test_step_targets.append(y_true)\n",
    "        return {\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "\n",
    "    def _save_results_to_csv(self, df_result, pred_dict, args, suffix):\n",
    "        # Save df_result to CSV\n",
    "        df_result.to_csv(f'{args.Output_dir}/{self.inp1_embed_type}_{self.inp2_embed_type}{suffix}.csv')\n",
    "\n",
    "        # Save pred_df to CSV\n",
    "        pred_df = pd.DataFrame(pred_dict)\n",
    "        pred_df.to_csv(f'{args.Output_dir}/{self.inp1_embed_type}_{self.inp2_embed_type}{suffix}_pred.csv')\n",
    "\n",
    "\n",
    "\n",
    "def main(args,config):\n",
    "    print(\"Using PyTorch Ver\", torch.__version__)\n",
    "    print(\"Fix Seed:\", config['random_seed'])\n",
    "    seed_everything( config['random_seed'])\n",
    "        \n",
    "    model = Model(args,config) \n",
    "    model.preprocess_dataframe()\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=f\"{SaveRoot}/Model/checkpoints\",\n",
    "        monitor='val_acc',\n",
    "        auto_insert_metric_name=True,\n",
    "        verbose=True,\n",
    "        mode='max', \n",
    "        save_top_k=1,\n",
    "        )    \n",
    "\n",
    "    print(\":: Start Training ::\")\n",
    "    #     \n",
    "    trainer = Trainer(\n",
    "        logger=False,\n",
    "        # callbacks=[early_stop_callback,checkpoint_callback],\n",
    "        callbacks=[early_stop_callback],\n",
    "        enable_checkpointing = True,\n",
    "        max_epochs=args.mdlArg.epochs,\n",
    "        fast_dev_run=args.mdlArg.test_mode,\n",
    "        num_sanity_val_steps=None if args.mdlArg.test_mode else 0,\n",
    "        # deterministic=True, # True會有bug，先false\n",
    "        deterministic=False,\n",
    "        # For GPU Setup\n",
    "        # gpus=[config['gpu']] if torch.cuda.is_available() else None,\n",
    "        strategy='ddp_find_unused_parameters_true',\n",
    "        precision=16 if args.mdlArg.fp16 else 32\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model,dataloaders=model.test_dataloader(),ckpt_path=\"best\")\n",
    "    \n",
    "\n",
    "if __name__ == '__main__': \n",
    "\n",
    "    parser = argparse.ArgumentParser(\"main.py\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--gpu\", type=int, default=1)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=5)\n",
    "    parser.add_argument(\"--lr\", type=float, default=2e-5, help=\"learning rate\")\n",
    "    parser.add_argument(\"--random_seed\", type=int, default=2023) \n",
    "    parser.add_argument(\"--inp1_embed\", type=str, default=\"mbert_sentence\", help=\"should only be raw text or raw audio. It has to be sentence level stuff\") \n",
    "    parser.add_argument(\"--inp2_embed\", type=str, default=\"en\", help=\"\") \n",
    "    parser.add_argument(\"--SaveRoot\", type=str, default='/mnt/External/Seagate/FedASR/LLaMa2/dacs') \n",
    "    \n",
    "    \n",
    "    config = parser.parse_args(args=[])\n",
    "    SaveRoot=config.SaveRoot\n",
    "    __file__ = os.path.abspath(\"__file__\")\n",
    "    script_path, file_extension = os.path.splitext(__file__)\n",
    "\n",
    "    # 使用os.path模組取得檔案名稱\n",
    "    script_name = os.path.basename(script_path)\n",
    "\n",
    "    Output_dir=f\"{SaveRoot}/result/{script_name}/\"\n",
    "    os.makedirs(Output_dir, exist_ok=True)\n",
    "    print(config)\n",
    "\n",
    "    \n",
    "    class Inp1Arg:\n",
    "        inp_hidden_size = Model_settings_dict[config.inp1_embed]['inp_hidden_size']\n",
    "        pool_hidden_size = inp_hidden_size # BERT-base: 768, BERT-large: 1024, BERT paper setting\n",
    "        linear_hidden_size = inp_hidden_size\n",
    "        inp_col_name = Model_settings_dict[config.inp1_embed]['inp_col_name']\n",
    "        file_in = Model_settings_dict[config.inp1_embed]['file_in']\n",
    "    class Inp2Arg:\n",
    "        inp_hidden_size = Model_settings_dict[config.inp2_embed]['inp_hidden_size']\n",
    "        pool_hidden_size = inp_hidden_size # BERT-base: 768, BERT-large: 1024, BERT paper setting\n",
    "        linear_hidden_size = inp_hidden_size\n",
    "        inp_col_name = Model_settings_dict[config.inp2_embed]['inp_col_name']\n",
    "        file_in = Model_settings_dict[config.inp2_embed]['file_in']\n",
    "    class Arg:\n",
    "        mdlArg=ModelArg()\n",
    "        inp1Arg=Inp1Arg()\n",
    "        inp2Arg=Inp2Arg()\n",
    "        Output_dir=Output_dir\n",
    "\n",
    "    args = Arg()\n",
    "    args.mdlArg.epochs=config.epochs\n",
    "    # main(args,config.__dict__)       \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed en\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed xlm --a_embed en\n",
    "\n",
    "# don\n",
    "python 0207_DM_multi.py --gpu 0 --t_embed xlm --a_embed gr\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed gr\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train:1868, val:206, test:800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4146765/190473761.py:88: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  torch.tensor(self.df_train[self.inp2Arg.inp_col_name].tolist(), dtype=dtype2),\n"
     ]
    }
   ],
   "source": [
    "args = Arg()\n",
    "args.mdlArg.epochs=config.epochs\n",
    "model = Model(args,config.__dict__) \n",
    "model.preprocess_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train:1868, val:206, test:800\n"
     ]
    }
   ],
   "source": [
    "self=model\n",
    "df_train = pd.read_csv(f\"{self.inp1Arg.file_in}/train.csv\")\n",
    "df_dev = pd.read_csv(f\"{self.inp1Arg.file_in}/dev.csv\")\n",
    "df_test = pd.read_csv(f\"{self.inp1Arg.file_in}/test.csv\")\n",
    "self.df_train=self._Tokenize(df_train, self.inp1_embed_type, self.inp1Arg.inp_col_name, self.inp1_tokenizer)\n",
    "self.df_dev=self._Tokenize(df_dev, self.inp1_embed_type, self.inp1Arg.inp_col_name, self.inp1_tokenizer)\n",
    "self.df_test=self._Tokenize(df_test, self.inp1_embed_type, self.inp1Arg.inp_col_name, self.inp1_tokenizer)\n",
    "\n",
    "self.df_train2=self._Tokenize(self.df_train, self.inp2_embed_type,self.inp2Arg.inp_col_name, self.inp2_tokenizer)\n",
    "self.df_dev2=self._Tokenize(self.df_dev, self.inp2_embed_type,self.inp2Arg.inp_col_name, self.inp2_tokenizer)\n",
    "self.df_test2=self._Tokenize(self.df_test, self.inp2_embed_type,self.inp2Arg.inp_col_name, self.inp2_tokenizer)\n",
    "\n",
    "print(f'# of train:{len(self.df_train2)}, val:{len(self.df_dev2)}, test:{len(self.df_test2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test\n",
    "self.df_test\n",
    "self.df_test2.columns\n",
    "\n",
    "\n",
    "# len(set(self.df_test2['ID   ']))\n",
    "df_test = pd.read_csv(f\"{self.inp1Arg.file_in}/test.csv\")\n",
    "\n",
    "df_test['role']=df_test['path'].apply(lambda x:x.split('.')[0].split('_')[1])\n",
    "# set(df_test['path'].apply(lambda x:tuple(x.split('.')[0].split('_')[:2])))\n",
    "len(df_test[df_test['role']=='PAR'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
