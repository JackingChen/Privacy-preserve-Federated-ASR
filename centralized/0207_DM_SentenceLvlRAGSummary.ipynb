{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Seed set to 2023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(gpu=1, epochs=5, lr=2e-05, random_seed=2023, t_embed='mbert', a_embed='en', SaveRoot='/mnt/External/Seagate/FedASR/LLaMa2/dacs', file_in='/home/FedASR/dacs/centralized/saves/results/data2vec-audio-large-960h_total.csv', summary_dir_in='/mnt/External/Seagate/FedASR/LLaMa2/dacs/EmbFeats/Lexical/Embeddings/text_data2vec-audio-large-960h_Phych-anomia')\n",
      "Using PyTorch Ver 2.1.1+cu121\n",
      "Fix Seed: 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train:1868, val:206, test:800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2823457/1444650004.py:250: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  torch.tensor(self.df_train[self.a_col_name].tolist(), dtype=torch.float),\n",
      "Trainer will use only 1 of 5 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=5)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: Start Training ::\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /mnt/External/Seagate/FedASR/LLaMa2/dacs/Model/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]\n",
      "/home/FedASR/.conda/envs/openai/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:375: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | t_model | BertModel     | 167 M \n",
      "1 | a_model | Wav2Vec2Model | 315 M \n",
      "2 | clf1    | Linear        | 1.3 M \n",
      "3 | clf2    | Linear        | 1.6 K \n",
      "------------------------------------------\n",
      "484 M     Trainable params\n",
      "0         Non-trainable params\n",
      "484 M     Total params\n",
      "1,936.504 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e633ba153e544e40899a4b354fb38a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2bd7f1ec76a47fb89e632bc07626de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved. New best score: 0.539\n",
      "Epoch 0, global step 234: 'val_acc' reached 0.53883 (best 0.53883), saving model to '/mnt/External/Seagate/FedASR/LLaMa2/dacs/Model/checkpoints/epoch=0-step=234.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------val_report-------\n",
      "              precision    recall  f1-score     support\n",
      "Control        0.601351  0.712000  0.652015  125.000000\n",
      "ProbableAD     0.379310  0.271605  0.316547   81.000000\n",
      "accuracy       0.538835  0.538835  0.538835    0.538835\n",
      "macro avg      0.490331  0.491802  0.484281  206.000000\n",
      "weighted avg   0.514044  0.538835  0.520107  206.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c768a856c45d4913a5795c2ea8119270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.010 >= min_delta = 0.0. New best score: 0.549\n",
      "Epoch 1, global step 468: 'val_acc' reached 0.54854 (best 0.54854), saving model to '/mnt/External/Seagate/FedASR/LLaMa2/dacs/Model/checkpoints/epoch=1-step=468.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------val_report-------\n",
      "              precision    recall  f1-score     support\n",
      "Control        0.597561  0.784000  0.678201  125.000000\n",
      "ProbableAD     0.357143  0.185185  0.243902   81.000000\n",
      "accuracy       0.548544  0.548544  0.548544    0.548544\n",
      "macro avg      0.477352  0.484593  0.461052  206.000000\n",
      "weighted avg   0.503028  0.548544  0.507433  206.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d1d3eca7d147ef8d7a6417cef0ea5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_acc improved by 0.005 >= min_delta = 0.0. New best score: 0.553\n",
      "Epoch 2, global step 702: 'val_acc' reached 0.55340 (best 0.55340), saving model to '/mnt/External/Seagate/FedASR/LLaMa2/dacs/Model/checkpoints/epoch=2-step=702.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------val_report-------\n",
      "              precision    recall  f1-score     support\n",
      "Control        0.600000  0.792000  0.682759  125.000000\n",
      "ProbableAD     0.365854  0.185185  0.245902   81.000000\n",
      "accuracy       0.553398  0.553398  0.553398    0.553398\n",
      "macro avg      0.482927  0.488593  0.464330  206.000000\n",
      "weighted avg   0.507933  0.553398  0.510985  206.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76747c088ddf40c68a1d3c5198cd0371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 936: 'val_acc' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------val_report-------\n",
      "              precision    recall  f1-score     support\n",
      "Control        0.613139  0.672000  0.641221  125.000000\n",
      "ProbableAD     0.405797  0.345679  0.373333   81.000000\n",
      "accuracy       0.543689  0.543689  0.543689    0.543689\n",
      "macro avg      0.509468  0.508840  0.507277  206.000000\n",
      "weighted avg   0.531611  0.543689  0.535887  206.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9021ad0b411f46f9a4e8270ef40eae1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1170: 'val_acc' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------val_report-------\n",
      "              precision    recall  f1-score     support\n",
      "Control        0.534653  0.432000  0.477876  125.000000\n",
      "ProbableAD     0.323810  0.419753  0.365591   81.000000\n",
      "accuracy       0.427184  0.427184  0.427184    0.427184\n",
      "macro avg      0.429231  0.425877  0.421734  206.000000\n",
      "weighted avg   0.451749  0.427184  0.433725  206.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /mnt/External/Seagate/FedASR/LLaMa2/dacs/Model/checkpoints/epoch=2-step=702.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]\n",
      "Loaded model weights from the checkpoint at /mnt/External/Seagate/FedASR/LLaMa2/dacs/Model/checkpoints/epoch=2-step=702.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc1f339a70d4cd7a6ca94e2bba1c173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')\n",
      "id tensor([ 8,  9, 10, 11, 12, 13, 14, 15], device='cuda:0')\n",
      "id tensor([16, 17, 18, 19, 20, 21, 22, 23], device='cuda:0')\n",
      "id tensor([24, 25, 26, 27, 28, 29, 30, 31], device='cuda:0')\n",
      "id tensor([32, 33, 34, 35, 36, 37, 38, 39], device='cuda:0')\n",
      "id tensor([40, 41, 42, 43, 44, 45, 46, 47], device='cuda:0')\n",
      "id tensor([48, 49, 50, 51, 52, 53, 54, 55], device='cuda:0')\n",
      "id tensor([56, 57, 58, 59, 60, 61, 62, 63], device='cuda:0')\n",
      "id tensor([64, 65, 66, 67, 68, 69, 70, 71], device='cuda:0')\n",
      "id tensor([72, 73, 74, 75, 76, 77, 78, 79], device='cuda:0')\n",
      "id tensor([80, 81, 82, 83, 84, 85, 86, 87], device='cuda:0')\n",
      "id tensor([88, 89, 90, 91, 92, 93, 94, 95], device='cuda:0')\n",
      "id tensor([ 96,  97,  98,  99, 100, 101, 102, 103], device='cuda:0')\n",
      "id tensor([104, 105, 106, 107, 108, 109, 110, 111], device='cuda:0')\n",
      "id tensor([112, 113, 114, 115, 116, 117, 118, 119], device='cuda:0')\n",
      "id tensor([120, 121, 122, 123, 124, 125, 126, 127], device='cuda:0')\n",
      "id tensor([128, 129, 130, 131, 132, 133, 134, 135], device='cuda:0')\n",
      "id tensor([136, 137, 138, 139, 140, 141, 142, 143], device='cuda:0')\n",
      "id tensor([144, 145, 146, 147, 148, 149, 150, 151], device='cuda:0')\n",
      "id tensor([152, 153, 154, 155, 156, 157, 158, 159], device='cuda:0')\n",
      "id tensor([160, 161, 162, 163, 164, 165, 166, 167], device='cuda:0')\n",
      "id tensor([168, 169, 170, 171, 172, 173, 174, 175], device='cuda:0')\n",
      "id tensor([176, 177, 178, 179, 180, 181, 182, 183], device='cuda:0')\n",
      "id tensor([184, 185, 186, 187, 188, 189, 190, 191], device='cuda:0')\n",
      "id tensor([192, 193, 194, 195, 196, 197, 198, 199], device='cuda:0')\n",
      "id tensor([200, 201, 202, 203, 204, 205, 206, 207], device='cuda:0')\n",
      "id tensor([208, 209, 210, 211, 212, 213, 214, 215], device='cuda:0')\n",
      "id tensor([216, 217, 218, 219, 220, 221, 222, 223], device='cuda:0')\n",
      "id tensor([224, 225, 226, 227, 228, 229, 230, 231], device='cuda:0')\n",
      "id tensor([232, 233, 234, 235, 236, 237, 238, 239], device='cuda:0')\n",
      "id tensor([240, 241, 242, 243, 244, 245, 246, 247], device='cuda:0')\n",
      "id tensor([248, 249, 250, 251, 252, 253, 254, 255], device='cuda:0')\n",
      "id tensor([256, 257, 258, 259, 260, 261, 262, 263], device='cuda:0')\n",
      "id tensor([264, 265, 266, 267, 268, 269, 270, 271], device='cuda:0')\n",
      "id tensor([272, 273, 274, 275, 276, 277, 278, 279], device='cuda:0')\n",
      "id tensor([280, 281, 282, 283, 284, 285, 286, 287], device='cuda:0')\n",
      "id tensor([288, 289, 290, 291, 292, 293, 294, 295], device='cuda:0')\n",
      "id tensor([296, 297, 298, 299, 300, 301, 302, 303], device='cuda:0')\n",
      "id tensor([304, 305, 306, 307, 308, 309, 310, 311], device='cuda:0')\n",
      "id tensor([312, 313, 314, 315, 316, 317, 318, 319], device='cuda:0')\n",
      "id tensor([320, 321, 322, 323, 324, 325, 326, 327], device='cuda:0')\n",
      "id tensor([328, 329, 330, 331, 332, 333, 334, 335], device='cuda:0')\n",
      "id tensor([336, 337, 338, 339, 340, 341, 342, 343], device='cuda:0')\n",
      "id tensor([344, 345, 346, 347, 348, 349, 350, 351], device='cuda:0')\n",
      "id tensor([352, 353, 354, 355, 356, 357, 358, 359], device='cuda:0')\n",
      "id tensor([360, 361, 362, 363, 364, 365, 366, 367], device='cuda:0')\n",
      "id tensor([368, 369, 370, 371, 372, 373, 374, 375], device='cuda:0')\n",
      "id tensor([376, 377, 378, 379, 380, 381, 382, 383], device='cuda:0')\n",
      "id tensor([384, 385, 386, 387, 388, 389, 390, 391], device='cuda:0')\n",
      "id tensor([392, 393, 394, 395, 396, 397, 398, 399], device='cuda:0')\n",
      "id tensor([400, 401, 402, 403, 404, 405, 406, 407], device='cuda:0')\n",
      "id tensor([408, 409, 410, 411, 412, 413, 414, 415], device='cuda:0')\n",
      "id tensor([416, 417, 418, 419, 420, 421, 422, 423], device='cuda:0')\n",
      "id tensor([424, 425, 426, 427, 428, 429, 430, 431], device='cuda:0')\n",
      "id tensor([432, 433, 434, 435, 436, 437, 438, 439], device='cuda:0')\n",
      "id tensor([440, 441, 442, 443, 444, 445, 446, 447], device='cuda:0')\n",
      "id tensor([448, 449, 450, 451, 452, 453, 454, 455], device='cuda:0')\n",
      "id tensor([456, 457, 458, 459, 460, 461, 462, 463], device='cuda:0')\n",
      "id tensor([464, 465, 466, 467, 468, 469, 470, 471], device='cuda:0')\n",
      "id tensor([472, 473, 474, 475, 476, 477, 478, 479], device='cuda:0')\n",
      "id tensor([480, 481, 482, 483, 484, 485, 486, 487], device='cuda:0')\n",
      "id tensor([488, 489, 490, 491, 492, 493, 494, 495], device='cuda:0')\n",
      "id tensor([496, 497, 498, 499, 500, 501, 502, 503], device='cuda:0')\n",
      "id tensor([504, 505, 506, 507, 508, 509, 510, 511], device='cuda:0')\n",
      "id tensor([512, 513, 514, 515, 516, 517, 518, 519], device='cuda:0')\n",
      "id tensor([520, 521, 522, 523, 524, 525, 526, 527], device='cuda:0')\n",
      "id tensor([528, 529, 530, 531, 532, 533, 534, 535], device='cuda:0')\n",
      "id tensor([536, 537, 538, 539, 540, 541, 542, 543], device='cuda:0')\n",
      "id tensor([544, 545, 546, 547, 548, 549, 550, 551], device='cuda:0')\n",
      "id tensor([552, 553, 554, 555, 556, 557, 558, 559], device='cuda:0')\n",
      "id tensor([560, 561, 562, 563, 564, 565, 566, 567], device='cuda:0')\n",
      "id tensor([568, 569, 570, 571, 572, 573, 574, 575], device='cuda:0')\n",
      "id tensor([576, 577, 578, 579, 580, 581, 582, 583], device='cuda:0')\n",
      "id tensor([584, 585, 586, 587, 588, 589, 590, 591], device='cuda:0')\n",
      "id tensor([592, 593, 594, 595, 596, 597, 598, 599], device='cuda:0')\n",
      "id tensor([600, 601, 602, 603, 604, 605, 606, 607], device='cuda:0')\n",
      "id tensor([608, 609, 610, 611, 612, 613, 614, 615], device='cuda:0')\n",
      "id tensor([616, 617, 618, 619, 620, 621, 622, 623], device='cuda:0')\n",
      "id tensor([624, 625, 626, 627, 628, 629, 630, 631], device='cuda:0')\n",
      "id tensor([632, 633, 634, 635, 636, 637, 638, 639], device='cuda:0')\n",
      "id tensor([640, 641, 642, 643, 644, 645, 646, 647], device='cuda:0')\n",
      "id tensor([648, 649, 650, 651, 652, 653, 654, 655], device='cuda:0')\n",
      "id tensor([656, 657, 658, 659, 660, 661, 662, 663], device='cuda:0')\n",
      "id tensor([664, 665, 666, 667, 668, 669, 670, 671], device='cuda:0')\n",
      "id tensor([672, 673, 674, 675, 676, 677, 678, 679], device='cuda:0')\n",
      "id tensor([680, 681, 682, 683, 684, 685, 686, 687], device='cuda:0')\n",
      "id tensor([688, 689, 690, 691, 692, 693, 694, 695], device='cuda:0')\n",
      "id tensor([696, 697, 698, 699, 700, 701, 702, 703], device='cuda:0')\n",
      "id tensor([704, 705, 706, 707, 708, 709, 710, 711], device='cuda:0')\n",
      "id tensor([712, 713, 714, 715, 716, 717, 718, 719], device='cuda:0')\n",
      "id tensor([720, 721, 722, 723, 724, 725, 726, 727], device='cuda:0')\n",
      "id tensor([728, 729, 730, 731, 732, 733, 734, 735], device='cuda:0')\n",
      "id tensor([736, 737, 738, 739, 740, 741, 742, 743], device='cuda:0')\n",
      "id tensor([744, 745, 746, 747, 748, 749, 750, 751], device='cuda:0')\n",
      "id tensor([752, 753, 754, 755, 756, 757, 758, 759], device='cuda:0')\n",
      "id tensor([760, 761, 762, 763, 764, 765, 766, 767], device='cuda:0')\n",
      "id tensor([768, 769, 770, 771, 772, 773, 774, 775], device='cuda:0')\n",
      "id tensor([776, 777, 778, 779, 780, 781, 782, 783], device='cuda:0')\n",
      "id tensor([784, 785, 786, 787, 788, 789, 790, 791], device='cuda:0')\n",
      "id tensor([792, 793, 794, 795, 796, 797, 798, 799], device='cuda:0')\n",
      "-------test_report-------\n",
      "              precision    recall  f1-score   support\n",
      "Control        0.670418  0.798851  0.729021  522.0000\n",
      "ProbableAD     0.410112  0.262590  0.320175  278.0000\n",
      "accuracy       0.612500  0.612500  0.612500    0.6125\n",
      "macro avg      0.540265  0.530720  0.524598  800.0000\n",
      "weighted avg   0.579962  0.612500  0.586947  800.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\npython 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed en\\npython 0207_DM_multi.py --gpu 1 --t_embed xlm --a_embed en\\n\\n# don\\npython 0207_DM_multi.py --gpu 0 --t_embed xlm --a_embed gr\\npython 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed gr\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# torch:\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
    "from transformers import BertTokenizer, BertConfig, BertModel,XLMTokenizer, XLMModel\n",
    "import librosa\n",
    "from Dementia_challenge_models import Embsize_map\n",
    "class Arg:\n",
    "    version = 1\n",
    "    # data\n",
    "    epochs: int = 5  # Max Epochs, BERT paper setting [3,4,5]\n",
    "    max_length: int = 350  # Max Length input size\n",
    "    report_cycle: int = 30  # Report (Train Metrics) Cycle\n",
    "    cpu_workers: int = os.cpu_count()  # Multi cpu workers\n",
    "    test_mode: bool = False  # Test Mode enables `fast_dev_run`\n",
    "    optimizer: str = 'AdamW'  # AdamW vs AdamP\n",
    "    lr_scheduler: str = 'exp'  # ExponentialLR vs CosineAnnealingWarmRestarts\n",
    "    fp16: bool = False  # Enable train on FP16\n",
    "    a_hidden_size = 512 # BERT-base: 768, BERT-large: 1024, BERT paper setting\n",
    "    t_hidden_size = 768\n",
    "    aug_hidden_size = Embsize_map['t_hidden_size']['Embedding']\n",
    "    t_x_hidden_size = a_hidden_size+t_hidden_size+aug_hidden_size\n",
    "    batch_size: int = 8\n",
    "            \n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self,hidden_size):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "    \n",
    "    \n",
    "class Model(LightningModule):\n",
    "    def __init__(self, args,config):\n",
    "        super().__init__()\n",
    "        # config:\n",
    "        \n",
    "        self.args = args\n",
    "        self.config = config\n",
    "        self.batch_size = self.args.batch_size\n",
    "        \n",
    "        # meta data:\n",
    "        self.epochs_index = 0\n",
    "        self.label_cols = 'dementia_labels'\n",
    "        self.label_names = ['Control','ProbableAD']\n",
    "        self.num_labels = 2\n",
    "        self.t_embed_type = self.config['t_embed']\n",
    "        self.a_embed_type = self.config['a_embed']\n",
    "        self.a_hidden = self.args.a_hidden_size\n",
    "        \n",
    "        # --> HERE STEP 1 <--\n",
    "        # ATTRIBUTES TO SAVE BATCH OUTPUTS\n",
    "        self.test_step_outputs = []   # save outputs in each batch to compute metric overall epoch\n",
    "        self.test_step_targets = []   # save targets in each batch to compute metric overall epoch\n",
    "        self.val_step_outputs = []        # save outputs in each batch to compute metric overall epoch\n",
    "        self.val_step_targets = []        # save targets in each batch to compute metric overall epoch\n",
    "\n",
    "\n",
    "        if self.t_embed_type == \"mbert\":\n",
    "            self.t_hidden = self.args.t_hidden_size\n",
    "            \n",
    "            t_pretrained = 'bert-base-multilingual-uncased'\n",
    "            self.t_tokenizer = BertTokenizer.from_pretrained(t_pretrained)\n",
    "            self.t_model = BertModel.from_pretrained(t_pretrained)\n",
    "            \n",
    "            \n",
    "        elif self.t_embed_type == \"xlm\":\n",
    "            self.t_hidden = self.args.t_hidden_size\n",
    "            \n",
    "            t_pretrained = 'xlm-mlm-100-1280'\n",
    "            self.t_tokenizer = XLMTokenizer.from_pretrained(t_pretrained)\n",
    "            self.t_model = XLMModel.from_pretrained(t_pretrained)\n",
    "            self.pooler = BertPooler(self.t_hidden)\n",
    "            \n",
    "        self.hidden = int(self.args.t_x_hidden_size)\n",
    "        \n",
    "        if self.a_embed_type == \"en\":\n",
    "            a_pretrained =  \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "            \n",
    "        elif self.a_embed_type == \"gr\":\n",
    "            a_pretrained =  \"lighteternal/wav2vec2-large-xlsr-53-greek\"\n",
    "\n",
    "        elif self.a_embed_type == \"multi\":\n",
    "            a_pretrained = \"voidful/wav2vec2-xlsr-multilingual-56\"\n",
    "            \n",
    "        elif self.a_embed_type == \"wv\":\n",
    "            a_pretrained ='facebook/wav2vec2-base'\n",
    "            \n",
    "        self.a_tokenizer = Wav2Vec2FeatureExtractor.from_pretrained(a_pretrained)\n",
    "        self.a_model = Wav2Vec2Model.from_pretrained(a_pretrained)\n",
    "        \n",
    "        \n",
    "        self.clf1 = nn.Linear(self.hidden, int(self.hidden/2))\n",
    "        self.clf2 = nn.Linear(int(self.hidden/2), self.num_labels)\n",
    "        \n",
    "            \n",
    "            \n",
    "    def forward(self, text, audio, Aug):\n",
    "    # def forward(self, text):\n",
    "        \n",
    "        if self.t_embed_type == \"mbert\":\n",
    "            t_out = self.t_model(text)[1] \n",
    "\n",
    "            \n",
    "        elif self.t_embed_type == \"xlm\":\n",
    "            t_out = self.t_model(text)[0]\n",
    "            t_out = self.pooler(t_out)\n",
    "            \n",
    "            \n",
    "        a_out = self.a_model(audio)['extract_features']#[2] #last_hidden_state , feature extraction\n",
    "        a_out = a_out[:, 0, :] \n",
    "        \n",
    "        # print(a_out)\n",
    "        # print(a_out['extract_features'].shape) # ([8, 437, 512])\n",
    "        # print(a_out['last_hidden_state'].shape) # ([8, 437, 1024]) => pooling 필요\n",
    "        \n",
    "        \n",
    "        output = torch.cat((t_out,a_out,Aug),axis=1)   \n",
    "        # output = t_out\n",
    "        # print(output.shape)\n",
    "        \n",
    "        logits = self.clf2(self.clf1(output))\n",
    "    \n",
    "        return logits\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config['lr'])\n",
    "        scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "        }\n",
    "\n",
    "    def preprocess_dataframe(self):\n",
    "        \n",
    "        tg_sr = 16000\n",
    "        t_col_name = \"text\" \n",
    "        a_col_name = \"path\"         \n",
    "        # df = pd.read_json('/mnt/Internal/FedASR/Data/230126_total_asr_data.json')\n",
    "        df = pd.read_csv(config.file_in)\n",
    "\n",
    "        df[t_col_name] = df[t_col_name].map(lambda x: self.t_tokenizer.encode(\n",
    "            str(x),\n",
    "            padding = 'max_length',\n",
    "            max_length=self.args.max_length,\n",
    "            truncation=True,\n",
    "            ))\n",
    "        \n",
    "        audio_root=\"/mnt/Internal/FedASR/Data/ADReSS-IS2020-data/clips\"\n",
    "        # 원래 길이: 562992, batch 16: 90000, batch 8: 140000\n",
    "        # max_length=16000, truncation=True 이건 일단 돌려보고 결정 => 뒤쪽, 앞에쪽 뭐보면 좋을 지 그런거 check하면 좋으니까! \n",
    "        df[a_col_name] = df[a_col_name].map(lambda x: self.a_tokenizer(\n",
    "            librosa.load(f\"{audio_root}/{x}\")[0],\n",
    "            padding='max_length',\n",
    "            sampling_rate = tg_sr,\n",
    "            max_length=100000, \n",
    "            truncation=True\n",
    "            )['input_values'][0])\n",
    "        \n",
    "        \n",
    "\n",
    "        df_train = df[df['ex'] == 'train']\n",
    "        df_val = df[df['ex'] == 'dev']\n",
    "        df_test = df[df['ex'] == 'test']\n",
    "\n",
    "\n",
    "        print(f'# of train:{len(df_train)}, val:{len(df_val)}, test:{len(df_test)}')\n",
    "        self.t_col_name=t_col_name\n",
    "        self.a_col_name=a_col_name\n",
    "        self.df_train=df_train\n",
    "        self.df_val=df_val\n",
    "        self.df_test=df_test\n",
    "    def merge_DataAug2Data(self):\n",
    "        pname_col_name='ID   '\n",
    "        similar_col_name='session'\n",
    "        def AppendID(df_data):\n",
    "            if pname_col_name not in df_data.columns:\n",
    "                df_data[pname_col_name]=df_data[similar_col_name]\n",
    "        AppendID(self.df_train_aug)\n",
    "        AppendID(self.df_val_aug)\n",
    "        AppendID(self.df_test_aug)\n",
    "        # self.df_train = pd.merge(self.df_train, self.df_train_aug, on='ID   ', how='left')\n",
    "        # self.df_val = pd.merge(self.df_val, self.df_val_aug, on='ID   ', how='left')\n",
    "        # self.df_test = pd.merge(self.df_test, self.df_test_aug, on='ID   ', how='left')\n",
    "        self.df_train = pd.merge(self.df_train, self.df_train_aug, on='ID   ', how='left', suffixes=('', '_aug'))\n",
    "        self.df_val = pd.merge(self.df_val, self.df_val_aug, on='ID   ', how='left', suffixes=('', '_aug'))\n",
    "        self.df_test = pd.merge(self.df_test, self.df_test_aug, on='ID   ', how='left', suffixes=('', '_aug'))\n",
    "        \n",
    "    def preprocess_loaded_summaries(self):\n",
    "        df_train = pd.read_pickle(f\"{config.summary_dir_in}/train.pkl\")\n",
    "        df_val = pd.read_pickle(f\"{config.summary_dir_in}/dev.pkl\")\n",
    "        df_test = pd.read_pickle(f\"{config.summary_dir_in}/test.pkl\")\n",
    "\n",
    "        Aug_col_name='Embedding'\n",
    "\n",
    "        def Tokenize(df_data):\n",
    "            df_data[Aug_col_name] = df_data[Aug_col_name].map(lambda x: self.t_tokenizer.encode(\n",
    "                str(x),\n",
    "                padding = 'max_length',\n",
    "                max_length=self.args.max_length,\n",
    "                truncation=True,\n",
    "                ))\n",
    "            return df_data\n",
    "        df_train=Tokenize(df_train)\n",
    "        df_val=Tokenize(df_val)\n",
    "        df_test=Tokenize(df_test)\n",
    "        df_test = df_test.reset_index(drop=True)\n",
    "        self.df_train_aug=df_train\n",
    "        self.df_val_aug=df_val\n",
    "        self.df_test_aug=df_test\n",
    "        self.Aug_col_name=Aug_col_name\n",
    "    def df2Dataset(self):\n",
    "        self.train_data = TensorDataset(\n",
    "            torch.tensor(self.df_train[self.t_col_name].tolist(), dtype=torch.long),\n",
    "            torch.tensor(self.df_train[self.a_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(self.df_train[self.Aug_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(self.df_train[self.label_cols].tolist(), dtype=torch.long),\n",
    "        )\n",
    "        \n",
    "        self.val_data = TensorDataset(\n",
    "             torch.tensor(self.df_val[self.t_col_name].tolist(), dtype=torch.long),\n",
    "             torch.tensor(self.df_val[self.a_col_name].tolist(), dtype=torch.float),\n",
    "             torch.tensor(self.df_val[self.Aug_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(self.df_val[self.label_cols].tolist(), dtype=torch.long),\n",
    "        )\n",
    "\n",
    "        self.test_data = TensorDataset(\n",
    "             torch.tensor(self.df_test[self.t_col_name].tolist(), dtype=torch.long),\n",
    "             torch.tensor(self.df_test[self.a_col_name].tolist(), dtype=torch.float),\n",
    "             torch.tensor(self.df_test[self.Aug_col_name].tolist(), dtype=torch.float),\n",
    "            torch.tensor(self.df_test[self.label_cols].tolist(), dtype=torch.long),\n",
    "             torch.tensor(self.df_test.index.tolist(), dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "\n",
    "        return DataLoader(\n",
    "            self.test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.args.cpu_workers,\n",
    "        )\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        token, audio, Aug,labels = batch  \n",
    "        # token,  labels = batch  \n",
    "        logits = self(token, audio, Aug) \n",
    "        # logits = self(token) \n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)   \n",
    "        \n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        token, audio, Aug, labels = batch  \n",
    "        # token, labels = batch  \n",
    "        logits = self(token, audio, Aug) \n",
    "        # logits = self(token) \n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)     \n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "        # --> HERE STEP 2 <--\n",
    "        self.val_step_outputs.append({\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        })\n",
    "        # self.val_step_targets.append(y_true)\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "        \n",
    "            \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        token, audio, Aug, labels,id_ = batch \n",
    "        # token, labels,id_ = batch \n",
    "        print('id', id_)\n",
    "        logits = self(token, audio, Aug) \n",
    "        # logits = self(token) \n",
    "        \n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        y_true = list(labels.cpu().numpy())\n",
    "        y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "        # --> HERE STEP 2 <--\n",
    "        self.test_step_outputs.append({\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        })\n",
    "        # self.test_step_targets.append(y_true)\n",
    "        return {\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred,\n",
    "        }\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        loss = torch.tensor(0, dtype=torch.float)\n",
    "        # print(\"Value= \",self.val_step_outputs)\n",
    "        # print(\"type(self.val_step_outputs)=\",type(self.val_step_outputs))\n",
    "        # print(\"type(self.val_step_outputs[0])=\",type(self.val_step_outputs[0]))\n",
    "        # print(\"type(self.val_step_outputs[0] loss)=\",type(self.val_step_outputs[0]['loss']))\n",
    "        for i in self.val_step_outputs:\n",
    "            loss += i['loss'].cpu().detach()\n",
    "        _loss = loss / len(self.val_step_outputs)\n",
    "        loss = float(_loss)\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for i in self.val_step_outputs:\n",
    "            y_true += i['y_true']\n",
    "            y_pred += i['y_pred']\n",
    "            \n",
    "        y_pred = np.asanyarray(y_pred)#y_temp_pred y_pred\n",
    "        y_true = np.asanyarray(y_true)\n",
    "        \n",
    "        pred_dict = {}\n",
    "        pred_dict['y_pred']= y_pred\n",
    "        pred_dict['y_true']= y_true\n",
    "        \n",
    "        val_acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "        \n",
    "        self.log(\"val_acc\", val_acc)\n",
    "        # print(\"y_pred= \", y_pred)\n",
    "        # print('\\n\\n\\n')\n",
    "        # print(\"y_true= \", y_true)\n",
    "        # print('\\n\\n\\n')\n",
    "        print(\"-------val_report-------\")\n",
    "        metrics_dict = classification_report(y_true, y_pred,zero_division=1,\n",
    "                                             target_names = self.label_names, \n",
    "                                             output_dict=True)\n",
    "        df_result = pd.DataFrame(metrics_dict).transpose()\n",
    "        pprint(df_result)\n",
    "        \n",
    "\n",
    "        df_result.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_val.csv')\n",
    "\n",
    "        pred_df = pd.DataFrame(pred_dict)\n",
    "        pred_df.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_val_pred.csv')\n",
    "        self.val_step_outputs.clear()\n",
    "        # self.val_step_targets.clear()\n",
    "        return {'loss': _loss}\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for i in self.test_step_outputs:\n",
    "            y_true += i['y_true']\n",
    "            y_pred += i['y_pred']\n",
    "            \n",
    "        y_pred = np.asanyarray(y_pred)#y_temp_pred y_pred\n",
    "        y_true = np.asanyarray(y_true)\n",
    "        \n",
    "        pred_dict = {}\n",
    "        pred_dict['y_pred']= y_pred\n",
    "        pred_dict['y_true']= y_true\n",
    "        \n",
    "        \n",
    "        print(\"-------test_report-------\")\n",
    "        metrics_dict = classification_report(y_true, y_pred,zero_division=1,\n",
    "                                             target_names = self.label_names, \n",
    "                                             output_dict=True)\n",
    "        df_result = pd.DataFrame(metrics_dict).transpose()\n",
    "        self.test_step_outputs.clear()\n",
    "        # self.test_step_targets.clear()\n",
    "        pprint(df_result)\n",
    "\n",
    "        df_result.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_test.csv')\n",
    "\n",
    "        pred_df = pd.DataFrame(pred_dict)\n",
    "        pred_df.to_csv(\n",
    "            f'{Output_dir}/{datetime.now().__format__(\"%m%d_%H%M\")}_DM_MM_{self.t_embed_type}_{self.a_embed_type}_test_pred.csv')\n",
    "\n",
    "    \n",
    "def main(args,config):\n",
    "    print(\"Using PyTorch Ver\", torch.__version__)\n",
    "    print(\"Fix Seed:\", config['random_seed'])\n",
    "    seed_everything( config['random_seed'])\n",
    "        \n",
    "    model = Model(args,config) \n",
    "    model.preprocess_dataframe()\n",
    "    model.preprocess_loaded_summaries()\n",
    "    model.merge_DataAug2Data()\n",
    "    model.df2Dataset()\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=f\"{SaveRoot}/Model/checkpoints\",\n",
    "        monitor='val_acc',\n",
    "        auto_insert_metric_name=True,\n",
    "        verbose=True,\n",
    "        mode='max', \n",
    "        save_top_k=1,\n",
    "      )    \n",
    "\n",
    "    print(\":: Start Training ::\")\n",
    "    #     \n",
    "    trainer = Trainer(\n",
    "        logger=False,\n",
    "        callbacks=[early_stop_callback,checkpoint_callback],\n",
    "        enable_checkpointing = True,\n",
    "        max_epochs=args.epochs,\n",
    "        fast_dev_run=args.test_mode,\n",
    "        num_sanity_val_steps=None if args.test_mode else 0,\n",
    "        deterministic=False, # ensure full reproducibility from run to run you need to set seeds for pseudo-random generators,\n",
    "        # For GPU Setup\n",
    "        # gpus=[config['gpu']] if torch.cuda.is_available() else None,\n",
    "        # strategy='ddp_find_unused_parameters_true',\n",
    "        precision=16 if args.fp16 else 32\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model,dataloaders=model.test_dataloader(),ckpt_path=\"best\")\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "\n",
    "    parser = argparse.ArgumentParser(\"main.py\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--gpu\", type=int, default=1)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=5)\n",
    "    parser.add_argument(\"--lr\", type=float, default=2e-5, help=\"learning rate\")\n",
    "    parser.add_argument(\"--random_seed\", type=int, default=2023) \n",
    "    parser.add_argument(\"--t_embed\", type=str, default=\"mbert\") \n",
    "    parser.add_argument(\"--a_embed\", type=str, default=\"en\") \n",
    "    parser.add_argument(\"--SaveRoot\", type=str, default='/mnt/External/Seagate/FedASR/LLaMa2/dacs') \n",
    "    parser.add_argument(\"--file_in\", type=str, default='/home/FedASR/dacs/centralized/saves/results/data2vec-audio-large-960h_total.csv') \n",
    "    parser.add_argument(\"--summary_dir_in\", type=str, default='/mnt/External/Seagate/FedASR/LLaMa2/dacs/EmbFeats/Lexical/Embeddings/text_data2vec-audio-large-960h_Phych-anomia') \n",
    "    \n",
    "    config = parser.parse_args(args=[])\n",
    "    SaveRoot=config.SaveRoot\n",
    "    \n",
    "    __file__ = os.path.abspath(\"__file__\")\n",
    "    script_path, file_extension = os.path.splitext(__file__)\n",
    "\n",
    "    # 使用os.path模組取得檔案名稱\n",
    "    script_name = os.path.basename(script_path)\n",
    "\n",
    "    Output_dir=f\"{SaveRoot}/result/{script_name}/\"\n",
    "    os.makedirs(Output_dir, exist_ok=True)\n",
    "\n",
    "    print(config)\n",
    "    args = Arg()\n",
    "    args.epochs=config.epochs\n",
    "    main(args,config.__dict__)       \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed en\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed xlm --a_embed en\n",
    "\n",
    "# don\n",
    "python 0207_DM_multi.py --gpu 0 --t_embed xlm --a_embed gr\n",
    "python 0207_DM_multi.py --gpu 1 --t_embed mbert --a_embed gr\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 768 350 1630\n"
     ]
    }
   ],
   "source": [
    "print(args.a_hidden_size,args.t_hidden_size,args.aug_hidden_size,args.t_x_hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train:1868, val:206, test:800\n"
     ]
    }
   ],
   "source": [
    "args = Arg()\n",
    "args.epochs=config.epochs\n",
    "model=Model(args, config.__dict__)\n",
    "model.preprocess_dataframe()\n",
    "model.preprocess_loaded_summaries()\n",
    "model.merge_DataAug2Data()\n",
    "model.df2Dataset()\n",
    "self=model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m token, audio, Aug, labels \u001b[38;5;241m=\u001b[39m batch  \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# token, labels = batch  \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAug\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# logits = self(token) \u001b[39;00m\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(logits, labels)     \n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 135\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, text, audio, Aug)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, audio, Aug):\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# def forward(self, text):\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_embed_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmbert\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 135\u001b[0m         t_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m] \n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_embed_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    139\u001b[0m         t_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_model(text)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:960\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn_if_padding_and_no_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/transformers/modeling_utils.py:4157\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   4154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   4156\u001b[0m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[0;32m-> 4157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m   4158\u001b[0m     warn_string \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4162\u001b[0m     )\n\u001b[1;32m   4164\u001b[0m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[1;32m   4165\u001b[0m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "for batch in model.val_data:\n",
    "    token, audio, Aug, labels = batch  \n",
    "    # token, labels = batch  \n",
    "    logits = self(token, audio, Aug) \n",
    "    # logits = self(token) \n",
    "    loss = nn.CrossEntropyLoss()(logits, labels)     \n",
    "\n",
    "    preds = logits.argmax(dim=-1)\n",
    "\n",
    "    y_true = list(labels.cpu().numpy())\n",
    "    y_pred = list(preds.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000])\n",
      "torch.Size([350])\n",
      "torch.Size([])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>text</th>\n",
       "      <th>dementia_labels</th>\n",
       "      <th>pred_str</th>\n",
       "      <th>ID</th>\n",
       "      <th>mmse</th>\n",
       "      <th>ex</th>\n",
       "      <th>path_aug</th>\n",
       "      <th>text_aug</th>\n",
       "      <th>dementia_labels_aug</th>\n",
       "      <th>...</th>\n",
       "      <th>session</th>\n",
       "      <th>role</th>\n",
       "      <th>number</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Psych_Summary</th>\n",
       "      <th>Psych_Prompt</th>\n",
       "      <th>Similarity_Emb</th>\n",
       "      <th>Similarity_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.0026251145, -0.0063604997, -0.009782849, -...</td>\n",
       "      <td>[101, 10103, 12291, 11378, 10127, 21812, 16053...</td>\n",
       "      <td>0</td>\n",
       "      <td>THE STOOL IS TIPPING OT THE LITTLE BOY AND HE'...</td>\n",
       "      <td>S015</td>\n",
       "      <td>29.0</td>\n",
       "      <td>train</td>\n",
       "      <td>S015_PAR_7_26693_32357</td>\n",
       "      <td>INV: AND HERE'S THE PICTURE\\nPAR: ALL OF THE A...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S015</td>\n",
       "      <td>INV+PAR</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>56600</td>\n",
       "      <td>[101, 138, 118, 121, 119, 63547, 62338, 48626,...</td>\n",
       "      <td>detected problems:\\n- Empty speech: PAR's utte...</td>\n",
       "      <td>psycological definition:\\n\\n            - defi...</td>\n",
       "      <td>[-0.017662624105078045, 0.0017859735411924677,...</td>\n",
       "      <td>S015/S154/S020/S033/S071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.03724482, -0.06742435, -0.07992418, -0.093...</td>\n",
       "      <td>[101, 10197, 14650, 10346, 11332, 102, 0, 0, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>IT MUST BE JUNE</td>\n",
       "      <td>S076</td>\n",
       "      <td>28.0</td>\n",
       "      <td>train</td>\n",
       "      <td>S076_PAR_8_37400_38800</td>\n",
       "      <td>INV: LOOK AT THAT PICTURE\\nPAR: MHM\\nINV: AND ...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S076</td>\n",
       "      <td>INV+PAR</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>43563</td>\n",
       "      <td>[101, 138, 118, 121, 119, 75188, 95584, 11444,...</td>\n",
       "      <td>detected problems:\\n- Empty speech: PAR's utte...</td>\n",
       "      <td>psycological definition:\\n\\n            - defi...</td>\n",
       "      <td>[-0.01690500758420462, 0.021220975649458663, 0...</td>\n",
       "      <td>S076/S095/S089/S156/S126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.3376349, 0.4526255, 0.3523399, 0.32248497, ...</td>\n",
       "      <td>[101, 151, 11530, 112, 162, 11811, 10197, 2238...</td>\n",
       "      <td>0</td>\n",
       "      <td>I DON'T SEE IT SNOWING</td>\n",
       "      <td>S018</td>\n",
       "      <td>29.0</td>\n",
       "      <td>train</td>\n",
       "      <td>S018_PAR_12_68242_69305</td>\n",
       "      <td>INV: OKAY\\nINV: AND THERE'S THE PICTURE\\nPAR: ...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S018</td>\n",
       "      <td>INV+PAR</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>71267</td>\n",
       "      <td>[101, 138, 118, 121, 119, 65945, 92425, 79225,...</td>\n",
       "      <td>detected problems:\\n- Empty speech: PAR's utte...</td>\n",
       "      <td>psycological definition:\\n\\n            - defi...</td>\n",
       "      <td>[-0.018221234206087917, 0.0172382670926329, 0....</td>\n",
       "      <td>S018/S032/S089/S048/S006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0017839464, 0.00059347186, 0.00038148474, 0...</td>\n",
       "      <td>[101, 10103, 20098, 10453, 40811, 10127, 11481...</td>\n",
       "      <td>0</td>\n",
       "      <td>THE COOKIE JAR IS OPEN OF COURSE THE CUPBOARD'...</td>\n",
       "      <td>S062</td>\n",
       "      <td>30.0</td>\n",
       "      <td>train</td>\n",
       "      <td>S062_PAR_2_14910_19143</td>\n",
       "      <td>PAR: WELL THERE'S A KID STEALING COOKIES FROM ...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S062</td>\n",
       "      <td>INV+PAR</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>41000</td>\n",
       "      <td>[101, 138, 118, 121, 119, 77694, 26622, 42299,...</td>\n",
       "      <td>detected problems:\\n- Empty speech: PAR's utte...</td>\n",
       "      <td>psycological definition:\\n\\n            - defi...</td>\n",
       "      <td>[-0.014507318536576209, 0.01572004712565187, 0...</td>\n",
       "      <td>S062/S001/S118/S083/S126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.0016942845, 0.022142626, 0.02669364, -0.01...</td>\n",
       "      <td>[101, 10103, 11917, 112, 161, 16484, 10323, 10...</td>\n",
       "      <td>0</td>\n",
       "      <td>THE WATER'S RUNNING OVER</td>\n",
       "      <td>S059</td>\n",
       "      <td>30.0</td>\n",
       "      <td>train</td>\n",
       "      <td>S059_PAR_4_14123_15540</td>\n",
       "      <td>INV: ON IN THE PICTURE\\nPAR: GIRL IS REACHING ...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S059</td>\n",
       "      <td>INV+PAR</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>58323</td>\n",
       "      <td>[101, 138, 118, 121, 119, 77694, 11444, 95732,...</td>\n",
       "      <td>detected problems:\\n- Empty speech: PAR's utte...</td>\n",
       "      <td>psycological definition:\\n\\n            - defi...</td>\n",
       "      <td>[-0.017170826914434424, 0.0022816185693933216,...</td>\n",
       "      <td>S059/S103/S033/S071/S020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>[0.0042455685, 0.0053692805, 0.0051270816, 0.0...</td>\n",
       "      <td>[101, 20098, 10453, 40811, 102, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>COOKIE JAR</td>\n",
       "      <td>S128</td>\n",
       "      <td>16.0</td>\n",
       "      <td>train</td>\n",
       "      <td>S128_INV_3_42878_48636</td>\n",
       "      <td>INV: OKAY GOOD\\nINV: GOING ON IN THE PICTURE\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S128</td>\n",
       "      <td>INV+PAR</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>50925</td>\n",
       "      <td>[101, 138, 118, 121, 119, 75188, 41282, 82386,...</td>\n",
       "      <td>detected problems:\\n- Empty speech: PAR's utte...</td>\n",
       "      <td>psycological definition:\\n\\n            - defi...</td>\n",
       "      <td>[-0.0186653762774538, 0.00418699146385226, 0.0...</td>\n",
       "      <td>S128/S055/S086/S103/S059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>[-0.3383731, -0.5167495, -0.51931936, -0.54898...</td>\n",
       "      <td>[101, 10110, 10191, 112, 161, 27948, 20098, 11...</td>\n",
       "      <td>1</td>\n",
       "      <td>AND HE'S GETTING COOKIES</td>\n",
       "      <td>S093</td>\n",
       "      <td>25.0</td>\n",
       "      <td>train</td>\n",
       "      <td>S093_PAR_13_60915_66073</td>\n",
       "      <td>INV: WHAT'S GOING ON IN THE PICTURE\\nPAR: UH W...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>S093</td>\n",
       "      <td>INV+PAR</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>81475</td>\n",
       "      <td>[101, 138, 118, 121, 119, 69560, 66865, 26622,...</td>\n",
       "      <td>detected problems:\\n- Empty speech: PAR's utte...</td>\n",
       "      <td>psycological definition:\\n\\n            - defi...</td>\n",
       "      <td>[-0.017022879645798548, 0.019089973114330444, ...</td>\n",
       "      <td>S093/S028/S080/S156/S100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>[-0.0017480814, -0.0034497022, -0.004737093, -...</td>\n",
       "      <td>[101, 10572, 112, 161, 15517, 87016, 143, 4683...</td>\n",
       "      <td>1</td>\n",
       "      <td>SHE'S GOT UH A DRESS ON AND AND THERE'S SHE'S ...</td>\n",
       "      <td>S142</td>\n",
       "      <td>14.0</td>\n",
       "      <td>train</td>\n",
       "      <td>S142_INV_0_328_4357</td>\n",
       "      <td>INV: OKAY CAN YOU TELL ME EVERYTHING YOU SEE G...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S142</td>\n",
       "      <td>INV+PAR</td>\n",
       "      <td>0</td>\n",
       "      <td>328</td>\n",
       "      <td>78752</td>\n",
       "      <td>[101, 138, 118, 121, 119, 77694, 10995, 80575,...</td>\n",
       "      <td>detected problems:\\n- Empty speech: PAR's utte...</td>\n",
       "      <td>psycological definition:\\n\\n            - defi...</td>\n",
       "      <td>[-0.01601245049102418, 0.01671071649298218, 0....</td>\n",
       "      <td>S142/S006/S148/S016/S048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1866</th>\n",
       "      <td>[-0.0022376555, -0.0061541726, -0.0041676927, ...</td>\n",
       "      <td>[101, 12866, 12866, 13997, 102, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "      <td>WINDOW WINDOWS IN</td>\n",
       "      <td>S107</td>\n",
       "      <td>22.0</td>\n",
       "      <td>train</td>\n",
       "      <td>S107_INV_14_106319_107473</td>\n",
       "      <td>INV: HAVE A LOOK AT THAT PICTURE AND TELL ME E...</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S107</td>\n",
       "      <td>INV+PAR</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>126744</td>\n",
       "      <td>[101, 138, 118, 121, 119, 70841, 11518, 73311,...</td>\n",
       "      <td>detected problems:\\n- Empty speech: PAR's utte...</td>\n",
       "      <td>psycological definition:\\n\\n            - defi...</td>\n",
       "      <td>[-0.020275976017968284, 0.018500706164331186, ...</td>\n",
       "      <td>S107/S095/S048/S076/S077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>[-0.09473529, -0.11176892, -0.111610614, -0.13...</td>\n",
       "      <td>[101, 11917, 112, 161, 10343, 10285, 10367, 10...</td>\n",
       "      <td>1</td>\n",
       "      <td>WATER'S POURING ALL OVER THE FLOOR</td>\n",
       "      <td>S129</td>\n",
       "      <td>19.0</td>\n",
       "      <td>train</td>\n",
       "      <td>S129_PAR_9_29919_31766</td>\n",
       "      <td>INV: TELL ME WHAT'S GOING ON\\nPAR: WELL THE KI...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>S129</td>\n",
       "      <td>INV+PAR</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>40000</td>\n",
       "      <td>[101, 138, 118, 121, 119, 69560, 32386, 91958,...</td>\n",
       "      <td>detected problems:\\n- Empty speech: PAR's utte...</td>\n",
       "      <td>psycological definition:\\n\\n            - defi...</td>\n",
       "      <td>[-0.01596907520598941, 0.018663419618138468, 0...</td>\n",
       "      <td>S129/S114/S016/S156/S077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1868 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  \\\n",
       "0     [-0.0026251145, -0.0063604997, -0.009782849, -...   \n",
       "1     [-0.03724482, -0.06742435, -0.07992418, -0.093...   \n",
       "2     [0.3376349, 0.4526255, 0.3523399, 0.32248497, ...   \n",
       "3     [0.0017839464, 0.00059347186, 0.00038148474, 0...   \n",
       "4     [-0.0016942845, 0.022142626, 0.02669364, -0.01...   \n",
       "...                                                 ...   \n",
       "1863  [0.0042455685, 0.0053692805, 0.0051270816, 0.0...   \n",
       "1864  [-0.3383731, -0.5167495, -0.51931936, -0.54898...   \n",
       "1865  [-0.0017480814, -0.0034497022, -0.004737093, -...   \n",
       "1866  [-0.0022376555, -0.0061541726, -0.0041676927, ...   \n",
       "1867  [-0.09473529, -0.11176892, -0.111610614, -0.13...   \n",
       "\n",
       "                                                   text  dementia_labels  \\\n",
       "0     [101, 10103, 12291, 11378, 10127, 21812, 16053...                0   \n",
       "1     [101, 10197, 14650, 10346, 11332, 102, 0, 0, 0...                0   \n",
       "2     [101, 151, 11530, 112, 162, 11811, 10197, 2238...                0   \n",
       "3     [101, 10103, 20098, 10453, 40811, 10127, 11481...                0   \n",
       "4     [101, 10103, 11917, 112, 161, 16484, 10323, 10...                0   \n",
       "...                                                 ...              ...   \n",
       "1863  [101, 20098, 10453, 40811, 102, 0, 0, 0, 0, 0,...                1   \n",
       "1864  [101, 10110, 10191, 112, 161, 27948, 20098, 11...                1   \n",
       "1865  [101, 10572, 112, 161, 15517, 87016, 143, 4683...                1   \n",
       "1866  [101, 12866, 12866, 13997, 102, 0, 0, 0, 0, 0,...                1   \n",
       "1867  [101, 11917, 112, 161, 10343, 10285, 10367, 10...                1   \n",
       "\n",
       "                                               pred_str ID     mmse     ex  \\\n",
       "0     THE STOOL IS TIPPING OT THE LITTLE BOY AND HE'...  S015  29.0  train   \n",
       "1                                       IT MUST BE JUNE  S076  28.0  train   \n",
       "2                                I DON'T SEE IT SNOWING  S018  29.0  train   \n",
       "3     THE COOKIE JAR IS OPEN OF COURSE THE CUPBOARD'...  S062  30.0  train   \n",
       "4                              THE WATER'S RUNNING OVER  S059  30.0  train   \n",
       "...                                                 ...   ...   ...    ...   \n",
       "1863                                         COOKIE JAR  S128  16.0  train   \n",
       "1864                           AND HE'S GETTING COOKIES  S093  25.0  train   \n",
       "1865  SHE'S GOT UH A DRESS ON AND AND THERE'S SHE'S ...  S142  14.0  train   \n",
       "1866                                  WINDOW WINDOWS IN  S107  22.0  train   \n",
       "1867                 WATER'S POURING ALL OVER THE FLOOR  S129  19.0  train   \n",
       "\n",
       "                       path_aug  \\\n",
       "0        S015_PAR_7_26693_32357   \n",
       "1        S076_PAR_8_37400_38800   \n",
       "2       S018_PAR_12_68242_69305   \n",
       "3        S062_PAR_2_14910_19143   \n",
       "4        S059_PAR_4_14123_15540   \n",
       "...                         ...   \n",
       "1863     S128_INV_3_42878_48636   \n",
       "1864    S093_PAR_13_60915_66073   \n",
       "1865        S142_INV_0_328_4357   \n",
       "1866  S107_INV_14_106319_107473   \n",
       "1867     S129_PAR_9_29919_31766   \n",
       "\n",
       "                                               text_aug  dementia_labels_aug  \\\n",
       "0     INV: AND HERE'S THE PICTURE\\nPAR: ALL OF THE A...                    0   \n",
       "1     INV: LOOK AT THAT PICTURE\\nPAR: MHM\\nINV: AND ...                    0   \n",
       "2     INV: OKAY\\nINV: AND THERE'S THE PICTURE\\nPAR: ...                    0   \n",
       "3     PAR: WELL THERE'S A KID STEALING COOKIES FROM ...                    0   \n",
       "4     INV: ON IN THE PICTURE\\nPAR: GIRL IS REACHING ...                    0   \n",
       "...                                                 ...                  ...   \n",
       "1863  INV: OKAY GOOD\\nINV: GOING ON IN THE PICTURE\\n...                    0   \n",
       "1864  INV: WHAT'S GOING ON IN THE PICTURE\\nPAR: UH W...                    1   \n",
       "1865  INV: OKAY CAN YOU TELL ME EVERYTHING YOU SEE G...                    0   \n",
       "1866  INV: HAVE A LOOK AT THAT PICTURE AND TELL ME E...                    0   \n",
       "1867  INV: TELL ME WHAT'S GOING ON\\nPAR: WELL THE KI...                    1   \n",
       "\n",
       "      ... session     role number start_time end_time  \\\n",
       "0     ...    S015  INV+PAR      7          0    56600   \n",
       "1     ...    S076  INV+PAR      8          0    43563   \n",
       "2     ...    S018  INV+PAR     12          0    71267   \n",
       "3     ...    S062  INV+PAR      2          0    41000   \n",
       "4     ...    S059  INV+PAR      4          0    58323   \n",
       "...   ...     ...      ...    ...        ...      ...   \n",
       "1863  ...    S128  INV+PAR      3          0    50925   \n",
       "1864  ...    S093  INV+PAR     13          0    81475   \n",
       "1865  ...    S142  INV+PAR      0        328    78752   \n",
       "1866  ...    S107  INV+PAR     14          0   126744   \n",
       "1867  ...    S129  INV+PAR      9          0    40000   \n",
       "\n",
       "                                              Embedding  \\\n",
       "0     [101, 138, 118, 121, 119, 63547, 62338, 48626,...   \n",
       "1     [101, 138, 118, 121, 119, 75188, 95584, 11444,...   \n",
       "2     [101, 138, 118, 121, 119, 65945, 92425, 79225,...   \n",
       "3     [101, 138, 118, 121, 119, 77694, 26622, 42299,...   \n",
       "4     [101, 138, 118, 121, 119, 77694, 11444, 95732,...   \n",
       "...                                                 ...   \n",
       "1863  [101, 138, 118, 121, 119, 75188, 41282, 82386,...   \n",
       "1864  [101, 138, 118, 121, 119, 69560, 66865, 26622,...   \n",
       "1865  [101, 138, 118, 121, 119, 77694, 10995, 80575,...   \n",
       "1866  [101, 138, 118, 121, 119, 70841, 11518, 73311,...   \n",
       "1867  [101, 138, 118, 121, 119, 69560, 32386, 91958,...   \n",
       "\n",
       "                                          Psych_Summary  \\\n",
       "0     detected problems:\\n- Empty speech: PAR's utte...   \n",
       "1     detected problems:\\n- Empty speech: PAR's utte...   \n",
       "2     detected problems:\\n- Empty speech: PAR's utte...   \n",
       "3     detected problems:\\n- Empty speech: PAR's utte...   \n",
       "4     detected problems:\\n- Empty speech: PAR's utte...   \n",
       "...                                                 ...   \n",
       "1863  detected problems:\\n- Empty speech: PAR's utte...   \n",
       "1864  detected problems:\\n- Empty speech: PAR's utte...   \n",
       "1865  detected problems:\\n- Empty speech: PAR's utte...   \n",
       "1866  detected problems:\\n- Empty speech: PAR's utte...   \n",
       "1867  detected problems:\\n- Empty speech: PAR's utte...   \n",
       "\n",
       "                                           Psych_Prompt  \\\n",
       "0     psycological definition:\\n\\n            - defi...   \n",
       "1     psycological definition:\\n\\n            - defi...   \n",
       "2     psycological definition:\\n\\n            - defi...   \n",
       "3     psycological definition:\\n\\n            - defi...   \n",
       "4     psycological definition:\\n\\n            - defi...   \n",
       "...                                                 ...   \n",
       "1863  psycological definition:\\n\\n            - defi...   \n",
       "1864  psycological definition:\\n\\n            - defi...   \n",
       "1865  psycological definition:\\n\\n            - defi...   \n",
       "1866  psycological definition:\\n\\n            - defi...   \n",
       "1867  psycological definition:\\n\\n            - defi...   \n",
       "\n",
       "                                         Similarity_Emb  \\\n",
       "0     [-0.017662624105078045, 0.0017859735411924677,...   \n",
       "1     [-0.01690500758420462, 0.021220975649458663, 0...   \n",
       "2     [-0.018221234206087917, 0.0172382670926329, 0....   \n",
       "3     [-0.014507318536576209, 0.01572004712565187, 0...   \n",
       "4     [-0.017170826914434424, 0.0022816185693933216,...   \n",
       "...                                                 ...   \n",
       "1863  [-0.0186653762774538, 0.00418699146385226, 0.0...   \n",
       "1864  [-0.017022879645798548, 0.019089973114330444, ...   \n",
       "1865  [-0.01601245049102418, 0.01671071649298218, 0....   \n",
       "1866  [-0.020275976017968284, 0.018500706164331186, ...   \n",
       "1867  [-0.01596907520598941, 0.018663419618138468, 0...   \n",
       "\n",
       "                Similarity_IDs  \n",
       "0     S015/S154/S020/S033/S071  \n",
       "1     S076/S095/S089/S156/S126  \n",
       "2     S018/S032/S089/S048/S006  \n",
       "3     S062/S001/S118/S083/S126  \n",
       "4     S059/S103/S033/S071/S020  \n",
       "...                        ...  \n",
       "1863  S128/S055/S086/S103/S059  \n",
       "1864  S093/S028/S080/S156/S100  \n",
       "1865  S142/S006/S148/S016/S048  \n",
       "1866  S107/S095/S048/S076/S077  \n",
       "1867  S129/S114/S016/S156/S077  \n",
       "\n",
       "[1868 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(audio.shape)\n",
    "print(token.shape)\n",
    "print(Aug.shape)\n",
    "model.val_data[0]\n",
    "model.df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(self.df_val, self.df_val_aug, on='ID   ', how='left', suffixes=('', '_aug'))\n",
    "\n",
    "print(model.df_val_aug.columns)\n",
    "print(model.df_test.columns)\n",
    "print(len(model.df_test.iloc[0]['Embedding']))\n",
    "print(len(model.df_test.iloc[0]['Similarity_Emb']))\n",
    "print(len(model.df_train.iloc[0]['Embedding']))\n",
    "print(len(model.df_train.iloc[0]['Similarity_Emb']))\n",
    "# print()\n",
    "print(len(model.df_val_aug.iloc[0]['Embedding']))\n",
    "print(len(model.df_val_aug.iloc[0]['Similarity_Emb']))\n",
    "self.df_val_aug\n",
    "merged_df\n",
    "self.df_val\n",
    "# self.df_test\n",
    "# self.df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.preprocess_loaded_summaries()\n",
    "print(self.df_test.columns)\n",
    "print(self.df_test_aug.columns)\n",
    "print(len(self.df_test))\n",
    "print(len(self.df_test_aug))\n",
    "merged_df = pd.merge(self.df_test, self.df_test_aug, on='ID   ', how='left')\n",
    "print(merged_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=self.test_data\n",
    "# 创建一个 DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 获取下一个项目\n",
    "iterator = iter(dataloader)\n",
    "next_item = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self=model\n",
    "for batch in model.train_data:\n",
    "    token, audio, labels = batch \n",
    "    # token, labels,id_ = batch \n",
    "    print('id', id_)\n",
    "    logits = self(token, audio) \n",
    "    # logits = self(token) \n",
    "    \n",
    "    preds = logits.argmax(dim=-1)\n",
    "\n",
    "    y_true = list(labels.cpu().numpy())\n",
    "    y_pred = list(preds.cpu().numpy())\n",
    "\n",
    "    # --> HERE STEP 2 <--\n",
    "    # self.test_step_outputs.append({\n",
    "    #     'y_true': y_true,\n",
    "    #     'y_pred': y_pred,\n",
    "    # })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, audio= token, audio\n",
    "print(audio)\n",
    "if self.t_embed_type == \"mbert\":\n",
    "    t_out = self.t_model(text)[1] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
